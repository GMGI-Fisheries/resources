{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the GMGI Fisheries Lab Handbook!","text":"<p>[team description]</p>"},{"location":"#gloucester-marine-genomics-institute","title":"Gloucester Marine Genomics Institute","text":"<p>[description and links]</p>"},{"location":"#laboratory-protocols","title":"Laboratory Protocols","text":""},{"location":"#bioinformatic-pipelines","title":"Bioinformatic Pipelines","text":""},{"location":"#recent-news-and-publications","title":"Recent News and Publications","text":""},{"location":"Computing_Begin_Resources/","title":"Computing Beginner Resources","text":""},{"location":"Computing_Begin_Resources/#shell-linux-and-bash","title":"Shell, Linux, and Bash","text":"<p>Shell: A shell is a command-line interface program that provides a user interface for interacting with an operating system Bash: short for Bourne-Again SHell, is a command-line shell and scripting language widely used in Unix-like operating systems, including Linux.    Linux/Unix: both operating systems that are used through a command line but Linux is an open-source version of Unix.  </p> <ul> <li>Bioinformatics | PNNL </li> <li>An Introduction to Linux Basics | DigitalOcean </li> <li>The Bash Guide </li> <li>Introducing the Shell </li> <li>Introduction to the Command Line for Genomics </li> </ul>"},{"location":"Computing_Begin_Resources/#rstudio","title":"RStudio","text":"<ul> <li>A Installing R and RStudio | Hands-On Programming with R (rstudio-education.github.io) </li> <li>Hands-On Programming with R (rstudio-education.github.io) </li> <li>R-Crash-Course </li> <li>Intro2R </li> <li>A crash-course in using a project-oriented workflow with Git + GitHub in scientific research </li> </ul>"},{"location":"Computing_Begin_Resources/#metabarcoding-bioinformatics","title":"Metabarcoding bioinformatics","text":"<ul> <li>Bioinformatic Methods for Biodiversity Metabarcoding \u2014 Bioinformatic Methods for Biodiversity Metabarcoding documentation (learnmetabarcoding.github.io) </li> <li>Introduction to the bioinformatic analysis of eDNA metabarcoding data \u2013 eDNA (otagobioinformaticsspringschool.github.io) </li> </ul>"},{"location":"Computing_Begin_Resources/#recommended-courses","title":"Recommended courses","text":"<ul> <li>FISH546 - Bioinformatics for Environmental Sciences </li> <li>FISH274 - Introduction to Data Analysis for Aquatic Sciences </li> <li>Data Carpentry for Biologists </li> </ul>"},{"location":"Computing_GMGI/","title":"GMGI in-house Computing Resources","text":"<p>Gloucester Marine Genomics Institute (GMGI) has 2 in-house servers that are used for bioinformatic analyses. </p> <p></p> <p>Ubuntu Linux operating system aka Humarus </p> <p>Humarus is primarily used for large-scale jobs (e.g., genome assemblies) and thus not the primary working area for Fisheries. </p> <p>Red Hat Enterprise Linux (RHEL) aka Gadus </p> <p>RHEL/Gadus is the primary working area, storage space, and is data is backed up daily to the Synology RackStation in-house. </p>"},{"location":"Computing_GMGI/#logging-in","title":"Logging in","text":"<p>Use ssh with username and the correct IP address that can be found on Lab Archives. Follow instructions for entering password. New users will need to get set-up with Jen while onboarding. </p> <pre><code>ssh username@123.456.7.8\n</code></pre>"},{"location":"Computing_GMGI/#server-structure","title":"Server Structure","text":"<p>Once logged in, users are directed to their home directory (<code>~/</code>) by default. This space has limited storage and is not intended for regular work. The Fisheries team primarily uses the NU Discovery Cluster for active projects and GMGI's in-house resources for long-term storage and data archiving. Consequently, team members typically use their home directory only for data transfers. </p> <p>General server structure: </p> <p>Do not edit any folder other than <code>data</code>. Only the RHEL main contact is responsible for downloading modules or setting up users. </p> <pre><code>[estrand@gadus ~]$ cd ../../\n[estrand@gadus /]$ ls\nbin  boot  data  dev  etc  home  lib  lib64  media  mnt  opt  proc  root  run  sbin  srv  sys  tmp  usr  var\n</code></pre> <p>Subdirectories within <code>data</code>:  </p> <ul> <li><code>prj</code>: Each lab has their own folder (e.g., <code>prj/Fisheries</code>) that is a working area for data and bioinformatic analyses.    </li> <li><code>resources</code>: Shared resources like common databases, modules, and scripts live here.  </li> <li><code>usr</code> and <code>var</code> are for the RHEL main contact only. </li> </ul> <pre><code>[estrand@gadus /]$ cd data/\n[estrand@gadus data]$ ls\nprj  resources  usr  var\n</code></pre> <p>Fisheries folders (<code>prj/Fisheries</code>):  </p> <p>We organize these folders by type of analyses or project. I.e., all eDNA projects should be nested within <code>edna</code>. </p> <pre><code>[estrand@gadus Fisheries]$ ls\n202402_negatives  edna  epiage  JonahCrabGenome  JonahCrabPopGen  lobster  SandLanceData\n</code></pre>"},{"location":"Computing_GMGI/#programs-and-modules","title":"Programs and modules","text":"<p>We run programs as 'modules' that are downloaded by the RHEL's main contact (Jen). If you need a program, send Jen a slack or email with the program name, download link, and if an R package, specify if it is a Bioconductor package or regular CRAN repository package. Once a program is downloaded as a module, this is available for all users. Global installation of programs and R packages helps keep the server uncluttered and not waste space with multiple installations. Do not install your own copies. </p> <p>Common commands:   </p> <ul> <li>To find already installed programs: <code>module avail</code> </li> <li>To get information about a module: <code>module help [module/version]</code> or <code>module whatis [module/version]</code>. \"help\" will provide what the module is, package information including version and install date, and a link to the documentation/github. \"whatis\" will provide a short, one line description of the program.    </li> <li>To load a module: <code>module load [module/version]</code> (e.g., <code>module load bamUtil/v1.0.15</code>). Loading a module will put all the necessary executables and dependencies for that program in your path so you can call the commands from any location (i.e. your working directory).    </li> </ul> <p>Replace \"[module/version]\" with the information for your module of interest, as it shows up in \"module avail\" list.</p>"},{"location":"Computing_GMGI/#resource-usage","title":"Resource usage","text":"<p>The GMGI RHEL does not currently have a job scheduler program so each user needs to be extremely careful with how much memory and resources their scripts take up. RHEL has 128 processors (CPUs) that are available total so users need to split this. Users should use CPU usage between 1-32 threads max at a time to allow other teams to use the server as well. </p> <p>Common commands:    </p> <ul> <li>Check all jobs that are running: <code>top</code> and to exit that screen, click Q  </li> <li>Check only our user: <code>top -u username</code> and to exit that screen, click Q  </li> </ul> <p>The most important aspects to watch are Job %CPU and %MEM, server %CPU, and load average. The load average is the average number of processes that are either running on the CPU or waiting for CPU time over the last 1, 5, and 15 minutes. </p> <p>In the example below, user #1 is using a program called 'cd-hit' that is currently using 1598% CPU, which is the equivalent of using 16 CPU cores or processors. When running a job, this is the value that I would check on the most to make sure I'm not taking up the entire server. Programs will have different default CPU maximums so check default flags prior to running scripts. </p> <p></p>"},{"location":"Computing_GMGI/#running-a-bioinformatic-script","title":"Running a bioinformatic script","text":"<p>Using \"tmux\" terminal multiplexer will allow you to runs scripts in multiple windows within a single terminal window, and to jump back and forth between them. This also allows a user to start a script, log off and have that continue to run while the user's computer isn't connected to internet. This is also called using a 'screen' on other servers but screen was deprecated after RHEL7, and our system was upgraded from RHEL7 to RHEL8 OS in Sept. 2023.</p> <p>Common commands:     </p> <ul> <li>Create a new session named \"test\": <code>tmux new -s test</code> </li> <li>Detach from a session: Press Ctrl+B, release, and then press D  </li> <li>Reopen/attach a detached session: <code>tmux attach-session -t test</code> </li> <li>View and/or switch between sessions without detaching from tmux: Prese Ctrl+B, release, than press W. A list will appear and you can toggle between options using the up and down arrows. The select one, make sure it is highlighted and press Enter.  </li> <li>End a tmux session (forever - not just detached): In the attached session, type <code>exit</code> and press enter. Or press Ctrl+D  </li> </ul> <p>Write everything in the tmux session to a text file: </p> <ul> <li>Output the history limit: <code>tmux display-message -p -F \"#{history_limit}\" -t test</code> </li> <li>Capture output to text file: <code>tmux capture-pane -Jp -S -### -t test &gt; test.txt</code>. Replace the ### with the history limit above.  </li> </ul> <p>Note: the automatic limit is 2000 lines. If you know you're going to run verbose commands and want to be able to capture it all in a log file run the command below right before starting a new session (note must already have another session open): <code>tmux set-option -g history-limit 99999</code></p>"},{"location":"Computing_NU/","title":"Northeastern University Computing Resources","text":"<p>A high-performance computing resource for the Northeastern research community, the Discovery cluster is located in the Massachusetts Green High Performance Computing Center in Holyoke, Massachusetts.</p> <p>The Discovery cluster provides Northeastern researchers with access to more than 45,000 CPU cores and more than 400 GPUs. Connected to the university network over 10 Gbps Ethernet (GbE) for high-speed data transfer, Discovery provides 5 PB of available storage on a high-performance GPFS parallel file system. Compute nodes are connected with either 10 GbE or a high-performance HDR100 InfiniBand (IB) interconnect running at 200 Gbps, supporting all types and scales of computational workloads.</p> <p>As GMGI's researchers, we have access to Northeastern's HPC resources through an MOU established in Fall 2023. </p> <p>Read the HPC resource documentation prior to getting started: Research Computing - NURC RTD (northeastern.edu).</p> <p></p>"},{"location":"Computing_NU/#logging-in","title":"Logging in","text":"<p>Before creating an account with the NU Discovery Cluster, claim your Northeastern email and sponsored account.</p> <ol> <li>Log into (with the northeastern email and pw previously claimed): Home - Northeastern Tech Service Portal.  </li> <li>Navigate to High Performance Computing - Northeastern Tech Service Portal.    </li> <li>Request an account (Research Computing Access Request - Northeastern Tech Service Portal). Fill out the form with your Northeastern email, and following options:  </li> <li>Select: I do not have access to Discovery. I am requesting a new account.  </li> <li>Affiliation with Northeastern University: Visiting Researcher (Greg says this answer doesn't really matter).   </li> <li>University Sponsor: Geoffrey Trusell  </li> <li>Gaussian: No (This is a specific program, if you don't know what it is, you don't need it).  </li> <li>Select: By clicking here, I acknowledge that I have read and agree to the following.  </li> <li>This triggers an email to Geoff Trusell to approve your account. Send an email to Geoff letting him know that you are activating your account that needs his approval.  </li> <li>Once Geoff approves the account sponsorship, then Greg and the computing team will finish setting up your account.</li> </ol> <p>You can operate on the Discovery Cluster in two ways: 1. via Linux operating system on your computer or ssh client 2. NU's Open On Demand (Open OnDemand (OOD) - RC RTD (northeastern.edu)) GUI. Accessing Open OnDemand - RC RTD (northeastern.edu). With OOD interface, you can access plug-ins (i.e. RStudio) and launch the server from the web. Serena recommended using incognito window b/c OOD usually works better without the caching.</p> <pre><code>ssh username@login.discovery.neu.edu\n</code></pre>"},{"location":"Computing_NU/#server-structure","title":"Server structure","text":"<p>We have a 30 TB maximum in our working space <code>/work/gmgi</code>:</p> <ul> <li>Each lab has their own subfolder that serves as their storage and working space (e.g., <code>work/gmgi/Fisheries/</code>).  </li> <li><code>databases/</code>: Shared folder for common databases for amplicon sequencing (i.e., 12S, 16S, 18S, COI) and NCBI nt database. View the README file for databases sources.</li> <li><code>containers/</code>: Shared folder for custom built containers.  </li> <li><code>packages/</code>: Shared folder for programs downloaded for all users.  </li> <li><code>miniconda3/</code> and <code>Miniconda3-latest-Linux-x86_64.sh</code>: Shared resource for building conda environments. Environments built here can be used by everyone. Do not edit. </li> <li><code>check_storage.sh</code>: Bash script built to calculate TB usage from each lab's folder and gmgi's work and output is <code>storage_summary_2024-09-23.txt</code> with the date calculated.</li> </ul> <pre><code>[e.strand@login-00 ~]$ cd /work/gmgi\n[e.strand@login-00 gmgi]$ ls\ncheck_storage.sh  containers  databases  ecosystem-diversity  Fisheries  miniconda3  Miniconda3-latest-Linux-x86_64.sh  packages  storage_summary_2024-09-23.txt\n</code></pre> <p>Fisheries folder (<code>work/gmgi/Fisheries/</code>) is split by the type of project. <code>reference_genomes</code> includes .fasta reference files for organisms rather than a database (<code>Haddock_ref.fasta</code>). </p> <pre><code>[e.strand@login-00 Fisheries]$ ls\neDNA  epiage  reference_genomes\n</code></pre>"},{"location":"Computing_NU/#storage-rules","title":"Storage rules","text":"<p>While analyzing data, NOT just at the end of a project!</p> <p>Raw data files are backed up on AWS services and on GMGI RHEL Gadus immediately upon receiving data. If working on NU cluster, once user is happy with data analysis pipeline, raw and final data is to be removed from NU cluster and only kept on AWS services or GMGI's RHEL server. </p> <p>Compressing files:  </p> <ul> <li>Gzip all fastq files (e.g., raw data, trimmed data), .fasta/.fa files (e.g., reference genomes), and large .txt files (e.g., intermediate files created during analysis): <code>gzip *.fastq</code> or create a slurm array with a sbatch script.    </li> <li>Genozip all .bam, .sam, .vcf files (e.g., intermediate files created during analysis). Genozip has been downloaded in /work/gmgi/packages for general use.   </li> </ul> <p>Space-related commands:  </p> <ul> <li>List all files within a directory and human-readable sizes (folder size is not total size of folder contents): <code>ls -lha</code> </li> <li>Calculate total storage taken up by one directory (change path as needed): <code>du -shc .[^.]* /work/gmgi/fisheries</code> </li> <li>In /work/gmgi/, there is a <code>check_storage.sh</code> bash script that will use the above commands to create a summary .txt file with the storage use of each team.  </li> </ul>"},{"location":"Computing_NU/#nu-contacts-and-research-computing-help","title":"NU Contacts and Research Computing Help","text":"<p>GMGI's two main contacts are: Greg Shomo (g.shomo@northeastern.edu) and Serena Caplins (s.caplins@northeastern.edu). </p> <p>If you need assistance, NU's support team is available at rchelp@northeastern.edu or consult the Frequently Asked Questions (FAQs). Emailing the rchelp team will create a help ticket one of NU's team members will be assigned to your case. When creating a help ticket, CC Geoff Trussell (g.trussell@northeastern.edu) and Jon Grabowski (j.grabowski@northeastern.edu). </p> <p>RC Help hosts office hours to connect with RC staff and Graduate Research Assistants (GRAs) to ask questions about any RC-related questions that you might have (e.g., Setting up conda environments, Installing software, Optimizing the runtime of your sbatch scripts, Effectively using the Open onDemand website): - Wednesdays 3 pm - 4 pm: zoom link - Thursdays 11 am - 12 pm: zoom link </p> <p>Ask 2-3 others at GMGI for assistance and/or attend office hours prior to creating a help ticket. We do not want to overwhelm the rchelp desk if we can troubleshoot internally first. GMGI has a #bioinformatics slack channel for this purpose.</p>"},{"location":"Computing_NU/#running-a-bioinformatic-script","title":"Running a bioinformatic script","text":"<p>Read through: Running Jobs - RC RTD (northeastern.edu) before starting. </p> <p>Jobs are run either through: 1. Interactive mode (immediate execution and feedback): <code>srun --pty bash</code> to claim a node and then utilize <code>bash scriptname.sh</code> to run a script.   2. Batch jobs: using scripts to manage longer-running jobs: <code>sbatch scriptname.sh</code> to run a script.  </p> <p>Interactive mode would be equivalent to running a job directly in your terminal window without starting a tmux session. Using batch jobs and shell scripts would be similar to tmux session where you can turn off wifi, walk away, etc. Interactive requires you to stay connected. </p> <p>Introduction to Slurm scripts:    </p> <ul> <li>Slurm - RC RTD (northeastern.edu) </li> <li>Best Practices - RC RTD (northeastern.edu) </li> </ul>"},{"location":"Computing_NU/#packages-and-modules","title":"Packages and modules","text":"<p>NU has some modules downloaded that are accessible for all users. Otherwise, larger packages should be installed in a conda environment by the user or simple packages can be downloaded to the <code>/work/gmgi/packages/</code> folder. Instructions for downloading to conda env: [https://rc-docs.northeastern.edu/en/latest/software/index.html]</p> <p>Common commands:   </p> <ul> <li>To find already installed programs: <code>module avail</code> </li> <li>To get information about a module: <code>module help [module/version]</code> or <code>module whatis [module/version]</code>. \"help\" will provide what the module is, package information including version and install date, and a link to the documentation/github. \"whatis\" will provide a short, one line description of the program.    </li> <li>To load a module: <code>module load [module/version]</code> (e.g., <code>module load bamUtil/v1.0.15</code>). Loading a module will put all the necessary executables and dependencies for that program in your path so you can call the commands from any location (i.e. your working directory).   </li> </ul>"},{"location":"Data%20Visualization%20in%20R/","title":"Data Visualization and Analysis in R","text":""},{"location":"Data%20Visualization%20in%20R/#visualization","title":"Visualization","text":"<p>Heatmaps:    </p> <ul> <li>R package for using pheatmap with minimal coding: Tidy Heatmaps </li> </ul> <p>Picking color:   </p> <ul> <li>Coolors palette generator</li> </ul>"},{"location":"Data%20Visualization%20in%20R/#data-analysis","title":"Data Analysis","text":"<p>Generalized Linear Models:     </p> <ul> <li>Workshop 6: Generalized linear models in R</li> </ul>"},{"location":"Data%20Visualization%20in%20R/#tips-and-tricks","title":"Tips and Tricks","text":"<p>Export an image as ggsave(\"file.svg\") and open in powerpoint. The image is vectorized and edit-able.  </p>"},{"location":"Data_Management/","title":"Data Management","text":"<p>GMGI Fisheries data management workflow:</p> <p></p>"},{"location":"Data_To_Server/","title":"Downloading sequencing data to servers","text":"<p>Goal: Download .fastq files from sequencing center to HPC and/or move data between HPCs and personal computers.</p> <p>Table of Contents: - GMGI in-house sequencing to HPC  - External sequencing to HPC  - HPC to Personal Computer - HPC to AWS back-up </p> <p>To transfer data from HPC to HPC (e.g., GMGI to NU), use Globus instructions outlined in External sequencing to HPC. </p>"},{"location":"Data_To_Server/#illumina-basespace-to-nu-discovery-cluster-or-gmgi-in-house-hpc","title":"Illumina BaseSpace to NU Discovery Cluster or GMGI in-house HPC","text":"<p>Illumina BaseSpace CLI instructions</p> <p>Connecting your user to Illumina BaseSpace:  </p> <p>Each user only needs to complete this once to set-up. If completed for previous projects, skip to downloading data steps. </p> <ol> <li>Create folder called bin: <code>mkdir $HOME/bin</code> </li> <li>Download BaseSpace CLI: <code>wget \"https://launch.basespace.illumina.com/CLI/latest/amd64-linux/bs\" -O $HOME/bin/bs</code> </li> <li>Change the file permissions to make the downloaded binary executable: <code>chmod u+x $HOME/bin/bs</code> </li> <li>Authenticate your account: <code>bs auth</code> </li> <li>Navigate to the webpage provided and authenticate use of BaseSpace CLI.  </li> </ol> <p>Download data from each run to desired output path: </p> <ol> <li>Find the Run ID of desired download: Within Sequence Hub, navigate to Runs and select the desired run. The Run ID is in the webpage handle (e.g., https://basespace.illumina.com/run/123456789/details). </li> </ol> <p></p> <ol> <li>Navigate to <code>cd $HOME/bin</code> and download dataset: <code>bs download run -n run_name --extension=fastq.gz -o /local/output/path</code>. Replace <code>run_name</code> with the exact name of the run on BaseSpace.  </li> <li>Navigate to the output path <code>cd /local/output/path</code> and move all files out of subdirectories: <code>mv */* .</code> </li> </ol>"},{"location":"Data_To_Server/#globus-to-nu-discovery-cluster-or-gmgi-in-house-hpc","title":"Globus to NU Discovery Cluster or GMGI in-house HPC","text":"<p>External sequencing centers (e.g., UConn) will share data via Globus. Instructions from NU on transfering data and using Globus. Globus works by transferring data between 'endpoints'. NU's endpoint is called Discovery Cluster which is searchable but our in-house GMGI endpoint needs to be created for each user. </p> <p>Globus instructions: [https://docs.globus.org/globus-connect-server/v5.4/quickstart/]. Create a Globus account prior to instructions below. If transferring to NU, user needs to connect their NU account to their Globus account (see above NU instructions for this step). </p> <p>GMGI endpoint set-up (only need to do this once): 1. Navigate to the globusconnectpersonal-3.2.2 module that is already downloaded on GMGI's in-house server: <code>cd /data/resources/app_modules/globusconnectpersonal-3.2.2</code>. 2. Set-up an endpoint: <code>./globusconnectpersonal -setup --no-gui</code> 3. This will then ask you to click on a log-in link. Once logged in, you receive a authorization code. Paste that in your terminal window where it asked for this code. 4. Name your endpoint with your own user (change this to your first and last name): <code>user.name</code> 5. If successfully, globus will output: </p> <pre><code>Input a value for the Endpoint Name: user.name\n\nregistered new endpoint, id: [unique ID to you]\n\nsetup completed successfully\n</code></pre> <p>Start Globus transfer: 1. [GMGI only] Navigate to the globusconnectpersonal-3.2.2 module that is already downloaded on GMGI's in-house server: <code>cd /data/resources/app_modules/globusconnectpersonal-3.2.2</code>. 2. [GMGI only] Activate personal endpoint: <code>./globusconnectpersonal -start &amp;</code> 3. [GMGI only] Your <code>user.name</code> endpoint will now appear as an option on the Globus online interface.  4. Log into Globus and Navigate to 'Collections' on the left hand panel. Confirm that your GMGI endpoint is activated (green icon):</p> <p></p> <ol> <li>Select the 'File Manager' on the left hand panel. Choose the sequencing center endpoint in the left side and the server end point on the right side. NU's Discovery Cluster is searchable but GMGI endpoint will the user.name set up in previous steps. </li> </ol> <p></p> <ol> <li>Select all files that you want to transfer.  </li> <li>Select Start to begin the transfer.  </li> <li>Check the status of a transfer by selecting 'Activity' on the left hand panel.  </li> <li>[GMGI only] Once transfer is complete, deactivate the endpoint: <code>./globusconnectpersonal -stop</code>. </li> </ol>"},{"location":"Data_To_Server/#hpc-to-personal-computer-or-vice-versa","title":"HPC to personal computer or vice versa","text":"<p>Users can do this via Globus or 'scp' (secure copy paste) commands detailed below. NU instructions on transfer via terminal. Make sure you're using \"xfer.discovery.neu.edu\" for the discovery cluster and not login.discovery.neu.edu, or you'll get an email warning you that you're using too much CPU!</p> <p>For all the below code, change content in &lt;&gt; and then delete the &lt;&gt;. All commands need to be run in own terminal and not logged onto either server. </p> <p>Transfer a file: - To NU from personal computer: <code>scp &lt;filename and path&gt; &lt;username&gt;@xfer.discovery.neu.edu:/path/</code>  - To personal computer from NU: <code>scp &lt;username&gt;@xfer.discovery.neu.edu:/path/ /output/path/</code> </p> <p>Transfer a directory (a.k.a., repository) to personal computer from NU: <code>scp -r &lt;username&gt;@xfer.discovery.neu.edu:/path/ /output/path/</code> </p> <p>To transfer directly from GMGI to NU or vice versa, use Globus. </p>"},{"location":"Data_To_Server/#aws-back-up","title":"AWS Back-up","text":"<p>AWS is our Amazon Web Services S3 cloud-based storage to backup data long-term data storage and back-up. GMGI uploads data from our in-house server to AWS.</p> <p>What should be backed up: - Raw data files such as fastq files directly from the sequencer - Final result data files (i.e. count table, assemblies, etc.)  </p> <ol> <li>Make sure all files are compressed by:  </li> <li>Gzip all fastq files (e.g., raw data, trimmed data), .fasta/.fa files (e.g., reference genomes), and large .txt files (e.g., intermediate files created during analysis): <code>gzip *.fastq</code> or create a slurm array with a sbatch script.  </li> <li>Genozip all .bam, .sam, .vcf files (e.g., intermediate files created during analysis). Genozip program is downloaded NU in <code>/work/gmgi/packages/</code> for general use.  </li> </ol> <p>Prior to AWS back-up, check with Tim or Emma for approval of files and compression. </p> <ol> <li>Create a new screen session called AWS_tar (user can change this name to desired): <code>tmux new -s AWS_tar</code> </li> <li>Create a txt file with file sizes of all desired input: <code>ls -l *gz &gt; file_size.txt</code> </li> <li>Edit this file to be only file sizes and names: <code>awk '{print $5,$9}' file_size.txt &gt; file_size_edited.txt</code> </li> <li>View this edited file: <code>head file_size_edited.txt</code></li> </ol> <pre><code>11400971821 Mae-263_S1_R1_001.fastq.gz\n\n12253428145 Mae-263_S1_R2_001.fastq.gz\n\n11962611469 Mae-266_S2_R1_001.fastq.gz\n\n12839131166 Mae-266_S2_R2_001.fastq.gz\n\n9691926610 Mae-274_S3_R1_001.fastq.gz\n</code></pre> <ol> <li>Sum the first column: <code>awk '{sum += $1} END {print sum}' file_size_edited.txt</code>. This value is in Byte (B), but convert to MB or TB for a more helpful value to work with. It's important to know the size of the data you are working for storage and cost purposes. Backing up to AWS costs $$/monthly based on TBs stored and our HPC systems have max TB storage limitations.  </li> <li>Tar all desired data to result in on zipped file: <code>tar -czvf HaddockEpiAge1_rawData_20231113.tar.gz ./*fastq.gz</code>. Tar file name needs to be a unique identifier that includes project name, date, and description of the files included.  </li> <li>Detach from a session: Press Ctrl+B, release, and then press D. This tar function will take awhile especially for large datasets.  </li> <li>Reopen/attach a detached session: <code>tmux attach-session -t AWS_tar</code>.  </li> <li>Check the tar file size: <code>ls -lha</code>. Example output: <code>-rw-rw-r--. 1 estrand science 1736366676523 Nov 15 17:54 HaddockEpiAge1_rawData_20231113.tar.gz</code>. This 1736366676523 value is the size in B which should match exactly the sum of all input file sizes calculated previously.   </li> <li>Once this tar function is complete, end a tmux session (forever - not just detached): In the attached session, type exit and press enter. Or press Ctrl+D.  </li> <li>Move the packed tar archive file to AWS transfer folder: <code>mv HaddockEpiAge1_rawData_20231113.tar.gz /data/prj/AWS-transfers</code>.  </li> <li>Notify Jen that there is a transfer waiting so she can move this to AWS services and then remove from the /AWS-transfers folder. </li> </ol>"},{"location":"test/","title":"Test","text":"<p>TEST</p>"},{"location":"EpiAge/eDNA_01/","title":"01-","text":"<p>TEST</p>"},{"location":"EpiAge/eDNA_02/","title":"eDNA 02","text":"<p>TEST</p>"},{"location":"PopGen/eDNA_01/","title":"01-","text":"<p>TEST</p>"},{"location":"PopGen/eDNA_02/","title":"eDNA 02","text":"<p>TEST</p>"},{"location":"eDNA/","title":"Environmental DNA (eDNA) Bioinformatic Workflow","text":"<p>Graphic from Szekely et al. 2022</p>"},{"location":"eDNA/#bony-fish-and-elasmobranch-targets","title":"Bony Fish and Elasmobranch Targets","text":""},{"location":"eDNA/#invertebrate-targets","title":"Invertebrate Targets","text":""},{"location":"eDNA/00-creating_eDNA_conda_env/","title":"Creating a conda environment for eDNA projects","text":"<p>Background information on Conda. </p> <p>GMGI Fisheries has a conda environment set-up with all the packages needed for this workflow. Code below was used to create this conda environment.</p> <p>DO NOT REPEAT every time user is running this workflow.</p> <pre><code># Activate conda\nsource ~/../../work/gmgi/miniconda3/bin/activate\n\n# Creating conda \nconda create --name fisheries_eDNA\n\n# Installing packages needed for this workflow \nconda install -c bioconda fastqc \nconda install multiqc \nconda install bioconda::nextflow \nconda install conda-forge::singularity\nconda install bioconda::blast\nconda install nextflow\nconda install blast\nconda install singularity\nconda install -c bioconda vsearch -y\npip install nsdpy\nconda install wget\n</code></pre> <p>The conda environment is started within each slurm script, but to activate conda environment outside of the slurm script to update packages or check what is installed:</p> <pre><code># Activate conda\nsource ~/../../work/gmgi/miniconda3/bin/activate\n\n# Activate fisheries eDNA conda environment \nconda activate fisheries_eDNA\n\n# List all available environments \nconda env list \n\n# List all packages installed in fisheries_eDNA\nconda list \n\n# Update a package\nconda update [package name]\n\n# Update nextflow ampliseq workflow \nnextflow pull nf-core/ampliseq\n</code></pre>"},{"location":"eDNA/01-Metabarcoding%20ampliseq%2012S%20MiFish/","title":"Metabarcoding workflow for 12S amplicon sequencing with MiFish primers","text":"<p>page details in progress. </p> <p>The 12S rRNA gene region of the mitogenome is ~950 bp. There are two popular primer sets to amplify two different regions of 12S: Riaz and MiFish. The following workflow includes script specific to the MiFish Universal (U) or Elasmobranch (E) primer set.</p> <p></p> <p>Citation: Miya et al. 2015</p> <p>The MiFish-U and MiFish-E primers are two variants of universal PCR primers developed for metabarcoding environmental DNA (eDNA) from fishes. Here are the key differences between them: - Target species: MiFish-U (Universal) is designed to amplify DNA from a wide range of bony fishes (Osteichthyes). MiFish-E (Elasmobranch) is specifically optimized for cartilaginous fishes like sharks and rays (Elasmobranchii).  - Primer sequences: While both primer sets target the mitochondrial 12S rRNA gene, they have slightly different nucleotide sequences to accommodate the genetic variations between bony and cartilaginous fishes. - Amplicon length: MiFish-U typically produces amplicons around 170-180 base pairs long and MiFish-E amplicons are usually slightly shorter, around 160-170 base pairs. </p> <p>Scripts to run: </p> <ol> <li>00-fastqc.sh   </li> <li>00-multiqc.sh  </li> <li>01a-metadata.R</li> <li>01b-ampliseq.sh</li> <li>02-taxonomicID.sh  </li> </ol>"},{"location":"eDNA/01-Metabarcoding%20ampliseq%2012S%20MiFish/#step-1-conda-environment-fisheries-edna","title":"Step 1: Conda environment: Fisheries eDNA","text":"<p>Background information on Conda: https://docs.conda.io/projects/conda/en/latest/user-guide/getting-started.html. </p> <p>GMGI Fisheries has a conda environment set-up with all the packages needed for this workflow. Code below was used to create this conda environment. DO NOT REPEAT every time user is running this workflow.</p> <pre><code># Activate conda\nsource ~/../../work/gmgi/miniconda3/bin/activate\n\n# Creating conda \nconda create --name fisheries_eDNA\n\n# Installing packages needed for this workflow \nconda install -c bioconda fastqc \nconda install multiqc \nconda install bioconda::nextflow \nconda install conda-forge::singularity\nconda install bioconda::blast\nconda install nextflow\n</code></pre> <p>The conda environment is started within each slurm script, but to activate conda environment outside of the slurm script to update packages or check what is installed:</p> <pre><code># Activate conda\nsource ~/../../work/gmgi/miniconda3/bin/activate\n\n# Activate fisheries eDNA conda environment \nconda activate fisheries_eDNA\n\n# List all available environments \nconda env list \n\n# List all packages installed in fisheries_eDNA\nconda list \n\n# Update a package\nconda update [package name]\n\n# Update nextflow ampliseq workflow \nnextflow pull nf-core/ampliseq\n</code></pre>"},{"location":"eDNA/01-Metabarcoding%20ampliseq%2012S%20MiFish/#step-2-assess-quality-of-raw-data","title":"Step 2: Assess quality of raw data","text":"<p>Background information on FASTQC: https://hbctraining.github.io/Intro-to-rnaseq-hpc-salmon-flipped/lessons/05_qc_running_fastqc_interactively.html. </p> <p><code>00-fastqc.sh</code>: </p> <pre><code>#!/bin/bash\n#SBATCH --error=output/fastqc_output/\"%x_error.%j\" #if your job fails, the error report will be put in this file\n#SBATCH --output=output/fastqc_output/\"%x_output.%j\" #once your job is completed, any final job report comments will be put in this file\n#SBATCH --partition=short\n#SBATCH --nodes=1\n#SBATCH --time=5:00:00\n#SBATCH --job-name=fastqc\n#SBATCH --mem=3GB\n#SBATCH --ntasks=24\n#SBATCH --cpus-per-task=2\n\n### USER TO-DO ### \n## 1. Set paths for your project\n\n# Activate conda environment\nsource ~/../../work/gmgi/miniconda3/bin/activate\nconda activate fisheries_eDNA\n\n## SET PATHS \nraw_path=\"\" \nout_dir=\"\" \n\n## CREATE SAMPLE LIST FOR SLURM ARRAY\n### 1. Create list of all .gz files in raw data path\nls -d ${raw_path}/*.gz &gt; ${raw_path}/rawdata\n\n### 2. Create a list of filenames based on that list created in step 1\nmapfile -t FILENAMES &lt; ${raw_path}/rawdata\n\n### 3. Create variable i that will assign each row of FILENAMES to a task ID\ni=${FILENAMES[$SLURM_ARRAY_TASK_ID]}\n\n## RUN FASTQC PROGRAM \nfastqc ${i} --outdir ${out_dir}\n</code></pre> <p>To run:   - Start slurm array (e.g., with 138 files) = <code>sbatch --array=0-136 00-fastqc.sh</code>.</p> <p>Notes: - This is going to output many error and output files. After job completes, use <code>cat *output.* &gt; ../fastqc_output.txt</code> to create one file with all the output and <code>cat *error.* &gt; ../fastqc_error.txt</code> to create one file with all of the error message outputs.  - Within the <code>out_dir</code> output folder, use <code>ls *html | wc</code> to count the number of html output files (1st/2nd column values). This should be equal to the --array range used and the number of raw data files. If not, the script missed some input files so address this before moving on.  </p>"},{"location":"eDNA/01-Metabarcoding%20ampliseq%2012S%20MiFish/#step-3-visualize-quality-of-raw-data","title":"Step 3: Visualize quality of raw data","text":"<p>Background information on MULTIQC: https://multiqc.info/docs/#:~:text=MultiQC%20is%20a%20reporting%20tool%20that%20parses%20results,experiments%20containing%20multiple%20samples%20and%20multiple%20analysis%20steps.</p> <p><code>00-multiqc.sh</code> </p> <pre><code>#!/bin/bash\n#SBATCH --error=output/\"%x_error.%j\" #if your job fails, the error report will be put in this file\n#SBATCH --output=output/\"%x_output.%j\" #once your job is completed, any final job report comments will be put in this file\n#SBATCH --partition=short\n#SBATCH --nodes=1\n#SBATCH --time=10:00:00\n#SBATCH --job-name=multiqc\n#SBATCH --mem=8GB\n#SBATCH --ntasks=24\n#SBATCH --cpus-per-task=2\n\n### USER TO-DO ### \n## 1. Set paths for your project\n## 2. Optional: change file name (multiqc_raw.html) as desired\n\n# Activate conda environment\nsource ~/../../work/gmgi/miniconda3/bin/activate\nconda activate fisheries_eDNA\n\n## SET PATHS \n## fastqc_output = output from 00-fastqc.sh; fastqc program\nfastqc_output=\"\"\nmultiqc_dir=\"\"\n\n## RUN MULTIQC \nmultiqc --interactive ${fastqc_output} -o ${multiqc_dir} --filename multiqc_raw.html\n</code></pre> <p>To run: - <code>sbatch 00-multiqc.sh</code> </p> <p>Notes: - Depending on the number of files per project, multiqc can be quick to run without a slurm script. To do this, run each line separately in the command line after activating the conda environment.  </p>"},{"location":"eDNA/01-Metabarcoding%20ampliseq%2012S%20MiFish/#step-4-nf-coreampliseq","title":"Step 4: nf-core/ampliseq","text":"<p>Nf-core: A community effort to collect a curated set of analysis pipelines built using Nextflow. Nextflow: scalable and reproducible scientific workflows using software containers, used to build wrapper programs like the one we use here.  </p> <p>https://nf-co.re/ampliseq/2.11.0: nfcore/ampliseq is a bioinformatics analysis pipeline used for amplicon sequencing, supporting denoising of any amplicon and supports a variety of taxonomic databases for taxonomic assignment including 16S, ITS, CO1 and 18S. </p> <p></p> <p>We use ampliseq for the following programs: - Cutadapt is trimming primer sequences from sequencing reads. Primer sequences are non-biological sequences that often introduce point mutations that do not reflect sample sequences. This is especially true for degenerated PCR primer. If primer trimming would be omitted, artifactual amplicon sequence variants might be computed by the denoising tool or sequences might be lost due to become labelled as PCR chimera. - DADA2 performs fast and accurate sample inference from amplicon data with single-nucleotide resolution. It infers exact amplicon sequence variants (ASVs) from amplicon data with fewer false positives than many other methods while maintaining high sensitivity.  </p> <p>We skip the taxonomic assignment because we use 3-db approach described in the next section. </p>"},{"location":"eDNA/01-Metabarcoding%20ampliseq%2012S%20MiFish/#12s-primer-sequences-required","title":"12S primer sequences (required)","text":"<p>Below is what we used for 12S amplicon sequencing. Ampliseq will automatically calculate and include the reverse compliment sequence. </p> <p>MiFish-U 12S amplicon F: GTCGGTAAAACTCGTGCCAGC MiFish-U 12S amplicon R: CATAGTGGGGTATCTAATCCCAGTTTG       </p> <p>MiFish-E 12S amplicon F: GTTGGTAAATCTCGTGCCAGC   MiFish-E 12S amplicon R: CATAGTGGGGTATCTAATCCTAGTTTG    </p>"},{"location":"eDNA/01-Metabarcoding%20ampliseq%2012S%20MiFish/#metadata-sheet-optional","title":"Metadata sheet (optional)","text":"<p>The metadata file has to follow the QIIME2 specifications (https://docs.qiime2.org/2021.2/tutorials/metadata/). Below is a preview of the sample sheet used for this test. Keep the column headers the same for future use. The first column needs to be \"ID\" and can only contain numbers, letters, or \"-\". This is different than the sample sheet. NAs should be empty cells rather than \"NA\". </p>"},{"location":"eDNA/01-Metabarcoding%20ampliseq%2012S%20MiFish/#create-samplesheet-sheet-for-ampliseq","title":"Create samplesheet sheet for ampliseq","text":"<p>This file indicates the sample ID and the path to R1 and R2 files. Below is a preview of the sample sheet used in this test. File created on RStudio Interactive on Discovery Cluster using (<code>create_metadatasheets.R</code>).  </p> <ul> <li>sampleID (required): Unique sample IDs, must start with a letter, and can only contain letters, numbers or underscores (no hyphons!).  </li> <li>forwardReads (required): Paths to (forward) reads zipped FastQ files  </li> <li>reverseReads (optional): Paths to reverse reads zipped FastQ files, required if the data is paired-end  </li> <li>run (optional): If the data was produced by multiple sequencing runs, any string  </li> </ul> sampleID forwardReads reverseReads run sample1 ./data/S1_R1_001.fastq.gz ./data/S1_R2_001.fastq.gz A sample2 ./data/S2_fw.fastq.gz ./data/S2_rv.fastq.gz A sample3 ./S4x.fastq.gz ./S4y.fastq.gz B sample4 ./a.fastq.gz ./b.fastq.gz B <p>This is an R script, not slurm script. Open RStudio interactive on Discovery Cluster to run this script.</p> <p>Prior to running R script, use the <code>rawdata</code> file created for the fastqc slurm array from within the raw data folder to create a list of files. Below is an example from our Offshore Wind project but the specifics of the sampleID will be project dependent. This project had four sequencing runs with different file names. </p> <p><code>01a-metadata.R</code></p> <pre><code>## Load libraries \n\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(strex) \n\n### Read in sample sheet \n\nsample_list &lt;- read.delim2(\"/work/gmgi/Fisheries/eDNA/offshore_wind2023/raw_data/rawdata\", header=F) %&gt;% \n  dplyr::rename(forwardReads = V1) %&gt;%\n  mutate(sampleID = str_after_nth(forwardReads, \"data/\", 1),\n         sampleID = str_before_nth(sampleID, \"_R\", 1))\n\n# creating sample ID \nsample_list$sampleID &lt;- gsub(\"-\", \"_\", sample_list$sampleID)\n\n# keeping only rows with R1\nsample_list &lt;- filter(sample_list, grepl(\"R1\", forwardReads, ignore.case = TRUE))\n\n# duplicating column \nsample_list$reverseReads &lt;- sample_list$forwardReads\n\n# replacing R1 with R2 in only one column \nsample_list$reverseReads &lt;- gsub(\"R1\", \"R2\", sample_list$reverseReads)\n\n# rearranging columns \nsample_list &lt;- sample_list[,c(2,1,3)]\n\nsample_list %&gt;% write.csv(\"/work/gmgi/Fisheries/eDNA/offshore_wind2023/metadata/samplesheet.csv\", \n                          row.names=FALSE, quote = FALSE)\n</code></pre>"},{"location":"eDNA/01-Metabarcoding%20ampliseq%2012S%20MiFish/#run-nf-coreampliseq-cutadapt-dada2","title":"Run nf-core/ampliseq (Cutadapt &amp; DADA2)","text":"<p>Update ampliseq workflow if needed: <code>nextflow pull nf-core/ampliseq</code>. </p> <p><code>01b-ampliseq.sh</code>:</p> <pre><code>#!/bin/bash\n#SBATCH --error=output/\"%x_error.%j\" #if your job fails, the error report will be put in this file\n#SBATCH --output=output/\"%x_output.%j\" #once your job is completed, any final job report comments will be put in this file\n#SBATCH --partition=short\n#SBATCH --nodes=1\n#SBATCH --time=20:00:00\n#SBATCH --job-name=ampliseq\n#SBATCH --mem=70GB\n#SBATCH --ntasks=24\n#SBATCH --cpus-per-task=2\n\n### USER TO-DO ### \n## 1. Set paths for project \n## 2. Adjust SBATCH options above (time, mem, ntasks, etc.) as desired  \n## 3. Fill in F and R primer information (no reverse compliment)\n## 4. Adjust parameters as needed (below is Fisheries team default for 12S)\n\n# LOAD MODULES\n# module load singularity/3.10.3\n# module load nextflow/23.10.1\n\n# Activate conda environment\nsource ~/../../work/gmgi/miniconda3/bin/activate\nconda activate fisheries_eDNA\n\n# SET PATHS \nmetadata=\"\" \noutput_dir=\"\"\n\nnextflow run nf-core/ampliseq -resume \\\n   -profile singularity \\\n   --input ${metadata}/samplesheet.csv \\\n   --FW_primer \"\" \\\n   --RV_primer \"\" \\\n   --outdir ${output_dir} \\\n   --trunclenf 100 \\\n   --trunclenr 100 \\\n   --trunc_qmin 25 \\\n   --max_len 200 \\\n   --max_ee 2 \\\n   --min_len_asv 100 \\\n   --max_len_asv 115 \\\n   --sample_inference pseudo \\\n   --skip_taxonomy \\\n   --ignore_failed_trimming\n</code></pre> <p>To run:  - <code>sbatch 01b-ampliseq.sh</code> </p>"},{"location":"eDNA/01-Metabarcoding%20ampliseq%2012S%20MiFish/#files-generated-by-ampliseq","title":"Files generated by ampliseq","text":"<p>Pipeline summary reports: - <code>summary_report/</code> - <code>summary_report.html</code>: pipeline summary report as standalone HTML file that can be viewed in your web browser. - <code>*.svg*</code>: plots that were produced for (and are included in) the report. - <code>versions.yml</code>: software versions used to produce this report.</p> <p>Preprocessing: - FastQC: <code>fastqc/</code> and <code>*_fastqc.html</code>: FastQC report containing quality metrics for your untrimmed raw fastq files. - Cutadapt: <code>cutadapt/</code> and <code>cutadapt_summary.tsv</code>: summary of read numbers that pass cutadapt - MultiQC: <code>multiqc</code>, <code>multiqc_data/</code>, <code>multiqc_plots/</code> with <code>multiqc_report.html</code>: a standalone HTML file that can be viewed in your web browser; </p> <p>ASV inferrence with DADA2: - <code>dada2/</code>, <code>dada2/args/</code>, <code>data2/log/</code>     - <code>ASV_seqs.fasta</code>: Fasta file with ASV sequences.    - <code>ASV_table.tsv</code>: Counts for each ASV sequence.    - <code>DADA2_stats.tsv</code>: Tracking read numbers through DADA2 processing steps, for each sample.    - <code>DADA2_table.rds</code>: DADA2 ASV table as R object.    - <code>DADA2_table.tsv</code>: DADA2 ASV table. - <code>dada2/QC/</code>    - <code>*.err.convergence.txt</code>: Convergence values for DADA2's dada command, should reduce over several magnitudes and approaching 0.    - <code>*.err.pdf</code>: Estimated error rates for each possible transition. The black line shows the estimated error rates after convergence of the machine-learning algorithm. The red line shows the error rates expected under the nominal definition of the Q-score. The estimated error rates (black line) should be a good fit to the observed rates (points), and the error rates should drop with increased quality.    - <code>*_qual_stats.pdf</code>: Overall read quality profiles: heat map of the frequency of each quality score at each base position. The mean quality score at each position is shown by the green line, and the quartiles of the quality score distribution by the orange lines. The red line shows the scaled proportion of reads that extend to at least that position.    - <code>*_preprocessed_qual_stats.pdf</code>: Same as above, but after preprocessing.  </p> <p>We add an ASV length filter that will output <code>asv_length_filter/</code> with: - <code>ASV_seqs.len.fasta</code>: Fasta file with filtered ASV sequences. - <code>ASV_table.len.tsv</code>: Counts for each filtered ASV sequence. - <code>ASV_len_orig.tsv</code>: ASV length distribution before filtering. - <code>ASV_len_filt.tsv</code>: ASV length distribution after filtering. - <code>stats.len.tsv</code>: Tracking read numbers through filtering, for each sample.  </p>"},{"location":"eDNA/01-Metabarcoding%20ampliseq%2012S%20MiFish/#step-5-blast-asv-sequences-output-from-dada2-against-our-3-databases","title":"Step 5: Blast ASV sequences (output from DADA2) against our 3 databases","text":""},{"location":"eDNA/01-Metabarcoding%20ampliseq%2012S%20MiFish/#populating-workgmgidatabases-folder","title":"Populating /work/gmgi/databases folder","text":"<p>We use NCBI, Mitofish, and GMGI-12S databases. </p>"},{"location":"eDNA/01-Metabarcoding%20ampliseq%2012S%20MiFish/#download-nbci","title":"Download NBCI","text":"<p>Option 1: Download ncbi-blast+ to <code>/work/gmgi/packages</code> using <code>wget ftp://ftp.ncbi.nlm.nih.gov/blast/executables/blast+/LATEST/ncbi-blast-2.16.0+-x64-linux.tar.gz</code> and then <code>tar -zxvf ncbi-blast-2.16.0+-x64-linux.tar.gz</code>. NCBI latest: https://ftp.ncbi.nlm.nih.gov/blast/executables/blast+/LATEST/. Once downloaded, user does not need to repeat this. I was struggling with the remote flag here within a slurm script.</p> <p>Option 2: Download 12S sequences from NCBI via CRABS: Creating Reference databases for Amplicon-Based Sequencing.</p> <p>CRABS requires python 3.9 so I created a new conda environment with this version: <code>conda create --name Env_py3.9 python=3.9</code>. Fisheries eDNA uses python 3.12. I was struggling to download CRABS in either conda environment... come back to this.</p> <p>https://github.com/gjeunen/reference_database_creator</p> <pre><code>conda activate Env_py3.9\nconda install -c bioconda crabs\n\ncd /work/gmgi/databases/12S/ncbi\n\ncrabs db_download --source ncbi --database nucleotide --query '12S[All Fields]' --output 12S_ncbi_[date].fasta --keep_original no --batchsize 5000\n\nmakeblastdb -in 12S_ncbi_[date].fasta -dbtype nucl -out 12S_ncbi_[date] -parse_seqids\n</code></pre> <p>Option 3: <code>blast</code> package is downloaded in the fisheries_eDNA conda environment with <code>conda install blast</code>. Install nt database <code>update_blastdb.pl --decompress nt</code> once inside the <code>work/gmgi/databases/ncbi/nt</code> folder. This is not ideal because it will take up more space and need to be updated.</p>"},{"location":"eDNA/01-Metabarcoding%20ampliseq%2012S%20MiFish/#download-mitofish","title":"Download Mitofish","text":"<p>Check Mitofish webpage (https://mitofish.aori.u-tokyo.ac.jp/download/) for the most recent database version number. Compare to the <code>work/gmgi/databases/12S/reference_fasta/12S/Mitofish/</code> folder. If needed, update Mitofish database:</p> <pre><code>## download db \nwget https://mitofish.aori.u-tokyo.ac.jp/species/detail/download/?filename=download%2F/complete_partial_mitogenomes.zip  \n\n## unzip \nunzip 'index.html?filename=download%2F%2Fcomplete_partial_mitogenomes.zip'\n\n## clean headers \nawk '/^&gt;/ {print $1} !/^&gt;/ {print}' mito-all &gt; Mitofish_v4.05.fasta\n\n## remove excess files \nrm mito-all* \nrm index*\n\n## make NCBI db \n## make sure fisheries_eDNA conda environment is activated or module load ncbi-blast+/2.13.0\nmakeblastdb -in Mitofish_v4.02.fasta -dbtype nucl -out Mitofish_v4.02.fasta -parse_seqids\n</code></pre> <p>Alternate option: Download Mitofish db with CRABS. This program and will download and format the db accordingly.   </p> <pre><code>git clone https://github.com/gjeunen/reference_database_creator.git\n\n## Download Mitofish \ncrabs db_download --source mitofish --output /work/gmgi/databases/12S/Mitofish/mitofish.fasta --keep_original yes\n### I couldn't get the function crabs to work \n</code></pre>"},{"location":"eDNA/01-Metabarcoding%20ampliseq%2012S%20MiFish/#running-taxonomic-id-script","title":"Running taxonomic ID script","text":"<p><code>02-taxonomicID.sh</code>: </p> <pre><code>#!/bin/bash\n#SBATCH --error=output/\"%x_error.%j\" #if your job fails, the error report will be put in this file\n#SBATCH --output=output/\"%x_output.%j\" #once your job is completed, any final job report comments will be put in this file\n#SBATCH --partition=short\n#SBATCH --nodes=1\n#SBATCH --time=20:00:00\n#SBATCH --job-name=tax_ID\n#SBATCH --mem=30GB\n#SBATCH --ntasks=24\n#SBATCH --cpus-per-task=2\n\n### USER TO-DO ### \n## 1. Set paths for project; change db path if not 12S\n\n## LOAD MODULES \n## can use module on NU cluster or own ncbi-blast+\n# module load ncbi-blast+/2.13.0\nncbi_program=\"/work/gmgi/packages/ncbi-blast-2.16.0+\"\n\n# Activate conda environment\nsource ~/../../work/gmgi/miniconda3/bin/activate\nconda activate fisheries_eDNA\n\n# SET PATHS \nASV_fasta=\"\"\nout=\"\"\n\ngmgi=\"/work/gmgi/databases/12S/GMGI\"\nmito=\"/work/gmgi/databases/12S/Mitofish\"\ntaxonkit=\"/work/gmgi/databases/taxonkit\"\n\n#### DATABASE QUERY ####\n### NCBI database \nblastn -remote -db nt \\\n   -query ${ASV_fasta}/ASV_seqs.len.fasta \\\n   -out ${out}/BLASTResults_NCBI.txt \\\n   -max_target_seqs 10 -perc_identity 100 -qcov_hsp_perc 95 \\\n   -outfmt '6  qseqid   sseqid   sscinames   staxid pident   length   mismatch gapopen  qstart   qend  sstart   send  evalue   bitscore' \\\n   -verbose\n\n## Mitofish database \nblastn -db ${mito}/*.fasta \\\n   -query ${ASV_fasta}/ASV_seqs.len.fasta \\\n   -out ${out}/BLASTResults_Mito.txt \\\n   -max_target_seqs 10 -perc_identity 100 -qcov_hsp_perc 95 \\\n   -outfmt '6  qseqid   sseqid  pident   length   mismatch gapopen  qstart   qend  sstart   send  evalue   bitscore'\n\n############################\n\n#### TAXONOMIC CLASSIFICATION #### \n## creating list of staxids from all three files \nawk -F $'\\t' '{ print $4}' ${out}/BLASTResults_NCBI.txt | sort -u &gt; ${out}/NCBI_sp.txt\n\n## annotating taxid with full taxonomic classification\ncat ${out}/NCBI_sp.txt | ${taxonkit}/taxonkit reformat -I 1 -r \"Unassigned\" &gt; ${out}/NCBI_taxassigned.txt\n</code></pre> <p>To run: - <code>sbatch 02-taxonomicID.sh</code> </p>"},{"location":"eDNA/01-Metabarcoding%20ampliseq%2012S%20Riaz/","title":"Metabarcoding workflow for 12S amplicon sequencing with Riaz primers","text":"<p>page details in progress. </p> <p>The 12S rRNA gene region of the mitogenome is ~950 bp. There are two popular primer sets to amplify two different regions of 12S: Riaz and MiFish. The following workflow includes script specific to the Riaz primer set.</p> <p></p> <p>Citation: Riaz et al. 2011</p> <p>Workflow done on HPC. Scripts to run: </p> <ol> <li>00-fastqc.sh   </li> <li>00-multiqc.sh  </li> <li>01a-metadata.R</li> <li>01b-ampliseq.sh</li> <li>02-taxonomicID.sh  </li> </ol>"},{"location":"eDNA/01-Metabarcoding%20ampliseq%2012S%20Riaz/#step-1-confirm-conda-environment-is-available","title":"Step 1: Confirm conda environment is available","text":"<p>The conda environment is started within each slurm script, but to activate conda environment outside of the slurm script to update packages or check what is installed:</p> <pre><code># Activate conda\nsource ~/../../work/gmgi/miniconda3/bin/activate\n\n# Activate fisheries eDNA conda environment \nconda activate fisheries_eDNA\n\n# List all available environments \nconda env list \n\n# List all packages installed in fisheries_eDNA\nconda list \n\n# Update a package\nconda update [package name]\n\n# Update nextflow ampliseq workflow \nnextflow pull nf-core/ampliseq\n</code></pre>"},{"location":"eDNA/01-Metabarcoding%20ampliseq%2012S%20Riaz/#step-2-assess-quality-of-raw-data","title":"Step 2: Assess quality of raw data","text":"<p>Background information on FASTQC. </p> <p><code>00-fastqc.sh</code>: </p> <pre><code>#!/bin/bash\n#SBATCH --error=output/fastqc_output/\"%x_error.%j\" #if your job fails, the error report will be put in this file\n#SBATCH --output=output/fastqc_output/\"%x_output.%j\" #once your job is completed, any final job report comments will be put in this file\n#SBATCH --partition=short\n#SBATCH --nodes=1\n#SBATCH --time=20:00:00\n#SBATCH --job-name=fastqc\n#SBATCH --mem=3GB\n#SBATCH --ntasks=24\n#SBATCH --cpus-per-task=2\n\n### USER TO-DO ### \n## 1. Set paths for your project\n\n# Activate conda environment\nsource ~/../../work/gmgi/miniconda3/bin/activate\nconda activate fisheries_eDNA\n\n## SET PATHS \nraw_path=\"\"\nout_dir=\"\"\n\n## CREATE SAMPLE LIST FOR SLURM ARRAY\n### 1. Create list of all .gz files in raw data path\nls -d ${raw_path}/*.gz &gt; ${raw_path}/rawdata\n\n### 2. Create a list of filenames based on that list created in step 1\nmapfile -t FILENAMES &lt; ${raw_path}/rawdata\n\n### 3. Create variable i that will assign each row of FILENAMES to a task ID\ni=${FILENAMES[$SLURM_ARRAY_TASK_ID]}\n\n## RUN FASTQC PROGRAM \nfastqc ${i} --outdir ${out_dir}\n</code></pre> <p>To run:   - Start slurm array (e.g., with 138 files) = <code>sbatch --array=0-137 00-fastqc.sh</code>.</p> <p>Notes:  </p> <ul> <li>This is going to output many error and output files. After job completes, use <code>cat *output.* &gt; ../fastqc_output.txt</code> to create one file with all the output and <code>cat *error.* &gt; ../fastqc_error.txt</code> to create one file with all of the error message outputs. </li> <li>Within the <code>out_dir</code> output folder, use <code>ls *html | wc</code> to count the number of html output files (1st/2nd column values). This should be equal to the --array range used and the number of raw data files. If not, the script missed some input files so address this before moving on.  </li> </ul>"},{"location":"eDNA/01-Metabarcoding%20ampliseq%2012S%20Riaz/#step-3-visualize-quality-of-raw-data","title":"Step 3: Visualize quality of raw data","text":"<p>Background information on MULTIQC.</p> <p><code>00-multiqc.sh</code> </p> <pre><code>#!/bin/bash\n#SBATCH --error=output/\"%x_error.%j\" #if your job fails, the error report will be put in this file\n#SBATCH --output=output/\"%x_output.%j\" #once your job is completed, any final job report comments will be put in this file\n#SBATCH --partition=short\n#SBATCH --nodes=1\n#SBATCH --time=10:00:00\n#SBATCH --job-name=multiqc\n#SBATCH --mem=8GB\n#SBATCH --ntasks=24\n#SBATCH --cpus-per-task=2\n\n### USER TO-DO ### \n## 1. Set paths for your project\n## 2. Optional: change file name (multiqc_raw.html) as desired\n\n# Activate conda environment\nsource ~/../../work/gmgi/miniconda3/bin/activate\nconda activate fisheries_eDNA\n\n## SET PATHS \n## fastqc_output = output from 00-fastqc.sh; fastqc program\nfastqc_output=\"\" \nmultiqc_dir=\"\" \n\n## RUN MULTIQC \nmultiqc --interactive ${fastqc_output} -o ${multiqc_dir} --filename multiqc_raw.html\n</code></pre> <p>To run: - <code>sbatch 00-multiqc.sh</code> </p> <p>Notes:  </p> <ul> <li>Depending on the number of files per project, multiqc can be quick to run without a slurm script. To do this, run each line separately in the command line after activating the conda environment.  </li> </ul>"},{"location":"eDNA/01-Metabarcoding%20ampliseq%2012S%20Riaz/#step-4-nf-coreampliseq","title":"Step 4: nf-core/ampliseq","text":"<p>Nf-core: A community effort to collect a curated set of analysis pipelines built using Nextflow. Nextflow: scalable and reproducible scientific workflows using software containers, used to build wrapper programs like the one we use here.  </p> <p>[https://nf-co.re/ampliseq/2.11.0]: nfcore/ampliseq is a bioinformatics analysis pipeline used for amplicon sequencing, supporting denoising of any amplicon and supports a variety of taxonomic databases for taxonomic assignment including 16S, ITS, CO1 and 18S. </p> <p></p> <p>We use ampliseq for the following programs:  </p> <ul> <li>Cutadapt is trimming primer sequences from sequencing reads. Primer sequences are non-biological sequences that often introduce point mutations that do not reflect sample sequences. This is especially true for degenerated PCR primer. If primer trimming would be omitted, artifactual amplicon sequence variants might be computed by the denoising tool or sequences might be lost due to become labelled as PCR chimera.  </li> <li>DADA2 performs fast and accurate sample inference from amplicon data with single-nucleotide resolution. It infers exact amplicon sequence variants (ASVs) from amplicon data with fewer false positives than many other methods while maintaining high sensitivity.  </li> </ul> <p>We skip the taxonomic assignment because we use 3-db approach described in the next section. </p>"},{"location":"eDNA/01-Metabarcoding%20ampliseq%2012S%20Riaz/#12s-primer-sequences-required","title":"12S primer sequences (required)","text":"<p>Below is what we used for 12S amplicon sequencing. Ampliseq will automatically calculate the reverse compliment and include this for us.</p> <p>Riaz 12S amplicon F Original: ACTGGGATTAGATACCCC Riaz 12S amplicon F Degenerate: ACTGGGATTAGATACCCY    Riaz 12S amplicon R: TAGAACAGGCTCCTCTAG     </p>"},{"location":"eDNA/01-Metabarcoding%20ampliseq%2012S%20Riaz/#metadata-sheet-optional","title":"Metadata sheet (optional)","text":"<p>The metadata file has to follow the QIIME2 specifications. Below is a preview of the sample sheet used for this test. Keep the column headers the same for future use. The first column needs to be \"ID\" and can only contain numbers, letters, or \"-\". This is different than the sample sheet. NAs should be empty cells rather than \"NA\". </p>"},{"location":"eDNA/01-Metabarcoding%20ampliseq%2012S%20Riaz/#create-samplesheet-sheet-for-ampliseq","title":"Create samplesheet sheet for ampliseq","text":"<p>This file indicates the sample ID and the path to R1 and R2 files. Below is a preview of the sample sheet used in this test. File created on RStudio Interactive on Discovery Cluster using (<code>create_metadatasheets.R</code>).  </p> <ul> <li>sampleID (required): Unique sample IDs, must start with a letter, and can only contain letters, numbers or underscores (no hyphons!).  </li> <li>forwardReads (required): Paths to (forward) reads zipped FastQ files  </li> <li>reverseReads (optional): Paths to reverse reads zipped FastQ files, required if the data is paired-end  </li> <li>run (optional): If the data was produced by multiple sequencing runs, any string  </li> </ul> sampleID forwardReads reverseReads run sample1 ./data/S1_R1_001.fastq.gz ./data/S1_R2_001.fastq.gz A sample2 ./data/S2_fw.fastq.gz ./data/S2_rv.fastq.gz A sample3 ./S4x.fastq.gz ./S4y.fastq.gz B sample4 ./a.fastq.gz ./b.fastq.gz B <p>This is an R script, not slurm script. Open RStudio interactive on Discovery Cluster to run this script.</p> <p>Prior to running R script, use the <code>rawdata</code> file created for the fastqc slurm array from within the raw data folder to create a list of files. Below is an example from our Offshore Wind project but the specifics of the sampleID will be project dependent. This project had four sequencing runs with different file names. </p> <p><code>01a-metadata.R</code></p> <pre><code>## Load libraries \n\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(strex) \n\n### Read in sample sheet \n\nsample_list &lt;- read.delim2(\"/work/gmgi/Fisheries/eDNA/offshore_wind2023/raw_data/rawdata\", header=F) %&gt;% \n  dplyr::rename(forwardReads = V1) %&gt;%\n  mutate(sampleID = str_after_nth(forwardReads, \"data/\", 1),\n         sampleID = str_before_nth(sampleID, \"_R\", 1))\n\n# creating sample ID \nsample_list$sampleID &lt;- gsub(\"-\", \"_\", sample_list$sampleID)\n\n# keeping only rows with R1\nsample_list &lt;- filter(sample_list, grepl(\"R1\", forwardReads, ignore.case = TRUE))\n\n# duplicating column \nsample_list$reverseReads &lt;- sample_list$forwardReads\n\n# replacing R1 with R2 in only one column \nsample_list$reverseReads &lt;- gsub(\"R1\", \"R2\", sample_list$reverseReads)\n\n# rearranging columns \nsample_list &lt;- sample_list[,c(2,1,3)]\n\nsample_list %&gt;% write.csv(\"/work/gmgi/Fisheries/eDNA/offshore_wind2023/metadata/samplesheet.csv\", \n                          row.names=FALSE, quote = FALSE)\n</code></pre>"},{"location":"eDNA/01-Metabarcoding%20ampliseq%2012S%20Riaz/#run-nf-coreampliseq-cutadapt-dada2","title":"Run nf-core/ampliseq (Cutadapt &amp; DADA2)","text":"<p>Update ampliseq workflow if needed: <code>nextflow pull nf-core/ampliseq</code>. </p> <p><code>01b-ampliseq.sh</code>:</p> <pre><code>#!/bin/bash\n#SBATCH --error=output/\"%x_error.%j\" #if your job fails, the error report will be put in this file\n#SBATCH --output=output/\"%x_output.%j\" #once your job is completed, any final job report comments will be put in this file\n#SBATCH --partition=short\n#SBATCH --nodes=1\n#SBATCH --time=20:00:00\n#SBATCH --job-name=ampliseq\n#SBATCH --mem=70GB\n#SBATCH --ntasks=24\n#SBATCH --cpus-per-task=2\n\n### USER TO-DO ### \n## 1. Set paths for project \n## 2. Adjust SBATCH options above (time, mem, ntasks, etc.) as desired  \n## 3. Fill in F primer information based on primer type (no reverse compliment needed)\n## 4. Adjust parameters as needed (below is Fisheries team default for 12S)\n\n# LOAD MODULES\nmodule load singularity/3.10.3\nmodule load nextflow/23.10.1\n\n# SET PATHS \nmetadata=\"\" \noutput_dir=\"\"\n\nnextflow run nf-core/ampliseq -resume \\\n   -profile singularity \\\n   --input ${metadata}/samplesheet.csv \\\n   --FW_primer \"\" \\\n   --RV_primer \"TAGAACAGGCTCCTCTAG\" \\\n   --outdir ${output_dir} \\\n   --trunclenf 100 \\\n   --trunclenr 100 \\\n   --trunc_qmin 25 \\\n   --max_len 200 \\\n   --max_ee 2 \\\n   --min_len_asv 100 \\\n   --max_len_asv 115 \\\n   --sample_inference pseudo \\\n   --skip_taxonomy \\\n   --ignore_failed_trimming\n</code></pre> <p>To run:  - <code>sbatch 01b-ampliseq.sh</code> </p>"},{"location":"eDNA/01-Metabarcoding%20ampliseq%2012S%20Riaz/#files-generated-by-ampliseq","title":"Files generated by ampliseq","text":"<p>Pipeline summary reports:  </p> <ul> <li><code>summary_report/</code></li> <li><code>summary_report.html</code>: pipeline summary report as standalone HTML file that can be viewed in your web browser.</li> <li><code>*.svg*</code>: plots that were produced for (and are included in) the report.</li> <li><code>versions.yml</code>: software versions used to produce this report.</li> </ul> <p>Preprocessing:  </p> <ul> <li>FastQC: <code>fastqc/</code> and <code>*_fastqc.html</code>: FastQC report containing quality metrics for your untrimmed raw fastq files.  </li> <li>Cutadapt: <code>cutadapt/</code> and <code>cutadapt_summary.tsv</code>: summary of read numbers that pass cutadapt  </li> <li>MultiQC: <code>multiqc</code>, <code>multiqc_data/</code>, <code>multiqc_plots/</code> with <code>multiqc_report.html</code>: a standalone HTML file that can be viewed in your web browser; </li> </ul> <p>ASV inferrence with DADA2:  </p> <ul> <li><code>dada2/</code>, <code>dada2/args/</code>, <code>data2/log/</code> </li> <li><code>ASV_seqs.fasta</code>: Fasta file with ASV sequences.</li> <li><code>ASV_table.tsv</code>: Counts for each ASV sequence.</li> <li><code>DADA2_stats.tsv</code>: Tracking read numbers through DADA2 processing steps, for each sample.</li> <li><code>DADA2_table.rds</code>: DADA2 ASV table as R object.</li> <li><code>DADA2_table.tsv</code>: DADA2 ASV table.  </li> <li><code>dada2/QC/</code></li> <li><code>*.err.convergence.txt</code>: Convergence values for DADA2's dada command, should reduce over several magnitudes and approaching 0.  </li> <li><code>*.err.pdf</code>: Estimated error rates for each possible transition. The black line shows the estimated error rates after convergence of the machine-learning algorithm. The red line shows the error rates expected under the nominal definition of the Q-score. The estimated error rates (black line) should be a good fit to the observed rates (points), and the error rates should drop with increased quality.  </li> <li><code>*_qual_stats.pdf</code>: Overall read quality profiles: heat map of the frequency of each quality score at each base position. The mean quality score at each position is shown by the green line, and the quartiles of the quality score distribution by the orange lines. The red line shows the scaled proportion of reads that extend to at least that position.  </li> <li><code>*_preprocessed_qual_stats.pdf</code>: Same as above, but after preprocessing.  </li> </ul> <p>We add an ASV length filter that will output <code>asv_length_filter/</code> with:  </p> <ul> <li><code>ASV_seqs.len.fasta</code>: Fasta file with filtered ASV sequences.  </li> <li><code>ASV_table.len.tsv</code>: Counts for each filtered ASV sequence.  </li> <li><code>ASV_len_orig.tsv</code>: ASV length distribution before filtering.  </li> <li><code>ASV_len_filt.tsv</code>: ASV length distribution after filtering.  </li> <li><code>stats.len.tsv</code>: Tracking read numbers through filtering, for each sample.  </li> </ul>"},{"location":"eDNA/01-Metabarcoding%20ampliseq%2012S%20Riaz/#step-5-blast-asv-sequences-output-from-dada2-against-our-3-databases","title":"Step 5: Blast ASV sequences (output from DADA2) against our 3 databases","text":""},{"location":"eDNA/01-Metabarcoding%20ampliseq%2012S%20Riaz/#populating-workgmgidatabases-folder","title":"Populating /work/gmgi/databases folder","text":"<p>We use NCBI, Mitofish, and GMGI-12S databases. </p>"},{"location":"eDNA/01-Metabarcoding%20ampliseq%2012S%20Riaz/#download-andor-update-nbci-blast-nt-database","title":"Download and/or update NBCI blast nt database","text":"<p>NCBI is updated daily and therefore needs to be updated each time a project is analyzed. This is the not the most ideal method but we were struggling to get the <code>-remote</code> flag to work within slurm because I don't think NU slurm is connected to the internet? NU help desk was helping for awhile but we didn't get anywhere.</p> <p>Within <code>/work/gmgi/databases/ncbi</code>, there is a <code>update_nt.sh</code> script with the following code. To run <code>sbatch update_nt.sh</code>. This won't take long as it will check for updates rather than re-downloading every time. </p> <pre><code>#!/bin/bash\n#SBATCH --partition=short\n#SBATCH --nodes=1\n#SBATCH --time=24:00:00\n#SBATCH --job-name=update_ncbi_nt\n#SBATCH --mem=50G\n#SBATCH --output=%x_%j.out\n#SBATCH --error=%x_%j.err\n\n# Activate conda environment\nsource /work/gmgi/miniconda3/bin/activate fisheries_eDNA\n\n# Create output directory if it doesn't exist\ncd /work/gmgi/databases/ncbi/nt\n\n# Update BLAST nt database\nupdate_blastdb.pl --decompress nt\n\n# Print completion message\necho \"BLAST nt database update completed\"\n</code></pre> <p>View the <code>update_ncbi_nt.out</code> file to confirm the echo printed at the end.</p> <p>Emma is still troubleshooting the -remote flag to also avoid storing the nt db within our /work/gmgi folder. </p>"},{"location":"eDNA/01-Metabarcoding%20ampliseq%2012S%20Riaz/#download-andor-update-mitofish-database","title":"Download and/or update Mitofish database","text":"<p>Check Mitofish webpage for the most recent database version number. Compare to the <code>work/gmgi/databases/12S</code> folder. If needed, update Mitofish database:</p> <pre><code>## download db \nwget https://mitofish.aori.u-tokyo.ac.jp/species/detail/download/?filename=download%2F/complete_partial_mitogenomes.zip  \n\n## unzip \nunzip 'index.html?filename=download%2F%2Fcomplete_partial_mitogenomes.zip'\n\n## clean headers \nawk '/^&gt;/ {print $1} !/^&gt;/ {print}' mito-all &gt; Mitofish_v4.05.fasta\n\n## remove excess files \nrm mito-all* \nrm index*\n\n## make NCBI db \n## make sure fisheries_eDNA conda environment is activated or module load ncbi-blast+/2.13.0\nmakeblastdb -in Mitofish_v4.02.fasta -dbtype nucl -out Mitofish_v4.02.fasta -parse_seqids\n</code></pre>"},{"location":"eDNA/01-Metabarcoding%20ampliseq%2012S%20Riaz/#download-gmgi-12s","title":"Download GMGI 12S","text":"<p>This is our in-house GMGI database that will include version numbers. Check <code>/work/gmgi/databases/12S/GMGI/</code> for current uploaded version number and check our Box folder for the most recent version number. </p> <p>On OOD portal, click the Interactive Apps dropdown. Select Home Directory under the HTML Viewer section. Navigate to the <code>/work/gmgi/databases/12S/GMGI/</code> folder. In the top right hand corner of the portal, select Upload and add the most recent .fasta file from our Box folder. </p> <p>To create a blast db from this reference fasta file (if updated): </p> <pre><code>cd /work/gmgi/databases/12S/GMGI/ \n\n## make NCBI db \n## make sure fisheries_eDNA conda environment is activated \n### CHANGE THE VERSION NUMBER BELOW TO LATEST\nmakeblastdb -in GMGI_Vert_Ref_2024v1.fasta -dbtype nucl -out GMGI_Vert_Ref_2024v1.fasta\n</code></pre>"},{"location":"eDNA/01-Metabarcoding%20ampliseq%2012S%20Riaz/#running-taxonomic-id-script","title":"Running taxonomic ID script","text":"<p><code>02-taxonomicID.sh</code>: </p> <pre><code>#!/bin/bash\n#SBATCH --error=output/\"%x_error.%j\" #if your job fails, the error report will be put in this file\n#SBATCH --output=output/\"%x_output.%j\" #once your job is completed, any final job report comments will be put in this file\n#SBATCH --partition=short\n#SBATCH --nodes=1\n#SBATCH --time=20:00:00\n#SBATCH --job-name=tax_ID\n#SBATCH --mem=30GB\n#SBATCH --ntasks=24\n#SBATCH --cpus-per-task=2\n\n### USER TO-DO ### \n## 1. Set paths for project; change db path if not 12S\n\n# Activate conda environment\nsource /work/gmgi/miniconda3/bin/activate\nconda activate fisheries_eDNA\n\n# SET PATHS \nASV_fasta=\"\"\nout=\"\"\n\ngmgi=\"/work/gmgi/databases/12S/GMGI\"\nmito=\"/work/gmgi/databases/12S/Mitofish\"\nncbi=\"/work/gmgi/databases/ncbi/nt\"\ntaxonkit=\"/work/gmgi/databases/taxonkit\"\n\n#### DATABASE QUERY ####\n### NCBI database \nblastn -db ${ncbi}/\"nt\" \\\n   -query ${ASV_fasta}/ASV_seqs.len.fasta \\\n   -out ${out}/BLASTResults_NCBI.txt \\\n   -max_target_seqs 10 -perc_identity 100 -qcov_hsp_perc 95 \\\n   -outfmt '6  qseqid   sseqid   sscinames   staxid pident   length   mismatch gapopen  qstart   qend  sstart   send  evalue   bitscore' \\\n   -verbose\n\n## Mitofish database \nblastn -db ${mito}/*.fasta \\\n   -query ${ASV_fasta}/ASV_seqs.len.fasta \\\n   -out ${out}/BLASTResults_Mito.txt \\\n   -max_target_seqs 10 -perc_identity 100 -qcov_hsp_perc 95 \\\n   -outfmt '6  qseqid   sseqid  pident   length   mismatch gapopen  qstart   qend  sstart   send  evalue   bitscore'\n\n## GMGI database \nblastn -db ${gmgi}/*.fasta \\\n   -query ${ASV_fasta}/ASV_seqs.len.fasta \\\n   -out ${out}/BLASTResults_GMGI.txt \\\n   -max_target_seqs 10 -perc_identity 98 -qcov_hsp_perc 95 \\\n   -outfmt '6  qseqid   sseqid   pident   length   mismatch gapopen  qstart   qend  sstart   send  evalue   bitscore'\n\n############################\n\n#### TAXONOMIC CLASSIFICATION #### \n## creating list of staxids from all three files \nawk -F $'\\t' '{ print $4}' ${out}/BLASTResults_NCBI.txt | sort -u &gt; ${out}/NCBI_sp.txt\n\n## annotating taxid with full taxonomic classification\ncat ${out}/NCBI_sp.txt | ${taxonkit}/taxonkit reformat -I 1 -r \"Unassigned\" &gt; ${out}/NCBI_taxassigned.txt\n</code></pre> <p>To run: - <code>sbatch 02-taxonomicID.sh</code> </p>"},{"location":"eDNA/01-Metabarcoding%20ampliseq%20COI/","title":"Metabarcoding workflow for COI amplicon sequencing","text":"<p>The COI region is commonly used for metabarcoding practices and consequently there are many primer options to choose from. The Fisheries team at GMGI has optimized the Leray Geller set (outlined in red box below). Citation: Leray et al 2013.</p> <p>We primarily use this set for invertebrate targets and 12S for vertebrate communities. </p> <p></p> <p>Workflow done on HPC. Scripts to run: </p> <ol> <li>00-fastqc.sh   </li> <li>00-multiqc.sh  </li> <li>01a-metadata.R</li> <li>01b-ampliseq.sh</li> <li>02-taxonomicID.sh  </li> </ol>"},{"location":"eDNA/01-Metabarcoding%20ampliseq%20COI/#step-1-confirm-conda-environment-is-available","title":"Step 1: Confirm conda environment is available","text":"<p>The conda environment is started within each slurm script, but to activate conda environment outside of the slurm script to update packages or check what is installed:</p> <pre><code># Activate conda\nsource ~/../../work/gmgi/miniconda3/bin/activate\n\n# Activate fisheries eDNA conda environment \nconda activate fisheries_eDNA\n\n# List all available environments \nconda env list \n\n# List all packages installed in fisheries_eDNA\nconda list \n\n# Update a package\nconda update [package name]\n\n# Update nextflow ampliseq workflow \nnextflow pull nf-core/ampliseq\n</code></pre>"},{"location":"eDNA/01-Metabarcoding%20ampliseq%20COI/#step-2-assess-quality-of-raw-data","title":"Step 2: Assess quality of raw data","text":"<p>Background information on FASTQC. </p> <p><code>00-fastqc.sh</code>: </p> <pre><code>#!/bin/bash\n#SBATCH --error=output/fastqc_output/\"%x_error.%j\" #if your job fails, the error report will be put in this file\n#SBATCH --output=output/fastqc_output/\"%x_output.%j\" #once your job is completed, any final job report comments will be put in this file\n#SBATCH --partition=short\n#SBATCH --nodes=1\n#SBATCH --time=20:00:00\n#SBATCH --job-name=fastqc\n#SBATCH --mem=3GB\n#SBATCH --ntasks=24\n#SBATCH --cpus-per-task=2\n\n### USER TO-DO ### \n## 1. Set paths for your project\n\n# Activate conda environment\nsource ~/../../work/gmgi/miniconda3/bin/activate\nconda activate fisheries_eDNA\n\n## SET PATHS \nraw_path=\"\"\nout_dir=\"\"\n\n## CREATE SAMPLE LIST FOR SLURM ARRAY\n### 1. Create list of all .gz files in raw data path\nls -d ${raw_path}/*.gz &gt; ${raw_path}/rawdata\n\n### 2. Create a list of filenames based on that list created in step 1\nmapfile -t FILENAMES &lt; ${raw_path}/rawdata\n\n### 3. Create variable i that will assign each row of FILENAMES to a task ID\ni=${FILENAMES[$SLURM_ARRAY_TASK_ID]}\n\n## RUN FASTQC PROGRAM \nfastqc ${i} --outdir ${out_dir}\n</code></pre> <p>To run:   - Start slurm array (e.g., with 138 files) = <code>sbatch --array=0-137 00-fastqc.sh</code>.</p> <p>Notes:  </p> <ul> <li>This is going to output many error and output files. After job completes, use <code>cat *output.* &gt; ../fastqc_output.txt</code> to create one file with all the output and <code>cat *error.* &gt; ../fastqc_error.txt</code> to create one file with all of the error message outputs. </li> <li>Within the <code>out_dir</code> output folder, use <code>ls *html | wc</code> to count the number of html output files (1st/2nd column values). This should be equal to the --array range used and the number of raw data files. If not, the script missed some input files so address this before moving on.  </li> </ul>"},{"location":"eDNA/01-Metabarcoding%20ampliseq%20COI/#step-3-visualize-quality-of-raw-data","title":"Step 3: Visualize quality of raw data","text":"<p>Background information on MULTIQC.</p> <p><code>00-multiqc.sh</code> </p> <pre><code>#!/bin/bash\n#SBATCH --error=output/\"%x_error.%j\" #if your job fails, the error report will be put in this file\n#SBATCH --output=output/\"%x_output.%j\" #once your job is completed, any final job report comments will be put in this file\n#SBATCH --partition=short\n#SBATCH --nodes=1\n#SBATCH --time=10:00:00\n#SBATCH --job-name=multiqc\n#SBATCH --mem=8GB\n#SBATCH --ntasks=24\n#SBATCH --cpus-per-task=2\n\n### USER TO-DO ### \n## 1. Set paths for your project\n## 2. Optional: change file name (multiqc_raw.html) as desired\n\n# Activate conda environment\nsource ~/../../work/gmgi/miniconda3/bin/activate\nconda activate fisheries_eDNA\n\n## SET PATHS \n## fastqc_output = output from 00-fastqc.sh; fastqc program\nfastqc_output=\"\" \nmultiqc_dir=\"\" \n\n## RUN MULTIQC \nmultiqc --interactive ${fastqc_output} -o ${multiqc_dir} --filename multiqc_raw.html\n</code></pre> <p>To run: - <code>sbatch 00-multiqc.sh</code> </p> <p>Notes:  </p> <ul> <li>Depending on the number of files per project, multiqc can be quick to run without a slurm script. To do this, run each line separately in the command line after activating the conda environment.  </li> </ul>"},{"location":"eDNA/01-Metabarcoding%20ampliseq%20COI/#step-4-nf-coreampliseq","title":"Step 4: nf-core/ampliseq","text":"<p>Nf-core: A community effort to collect a curated set of analysis pipelines built using Nextflow. Nextflow: scalable and reproducible scientific workflows using software containers, used to build wrapper programs like the one we use here.  </p> <p>[https://nf-co.re/ampliseq/2.11.0]: nfcore/ampliseq is a bioinformatics analysis pipeline used for amplicon sequencing, supporting denoising of any amplicon and supports a variety of taxonomic databases for taxonomic assignment including 16S, ITS, CO1 and 18S. </p> <p></p> <p>We use ampliseq for the following programs:  </p> <ul> <li>Cutadapt is trimming primer sequences from sequencing reads. Primer sequences are non-biological sequences that often introduce point mutations that do not reflect sample sequences. This is especially true for degenerated PCR primer. If primer trimming would be omitted, artifactual amplicon sequence variants might be computed by the denoising tool or sequences might be lost due to become labelled as PCR chimera.  </li> <li>DADA2 performs fast and accurate sample inference from amplicon data with single-nucleotide resolution. It infers exact amplicon sequence variants (ASVs) from amplicon data with fewer false positives than many other methods while maintaining high sensitivity.  </li> </ul> <p>We skip the taxonomic assignment because we use 3-db approach described in the next section. </p>"},{"location":"eDNA/01-Metabarcoding%20ampliseq%20COI/#coi-primer-sequences-required","title":"COI primer sequences (required)","text":"<p>Below is what we used for COI amplicon sequencing. This results in ~313 bp expected ASV. </p> <p>LG COI amplicon F: GGWACWGGWTGAACWGTWTAYCCYCC     LG COI amplicon R: TAIACYTCIGGRTGICCRAARAAYCA       </p> <p>Ampliseq will automatically calculate the reverse compliment and include this for us.</p>"},{"location":"eDNA/01-Metabarcoding%20ampliseq%20COI/#metadata-sheet-optional","title":"Metadata sheet (optional)","text":"<p>The metadata file has to follow the QIIME2 specifications. Below is a preview of the sample sheet used for this test. Keep the column headers the same for future use. The first column needs to be \"ID\" and can only contain numbers, letters, or \"-\". This is different than the sample sheet. NAs should be empty cells rather than \"NA\". </p>"},{"location":"eDNA/01-Metabarcoding%20ampliseq%20COI/#create-samplesheet-sheet-for-ampliseq","title":"Create samplesheet sheet for ampliseq","text":"<p>This file indicates the sample ID and the path to R1 and R2 files. Below is a preview of the sample sheet used in this test. File created on RStudio Interactive on Discovery Cluster using (<code>create_metadatasheets.R</code>).  </p> <ul> <li>sampleID (required): Unique sample IDs, must start with a letter, and can only contain letters, numbers or underscores (no hyphons!).  </li> <li>forwardReads (required): Paths to (forward) reads zipped FastQ files  </li> <li>reverseReads (optional): Paths to reverse reads zipped FastQ files, required if the data is paired-end  </li> <li>run (optional): If the data was produced by multiple sequencing runs, any string  </li> </ul> sampleID forwardReads reverseReads run sample1 ./data/S1_R1_001.fastq.gz ./data/S1_R2_001.fastq.gz A sample2 ./data/S2_fw.fastq.gz ./data/S2_rv.fastq.gz A sample3 ./S4x.fastq.gz ./S4y.fastq.gz B sample4 ./a.fastq.gz ./b.fastq.gz B <p>This is an R script, not slurm script. Open RStudio interactive on Discovery Cluster to run this script.</p> <p>Prior to running R script, use the <code>rawdata</code> file created for the fastqc slurm array from within the raw data folder to create a list of files. Below is an example from our Offshore Wind project but the specifics of the sampleID will be project dependent. This project had four sequencing runs with different file names. </p> <p><code>01a-metadata.R</code></p> <pre><code>## Load libraries \n\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(strex) \n\n### Read in sample sheet \n\nsample_list &lt;- read.delim2(\"/work/gmgi/Fisheries/eDNA/offshore_wind2023/raw_data/rawdata\", header=F) %&gt;% \n  dplyr::rename(forwardReads = V1) %&gt;%\n  mutate(sampleID = str_after_nth(forwardReads, \"data/\", 1),\n         sampleID = str_before_nth(sampleID, \"_R\", 1))\n\n# creating sample ID \nsample_list$sampleID &lt;- gsub(\"-\", \"_\", sample_list$sampleID)\n\n# keeping only rows with R1\nsample_list &lt;- filter(sample_list, grepl(\"R1\", forwardReads, ignore.case = TRUE))\n\n# duplicating column \nsample_list$reverseReads &lt;- sample_list$forwardReads\n\n# replacing R1 with R2 in only one column \nsample_list$reverseReads &lt;- gsub(\"R1\", \"R2\", sample_list$reverseReads)\n\n# rearranging columns \nsample_list &lt;- sample_list[,c(2,1,3)]\n\nsample_list %&gt;% write.csv(\"/work/gmgi/Fisheries/eDNA/offshore_wind2023/metadata/samplesheet.csv\", \n                          row.names=FALSE, quote = FALSE)\n</code></pre>"},{"location":"eDNA/01-Metabarcoding%20ampliseq%20COI/#run-nf-coreampliseq-cutadapt-dada2","title":"Run nf-core/ampliseq (Cutadapt &amp; DADA2)","text":"<p>Update ampliseq workflow if needed: <code>nextflow pull nf-core/ampliseq</code>. </p> <p><code>01b-ampliseq.sh</code>:</p> <pre><code>#!/bin/bash\n#SBATCH --error=output/\"%x_error.%j\" #if your job fails, the error report will be put in this file\n#SBATCH --output=output/\"%x_output.%j\" #once your job is completed, any final job report comments will be put in this file\n#SBATCH --partition=short\n#SBATCH --nodes=1\n#SBATCH --time=20:00:00\n#SBATCH --job-name=ampliseq\n#SBATCH --mem=70GB\n#SBATCH --ntasks=24\n#SBATCH --cpus-per-task=2\n\n### USER TO-DO ### \n## 1. Set paths for project \n## 2. Adjust SBATCH options above (time, mem, ntasks, etc.) as desired  \n## 3. Fill in F primer information based on primer type (no reverse compliment needed)\n## 4. Adjust parameters as needed (below is Fisheries team default for COI)\n\n# LOAD MODULES\nmodule load singularity/3.10.3\nmodule load nextflow/23.10.1\n\n# SET PATHS \nmetadata=\"\" \noutput_dir=\"\"\n\nnextflow run nf-core/ampliseq -resume \\\n   -profile singularity \\\n   --input ${metadata}/samplesheet.csv \\\n   --FW_primer \"GGWACWGGWTGAACWGTWTAYCCYCC\" \\\n   --RV_primer \"TAIACYTCIGGRTGICCRAARAAYCA\" \\\n   --outdir ${output_dir} \\\n   --trunclenf 220 \\\n   --trunclenr 220 \\\n   --trunc_qmin 25 \\\n   --max_ee 2 \\\n   --min_len_asv 300 \\\n   --max_len_asv 330 \\\n   --sample_inference pseudo \\\n   --skip_taxonomy \\\n   --ignore_failed_trimming\n</code></pre> <p>Could add back in?     --max_len 200 \\</p> <p>To run:  - <code>sbatch 01b-ampliseq.sh</code> </p>"},{"location":"eDNA/01-Metabarcoding%20ampliseq%20COI/#files-generated-by-ampliseq","title":"Files generated by ampliseq","text":"<p>Pipeline summary reports:  </p> <ul> <li><code>summary_report/</code></li> <li><code>summary_report.html</code>: pipeline summary report as standalone HTML file that can be viewed in your web browser.</li> <li><code>*.svg*</code>: plots that were produced for (and are included in) the report.</li> <li><code>versions.yml</code>: software versions used to produce this report.</li> </ul> <p>Preprocessing:  </p> <ul> <li>FastQC: <code>fastqc/</code> and <code>*_fastqc.html</code>: FastQC report containing quality metrics for your untrimmed raw fastq files.  </li> <li>Cutadapt: <code>cutadapt/</code> and <code>cutadapt_summary.tsv</code>: summary of read numbers that pass cutadapt  </li> <li>MultiQC: <code>multiqc</code>, <code>multiqc_data/</code>, <code>multiqc_plots/</code> with <code>multiqc_report.html</code>: a standalone HTML file that can be viewed in your web browser; </li> </ul> <p>ASV inferrence with DADA2:  </p> <ul> <li><code>dada2/</code>, <code>dada2/args/</code>, <code>data2/log/</code> </li> <li><code>ASV_seqs.fasta</code>: Fasta file with ASV sequences.</li> <li><code>ASV_table.tsv</code>: Counts for each ASV sequence.</li> <li><code>DADA2_stats.tsv</code>: Tracking read numbers through DADA2 processing steps, for each sample.</li> <li><code>DADA2_table.rds</code>: DADA2 ASV table as R object.</li> <li><code>DADA2_table.tsv</code>: DADA2 ASV table.  </li> <li><code>dada2/QC/</code></li> <li><code>*.err.convergence.txt</code>: Convergence values for DADA2's dada command, should reduce over several magnitudes and approaching 0.  </li> <li><code>*.err.pdf</code>: Estimated error rates for each possible transition. The black line shows the estimated error rates after convergence of the machine-learning algorithm. The red line shows the error rates expected under the nominal definition of the Q-score. The estimated error rates (black line) should be a good fit to the observed rates (points), and the error rates should drop with increased quality.  </li> <li><code>*_qual_stats.pdf</code>: Overall read quality profiles: heat map of the frequency of each quality score at each base position. The mean quality score at each position is shown by the green line, and the quartiles of the quality score distribution by the orange lines. The red line shows the scaled proportion of reads that extend to at least that position.  </li> <li><code>*_preprocessed_qual_stats.pdf</code>: Same as above, but after preprocessing.  </li> </ul> <p>We add an ASV length filter that will output <code>asv_length_filter/</code> with:  </p> <ul> <li><code>ASV_seqs.len.fasta</code>: Fasta file with filtered ASV sequences.  </li> <li><code>ASV_table.len.tsv</code>: Counts for each filtered ASV sequence.  </li> <li><code>ASV_len_orig.tsv</code>: ASV length distribution before filtering.  </li> <li><code>ASV_len_filt.tsv</code>: ASV length distribution after filtering.  </li> <li><code>stats.len.tsv</code>: Tracking read numbers through filtering, for each sample.  </li> </ul>"},{"location":"eDNA/01-Metabarcoding%20ampliseq%20COI/#step-5-blast-asv-sequences-output-from-dada2-against-our-database-list","title":"Step 5: Blast ASV sequences (output from DADA2) against our database list","text":""},{"location":"eDNA/01-Metabarcoding%20ampliseq%20COI/#populating-workgmgidatabases-folder","title":"Populating /work/gmgi/databases folder","text":"<p>MIDORI: A database specifically for COI sequences  </p> <p>We use a program called COInr that downloads NCBI and BOLD (Barcode of Life Database) databases to create one database for comparison. COInr is already downloaded in the conda environment and pulls NCBI And BOLD directly. </p> <p></p> <p>Alternative to COInr program:  </p> <ul> <li>MARES (MARine Eukaryote Species) and github: This database combines sequences from both GenBank and BOLD to increase taxonomic coverage and confidence for marine eukaryotes   </li> </ul>"},{"location":"eDNA/01-Metabarcoding%20ampliseq%20COI/#download-andor-update-nbci-blast-nt-database","title":"Download and/or update NBCI blast nt database","text":"<p>NCBI is updated daily and therefore needs to be updated each time a project is analyzed. This is the not the most ideal method but we were struggling to get the <code>-remote</code> flag to work within slurm because I don't think NU slurm is connected to the internet? NU help desk was helping for awhile but we didn't get anywhere.</p> <p>Within <code>/work/gmgi/databases/ncbi</code>, there is a <code>update_nt.sh</code> script with the following code. To run <code>sbatch update_nt.sh</code>. This won't take long as it will check for updates rather than re-downloading every time. </p> <pre><code>#!/bin/bash\n#SBATCH --partition=short\n#SBATCH --nodes=1\n#SBATCH --time=24:00:00\n#SBATCH --job-name=update_ncbi_nt\n#SBATCH --mem=50G\n#SBATCH --output=%x_%j.out\n#SBATCH --error=%x_%j.err\n\n# Activate conda environment\nsource /work/gmgi/miniconda3/bin/activate fisheries_eDNA\n\n# Create output directory if it doesn't exist\ncd /work/gmgi/databases/ncbi/nt\n\n# Update BLAST nt database\nupdate_blastdb.pl --decompress nt\n\n# Print completion message\necho \"BLAST nt database update completed\"\n</code></pre> <p>View the <code>update_ncbi_nt.out</code> file to confirm the echo printed at the end.</p> <p>Emma is still troubleshooting the -remote flag to also avoid storing the nt db within our /work/gmgi folder. </p>"},{"location":"eDNA/01-Metabarcoding%20ampliseq%20COI/#download-andor-update-coinr-program","title":"Download and/or update COInr program","text":"<p>COInr database instructions. There are options to include custom sequences if needed.</p> <p>The latest version of BOLD is 2015 so this 2022 set is the most updated. Use our own NCBI as well to catch recent entries. </p> <pre><code>cd /work/gmgi/packages \ngit clone https://github.com/meglecz/mkCOInr.git\n\ncd /work/gmgi/databases/COI\nwget https://zenodo.org/record/6555985/files/COInr_2022_05_06.tar.gz\ntar -zxvf COInr_2022_05_06.tar.gz\nrm COInr_2022_05_06.tar.gz\nmv COInr_2022_05_06 COInr\n\n## converting database information for blast \nperl /work/gmgi/packages/mkCOInr/scripts/format_db.pl -tsv COInr/COInr.tsv -outfmt blast -outdir /work/gmgi/databases/COI/COInr -out COInr_blast\n</code></pre>"},{"location":"eDNA/01-Metabarcoding%20ampliseq%20COI/#download-andor-update-mares-program","title":"Download and/or update MARES program","text":"<p>MARES Github repo. Paper link. </p>"},{"location":"eDNA/01-Metabarcoding%20ampliseq%20COI/#download-andor-update-midori","title":"Download and/or update MIDORI","text":"<p>MIDORI webpage. MIDORI Reference pulls from GenBank</p> <p>Visit the MIDORI website to check for the most updated db. This folder is already formatted for blast searching so we don't need to create a blast formatted db. </p> <pre><code>cd /work/gmgi/databases/COI/MIDORI\n\n## download zip file from MIDORI website for CO1 sequences in BLAST format from nucleotide reference\nwget https://www.reference-midori.info/download/Databases/GenBank261_2024-06-15/BLAST/uniq/fasta/MIDORI2_UNIQ_NUC_GB261_CO1_BLAST.fasta.zip\nunzip MIDORI2_UNIQ_NUC_GB261_CO1_BLAST.fasta.zip \n\n## change notation if version is different \nmakeblastdb -in MIDORI2_UNIQ_NUC_GB261_CO1_BLAST.fasta -dbtype nucl -out MIDORI2_UNIQ_NUC_GB261_CO1_BLAST.fasta\n</code></pre>"},{"location":"eDNA/01-Metabarcoding%20ampliseq%20COI/#other-options-to-consider","title":"Other options to consider","text":"<ul> <li>Download BOLD directly? database webpage. You can search for particular orders, primer sets, etc.  </li> <li>MARES Github repo. Paper link. This looks similar to the COInr program? I'm nore sure which one is best to use. Trying the COInr first. </li> </ul>"},{"location":"eDNA/01-Metabarcoding%20ampliseq%20COI/#running-taxonomic-id-script","title":"Running taxonomic ID script","text":"<p><code>02-taxonomicID.sh</code>: </p> <pre><code>#!/bin/bash\n#SBATCH --error=output/\"%x_error.%j\" #if your job fails, the error report will be put in this file\n#SBATCH --output=output/\"%x_output.%j\" #once your job is completed, any final job report comments will be put in this file\n#SBATCH --partition=short\n#SBATCH --nodes=1\n#SBATCH --time=20:00:00\n#SBATCH --job-name=tax_ID\n#SBATCH --mem=30GB\n#SBATCH --ntasks=24\n#SBATCH --cpus-per-task=2\n\n### USER TO-DO ### \n## 1. Set paths for project; change db path if not 12S\n\n# Activate conda environment\nsource /work/gmgi/miniconda3/bin/activate fisheries_eDNA\n\n# SET PATHS \nASV_fasta=\"\"\nout=\"\"\n\nCOInr=\"/work/gmgi/databases/COI/COInr\"\nmidori=\"/work/gmgi/databases/COI/MIDORI\"\nncbi=\"/work/gmgi/databases/ncbi/nt\"\ntaxonkit=\"/work/gmgi/databases/taxonkit\"\n\n#### DATABASE QUERY ####\n### NCBI database \nblastn -db ${ncbi}/\"nt\" \\\n   -query ${ASV_fasta}/ASV_seqs.len.fasta \\\n   -out ${out}/BLASTResults_NCBI.txt \\\n   -max_target_seqs 20 -perc_identity 99 -qcov_hsp_perc 95 \\\n   -outfmt '6  qseqid   sseqid   sscinames   staxid pident   length   mismatch gapopen  qstart   qend  sstart   send  evalue   bitscore'\n\n## MIDORI database \nblastn -db ${midori}/*.fasta\" \\\n   -query ${ASV_fasta}/ASV_seqs.len.fasta \\\n   -out ${out}/BLASTResults_midori.txt \\\n   -max_target_seqs 10 -perc_identity 98 -qcov_hsp_perc 95 \\\n   -outfmt '6  qseqid   sseqid  pident   length   mismatch gapopen  qstart   qend  sstart   send  evalue   bitscore'\n\n## COInr database \nblastn -db ${COInr}/*.fasta \\\n   -query ${ASV_fasta}/ASV_seqs.len.fasta \\\n   -out ${out}/BLASTResults_COInr.txt \\\n   -max_target_seqs 10 -perc_identity 98 -qcov_hsp_perc 95 \\\n   -outfmt '6  qseqid   sseqid   pident   length   mismatch gapopen  qstart   qend  sstart   send  evalue   bitscore'\n\n############################\n\n#### TAXONOMIC CLASSIFICATION #### \n## creating list of staxids from all three files \nawk -F $'\\t' '{ print $4}' ${out}/BLASTResults_NCBI.txt | sort -u &gt; ${out}/NCBI_sp.txt\n\n## annotating taxid with full taxonomic classification\ncat ${out}/NCBI_sp.txt | ${taxonkit}/taxonkit reformat -I 1 -r \"Unassigned\" &gt; ${out}/NCBI_taxassigned.txt\n</code></pre> <p>To run: - <code>sbatch 02-taxonomicID.sh</code> </p>"},{"location":"eDNA/02-datatable_prep_12S_Riaz/","title":"Datatable preparation base script for eDNA metabarcoding","text":"<p>.Rmd script</p> <p>This script takes your Blast output from the GMGI database, Mitofish database, and NCBI database to create one datatable with read counts and taxonomic assignment.</p> <p>Workflow summary: 1. Load libraries 2. Load metadata 3. Load BLAST output from GMGI, Mitofish, and NCBI 4. Load DADA2 ASV Table 5. Taxonomic Assignment - 5a. Identify ASVs with multiple hits from GMGI\u2019s database - 5b. Identify entries that mismatch between GMGI, Mitofish, and NCBI databases - 5c. Assign taxonomy based on hierarchical approach - 5d. Edit taxonomy annotations based on mismatch table choices - 5e. Adjusting common name and category for those entries that don\u2019t have one (from Mito or NCBI) 6. Filtering: Filter ASV by less than 0.1% reads and then collapse by group 7. Collapsing read counts by species name 8. Creating results output</p>"},{"location":"eDNA/02-datatable_prep_12S_Riaz/#load-libraries","title":"Load libraries","text":"<pre><code>library(ggplot2) ## for plotting\nlibrary(dplyr) ## for data table manipulation\n</code></pre> <pre><code>## \n## Attaching package: 'dplyr'\n\n## The following objects are masked from 'package:stats':\n## \n##     filter, lag\n\n## The following objects are masked from 'package:base':\n## \n##     intersect, setdiff, setequal, union\n</code></pre> <pre><code>library(tidyr) ## for data table manipulation\nlibrary(readr) ## for reading in tsv files\nlibrary(readxl) ## for reading in excel files\nlibrary(stringr) ## for data transformation\nlibrary(strex) ## for data transformation\nlibrary(writexl) ## for excel output\nlibrary(purrr) ## for data transformation\nlibrary(funrar) ## for make_relative()\nlibrary(tidyverse) ## for data table manipulation\n</code></pre> <pre><code>## \u2500\u2500 Attaching core tidyverse packages \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 tidyverse 2.0.0 \u2500\u2500\n## \u2714 forcats   1.0.0     \u2714 tibble    3.2.1\n## \u2714 lubridate 1.9.3\n\n## \u2500\u2500 Conflicts \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 tidyverse_conflicts() \u2500\u2500\n## \u2716 dplyr::filter() masks stats::filter()\n## \u2716 dplyr::lag()    masks stats::lag()\n## \u2139 Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n</code></pre>"},{"location":"eDNA/02-datatable_prep_12S_Riaz/#metadata-input","title":"Metadata input","text":""},{"location":"eDNA/02-datatable_prep_12S_Riaz/#identify-paths-for-metadata-and-project-data","title":"Identify paths for metadata and project data","text":"<p>Each user needs to write in their specific directory outputs prior to the file name. The default working directory is this document so the folder where this script is saved for the user. To change the workign directory to the Rproject directory, select \u2018Knit\u2019 and \u2018Knit Directory\u2019 &gt; \u2018Project Directory\u2019.</p> <pre><code>### User edits:\n### 1. change paths of input and output as desired \n\n## GMGI Fish database\npath_GMGIdb = \"../../../../Projects/eDNA/Metabarcoding Lab Resources/Reference Databases/GMGI_Vert_Ref.xlsx\"\npath_fishbase_tax = \"../../../../Projects/eDNA/Metabarcoding Lab Resources/Reference Databases/taxonomic_classification_fishbase.csv\"\npath_mitofish_tax = \"../../../../Projects/eDNA/Metabarcoding Lab Resources/Reference Databases/taxonomic_classification_mitofish.csv\"\n\n## BLAST results\npath_blast_gmgi = \"example_input/BLASTResults_GMGI.txt\"\npath_blast_mito = \"example_input/BLASTResults_Mito.txt\"\npath_blast_ncbi_taxassigned = \"example_input/NCBI_taxassigned.txt\"\npath_blast_ncbi = \"example_input/BLASTResults_NCBI.txt\"\n\n## ASV table results \n## confirm that the ASV_table.len.tsv name is correct for user's project\npath_asv_table = \"example_input/ASV_table.len.tsv\"\npath_output_summary = \"example_input/overall_summary.tsv\"\n\n# output paths \npath_choice_required = \"example_output/Taxonomic_assignments/Choice_required_GMGI_multiplehits.xlsx\"\npath_disagree_list = \"example_output/Taxonomic_assignments/SampleReport_taxonomic_ID.xlsx\"\n\nresults_rawreads_matrix = \"example_output/Results_rawreads_matrix.xlsx\"\nresults_rawreads_long = \"example_output/Results_rawreads_long.xlsx\"\nresults_relab_matrix = \"example_output/Results_relab_matrix.xlsx\"\nresults_relab_long = \"example_output/Results_relab_long.xlsx\"\n</code></pre>"},{"location":"eDNA/02-datatable_prep_12S_Riaz/#load-project-metadata","title":"Load project metadata","text":"<p>Metadata specific to each project. This contains information about each sample (e.g., month, site, time, sample type, etc.). Confirm that sample IDs match those used in the ASV_table.len.tsv file.</p> <pre><code>### User edits:\n### 1. change path of metadata file\n\n## EXCEL\n# meta &lt;- read_excel(\"example_input/metadata.xlsx\")\n## CSV \nmeta &lt;- read.csv(\"example_input/metadata.csv\", header = TRUE)\n</code></pre>"},{"location":"eDNA/02-datatable_prep_12S_Riaz/#load-database-metadata","title":"Load database metadata","text":"<p>No user edits in this section because paths have already been set above.</p> <pre><code># Load GMGI database information (common name, species name, etc.)\ngmgi_db &lt;- read_xlsx(path_GMGIdb, sheet = 1) %&gt;% dplyr::rename(sseqid = Ref) %&gt;%\n  ## removing &gt; from beginning of entires within Ref column\n  mutate(sseqid = gsub(\"&gt;\", \"\", sseqid))\n</code></pre>"},{"location":"eDNA/02-datatable_prep_12S_Riaz/#blast-data-input","title":"BLAST data input","text":"<p>No user edits unless user changed blastn parameters from fisheries team default.</p> <pre><code>## Setting column header names and classes\nblast_col_headers = c(\"ASV_ID\", \"sseqid\", \"pident\", \"length\", \"mismatch\", \"gapopen\",\n                                        \"qstart\", \"qend\", \"sstart\", \"send\", \"evalue\", \"bitscore\")\nblast_col_classes = c(rep(\"character\", 2), rep(\"numeric\", 10))\n</code></pre>"},{"location":"eDNA/02-datatable_prep_12S_Riaz/#gmgi-database","title":"GMGI database","text":"<p>No user edits.</p> <pre><code>Blast_GMGI &lt;- read.table(path_blast_gmgi, header=F, col.names = blast_col_headers, colClasses = blast_col_classes) %&gt;%\n  ## blast changes spaces to hyphons so we need to change that back to match our metadata\n  mutate(sseqid = gsub(\"-\", \" \", sseqid)) %&gt;%\n  ## join with GMGI database information\n  left_join(., gmgi_db, by = \"sseqid\")\n\n## Check how many ASVs were identified with the GMGI Database\nlength(unique(Blast_GMGI$ASV_ID)) \n</code></pre> <pre><code>## [1] 61\n</code></pre>"},{"location":"eDNA/02-datatable_prep_12S_Riaz/#mitofish-database","title":"Mitofish database","text":"<p>No user edits.</p> <pre><code>Blast_Mito &lt;- read.table(path_blast_mito, header=F, col.names = blast_col_headers, colClasses = blast_col_classes) %&gt;%\n  # renaming sseqid to species name\n  dplyr::rename(Species_name = sseqid) %&gt;%\n\n  # replacing _ with spaces\n  mutate(Species_name = gsub(\"_\", \" \", Species_name))\n</code></pre>"},{"location":"eDNA/02-datatable_prep_12S_Riaz/#ncbi-database","title":"NCBI database","text":"<p>No user edits.</p> <pre><code>NCBI_taxassigned &lt;- read.delim2(path_blast_ncbi_taxassigned, header=F, col.names = c(\"staxid\", \"Phylo\")) %&gt;%\n  ## creating taxonomic assignment columns\n  separate(Phylo, c(\"Kingdom\", \"Phylum\", \"Class\", \"Order\", \"Family\", \"Genus\", \"Species_name\"), sep = \";\") %&gt;%\n  ## creating species column based on Species_name\n  mutate(., species = str_after_nth(Species_name, \" \", 1))\n\nBlast_NCBI &lt;- read.table(path_blast_ncbi, header=F,\n                           col.names = c(\"ASV_ID\", \"sseqid\", \"sscinames\", \"staxid\", \"pident\", \"length\", \"mismatch\",\n                                         \"gapopen\", \"qstart\", \"qend\", \"sstart\", \"send\", \"evalue\", \"bitscore\"),\n                           colClasses = c(rep(\"character\", 3), \"integer\", rep(\"numeric\", 9))) %&gt;%\n  left_join(., NCBI_taxassigned, by = \"staxid\")\n</code></pre>"},{"location":"eDNA/02-datatable_prep_12S_Riaz/#load-dada2-asv-table","title":"Load DADA2 ASV Table","text":"<p>The column headers will be the Sample IDs and the first column is the ASV ID. ASVs are given a \u201crank\u201d based on sum of reads from that ASV (pre-filtering). \u2018Random\u2019 indicates that if ASVs are tied, then the code will randomly assign a rank for those tied. Because we don\u2019t need an exact rank here, \u2018random\u2019 will do for a tie-breaker.</p> <p>No user edits.</p> <pre><code>ASV_table &lt;- read_tsv(path_asv_table, show_col_types = FALSE) %&gt;%\n  ## calculate the sum of all reads for each ASV\n  mutate(., ASV_sum = rowSums(across(where(is.numeric)))) %&gt;% \n\n  ## calculate a ranking based on those sum calculated above\n  mutate(ASV_rank = rank(-ASV_sum, ties.method='random')) %&gt;%\n\n  ## move the sum and rank columns to after ASV_ID and arrange by rank\n  relocate(c(ASV_sum,ASV_rank), .after = ASV_ID) %&gt;% arrange((ASV_rank))\n\n## creating list of rankings\nASV_rank_list &lt;- ASV_table %&gt;% dplyr::select(ASV_ID, ASV_sum, ASV_rank)\n</code></pre>"},{"location":"eDNA/02-datatable_prep_12S_Riaz/#taxonomic-assignment","title":"Taxonomic Assignment","text":"<p>Identifying where NCBI, Mito, and GMGI disagree on tax assignment. With the hierarchial approach, ASVs that match to GMGI and several other databases will only result in GMGI assignment. By reviewing this df, we can be sure we aren\u2019t missing an assignment in our GMGI curated database.</p> <p>Sub-workflow: 1. Identify any ASVs that contain multiple hits within the GMGI database. 2. Identify entries that mismatch between GMGI, Mitofish, and NCBI databases. 3. Assign taxonomy based on hierarchical approach. 4. Edit taxonomy annotations based on mismatch table. 5. Adjusting common name for those entries that don\u2019t have one (from Mito or GMGI).</p>"},{"location":"eDNA/02-datatable_prep_12S_Riaz/#identify-any-asvs-that-contain-multiple-hits-within-the-gmgi-database","title":"Identify any ASVs that contain multiple hits within the GMGI database","text":"<p>At this point, a fisheries team member needs to make choices about which taxonomic assignment to accept.</p>"},{"location":"eDNA/02-datatable_prep_12S_Riaz/#create-list-of-those-asvs-with-multiple-hits","title":"Create list of those ASVs with multiple hits","text":"<p>No user edits.</p> <pre><code>multiple_hit_choice &lt;- Blast_GMGI %&gt;% group_by(ASV_ID) %&gt;%\n  ## take top percent identity hit, count the number of top hits, and filter to those with more than 1 top hit \n  slice_max(pident, n=1) %&gt;% count() %&gt;% filter(n&gt;1) %&gt;%\n\n  ## adding BLAST_GMGI information with these ASVs and ASV rank and sum\n  left_join(., Blast_GMGI, by = \"ASV_ID\") %&gt;%\n  left_join(., ASV_rank_list, by = \"ASV_ID\") %&gt;%\n\n  ## moving database percent ID to be next to Blast percent ID\n  relocate(c(db_percent_ID, ASV_sum, ASV_rank), .after = pident) %&gt;%\n\n  ## adding choice column for next steps \n  mutate(Choice = NA)\n\n## export this data frame as excel sheet \nmultiple_hit_choice %&gt;% write_xlsx(path_choice_required)\n</code></pre> <p>Based on the output above, user needs to make some choices. In the excel spreadsheet, user needs to mark \u2018x\u2019 on the choices desired while leaving the other entries blank.</p>"},{"location":"eDNA/02-datatable_prep_12S_Riaz/#choosing-one-of-several-hits","title":"Choosing one of several hits.","text":"<p>Load choice edited dataset. No user edits.</p> <pre><code>multiple_hit_choice_edited &lt;- read_xlsx(\"example_output/Taxonomic_assignments/Choice_required_GMGI_multiplehits_edited.xlsx\") %&gt;%\n  ## selecting the choices made\n  filter(!is.na(Choice)) %&gt;%\n  ## selecting only columns needed \n  dplyr::select(ASV_ID, sseqid, Choice)\n</code></pre> <p>A for loop will filter Blast_GMGI df based on these choices. No user edits.</p> <pre><code># Create a new edited df\nBlast_GMGI_edited &lt;- Blast_GMGI \n\n# Loop through each row of the dataframe\nfor (i in multiple_hit_choice_edited$ASV_ID) {\n  # Extract the current row (will do this for each ASV_ID in the choice df)\n  current_row &lt;- multiple_hit_choice_edited %&gt;% subset(ASV_ID==i)\n\n  # Apply filter based on the current row's condition\n  Blast_GMGI_edited &lt;- Blast_GMGI_edited %&gt;%\n    filter(case_when(ASV_ID == current_row$ASV_ID ~ sseqid == current_row$sseqid,\n           TRUE ~ TRUE))\n}\n</code></pre>"},{"location":"eDNA/02-datatable_prep_12S_Riaz/#confirming-that-all-entries-have-been-dealth-with","title":"Confirming that all entries have been dealth with","text":"<p>No user edits.</p> <pre><code>### Check the below output to confirm the filtering steps above worked (if it worked, it won't be in output)\nBlast_GMGI_edited %&gt;% group_by(ASV_ID) %&gt;% slice_max(pident, n=1) %&gt;% count() %&gt;% filter(n&gt;1)\n</code></pre> <pre><code>## # A tibble: 0 \u00d7 2\n## # Groups:   ASV_ID [0]\n## # \u2139 2 variables: ASV_ID &lt;chr&gt;, n &lt;int&gt;\n</code></pre>"},{"location":"eDNA/02-datatable_prep_12S_Riaz/#identify-entries-that-mismatch-between-gmgi-mitofish-and-ncbi-databases","title":"Identify entries that mismatch between GMGI, Mitofish, and NCBI databases","text":"<p>Creating a df called \u201cDisagree\u201d. Review the output before moving onto the next section.</p> <p>No user edits.</p> <pre><code>Disagree &lt;- Blast_GMGI_edited %&gt;% group_by(ASV_ID) %&gt;% \n  dplyr::rename(., GMGI_db_ID = db_percent_ID, GMGI_pident = pident) %&gt;%\n  ## Creating new columns with species name based on pident information\n  mutate(\n    GMGI_100 = if_else(GMGI_pident == 100, Species_name, NA_character_),\n    GMGI_lessthan100 = if_else(GMGI_pident &lt; 100, Species_name, NA_character_)) %&gt;%\n\n  ## taking only the top hit per ASV ID\n  slice_max(GMGI_pident, n = 1, with_ties = FALSE) %&gt;% ungroup() %&gt;%\n\n  ## filtering to distinct rows with selected columns\n  distinct(ASV_ID, GMGI_db_ID, GMGI_pident, GMGI_100, GMGI_lessthan100) %&gt;%\n\n  ## adding Mitofish and editing the Blast_Mito df in the process\n  full_join(Blast_Mito %&gt;% dplyr::select(ASV_ID, Species_name) %&gt;%\n              dplyr::rename(Mitofish = Species_name) %&gt;%\n              distinct() %&gt;% group_by(ASV_ID) %&gt;%\n              mutate(Mitofish = paste0(Mitofish, collapse = \";\")),\n            by = \"ASV_ID\") %&gt;%\n\n  ## adding NCBI and editing the Blast_NCBI df in the process\n  full_join(Blast_NCBI %&gt;% dplyr::select(ASV_ID, Species_name) %&gt;%\n              dplyr::rename(NCBI = Species_name) %&gt;%\n              distinct() %&gt;% group_by(ASV_ID) %&gt;%\n              mutate(NCBI = paste0(NCBI, collapse = \";\")),\n            by = \"ASV_ID\") %&gt;%\n\n  ## adding ASV rank and sum information\n  left_join(., ASV_rank_list, by = \"ASV_ID\") %&gt;%\n\n  ## filtering out duplicate rows\n  distinct() %&gt;%\n\n  ## filtering to those entries that mismatch between GMGI, Mitofish, and NCBI\n  filter((GMGI_100 != GMGI_lessthan100 | GMGI_100 != Mitofish | GMGI_100 != NCBI | is.na(GMGI_100))) %&gt;%\n\n  ## adding choice column for next steps \n  mutate(Choice = NA)\n</code></pre> <pre><code>## Warning in full_join(., Blast_NCBI %&gt;% dplyr::select(ASV_ID, Species_name) %&gt;% : Detected an unexpected many-to-many relationship between `x` and `y`.\n## \u2139 Row 3 of `x` matches multiple rows in `y`.\n## \u2139 Row 10 of `y` matches multiple rows in `x`.\n## \u2139 If a many-to-many relationship is expected, set `relationship =\n##   \"many-to-many\"` to silence this warning.\n</code></pre> <pre><code>## export this data frame as excel sheet \nDisagree %&gt;% write_xlsx(path_disagree_list)\n</code></pre>"},{"location":"eDNA/02-datatable_prep_12S_Riaz/#assign-taxonomy-based-on-hierarchical-approach","title":"Assign taxonomy based on hierarchical approach","text":"<p>Taxonomic identification is taken from GMGI 100%, then GMGI \\&lt;100%, then Mitofish 100%, and finally NCBI 100%.</p> <p>No user edits.</p> <pre><code>ASV_table_taxID &lt;- ASV_table %&gt;% \n\n  ## 1. Top hit from GMGI's database\n  left_join(Blast_GMGI_edited %&gt;%  group_by(ASV_ID) %&gt;%\n              slice_max(pident, n = 1) %&gt;%\n                            dplyr::select(ASV_ID, Species_name),\n            by = join_by(ASV_ID)) %&gt;%\n\n  ## 2. Mitofish database\n  ### join df, select ASV_ID and Species_name columns, rename Species_name to Mito, call only distinct rows\n  left_join(., Blast_Mito %&gt;% dplyr::select(ASV_ID, Species_name) %&gt;% dplyr::rename(Mito = Species_name) %&gt;% distinct() %&gt;%\n\n              ### group by ASV_ID, and collapse all species names separated by ;, then take only distinct rows\n              group_by(ASV_ID) %&gt;% mutate(Mito = paste0(Mito, collapse = \";\")) %&gt;% distinct(), by = \"ASV_ID\") %&gt;%\n\n  ### if GMGI annotation is NA, then replace with Mitofish \n  mutate(., Species_name = ifelse(is.na(Species_name), Mito, Species_name)) %&gt;%\n\n  ## 3. NCBI database; same functions as above\n  left_join(., Blast_NCBI %&gt;% dplyr::select(ASV_ID, Species_name) %&gt;% dplyr::rename(NCBI = Species_name) %&gt;% distinct() %&gt;%\n              group_by(ASV_ID) %&gt;% mutate(NCBI = paste0(NCBI, collapse = \";\")) %&gt;% distinct(), by = \"ASV_ID\") %&gt;%\n  mutate(., Species_name = ifelse(is.na(Species_name), NCBI, Species_name)) %&gt;%\n\n  ## 4. if Species name is STILL not filled, call it \"unassigned\"\n  mutate(., Species_name = ifelse(is.na(Species_name), \"unassigned\", Species_name)) %&gt;%  \n\n  ## removing Mito spp and NCBI spp\n  dplyr::select(-Mito, -NCBI) %&gt;%\n\n  ## move species name to be after ASV_ID\n  relocate(., c(Species_name), .after = ASV_ID)\n</code></pre>"},{"location":"eDNA/02-datatable_prep_12S_Riaz/#edit-taxonomy-annotations-based-on-mismatch-table","title":"Edit taxonomy annotations based on mismatch table","text":"<p>Override any annotations with edited taxonomic identification table. No user edits.</p> <pre><code>## read in edited df \ntaxonomic_choice &lt;- read_xlsx(\"example_output/Taxonomic_assignments/SampleReport_taxonomic_ID_edited.xlsx\") %&gt;%\n    ## selecting only columns needed \n  dplyr::select(ASV_ID, Choice)  \n\n# Create a new edited df\nASV_table_taxID_edited &lt;- ASV_table_taxID \n\n# Loop through each row of the dataframe\nfor (i in taxonomic_choice$ASV_ID) {\n  # Extract the current row (will do this for each ASV_ID in the choice df)\n  current_row &lt;- taxonomic_choice %&gt;% subset(ASV_ID==i)\n\n  # Apply filter based on the current row's condition\n  ASV_table_taxID_edited &lt;- ASV_table_taxID_edited %&gt;%\n    mutate(Species_name = case_when(\n          ASV_ID == current_row$ASV_ID ~ current_row$Choice,\n           TRUE ~ Species_name))\n}\n</code></pre>"},{"location":"eDNA/02-datatable_prep_12S_Riaz/#confirm-all-entries-are-dealt-with","title":"Confirm all entries are dealt with","text":"<p>No user edits.</p> <pre><code>## Output will be blank\nASV_table_taxID_edited %&gt;% dplyr::select(Species_name) %&gt;% distinct() %&gt;% \n  filter(., grepl(\";\", Species_name)) %&gt;% arrange(Species_name) \n</code></pre> <pre><code>## # A tibble: 0 \u00d7 1\n## # \u2139 1 variable: Species_name &lt;chr&gt;\n</code></pre>"},{"location":"eDNA/02-datatable_prep_12S_Riaz/#adjusting-common-name-for-those-entries-that-dont-have-one-from-mito-or-ncbi","title":"Adjusting common name for those entries that don\u2019t have one (from Mito or NCBI)","text":"<p>No user edits.</p> <pre><code>### add common name column to df\nASV_table_taxID_edited &lt;- ASV_table_taxID_edited %&gt;%\n  left_join(., gmgi_db %&gt;% dplyr::select(Species_name, Common_name, Category) %&gt;% distinct(), by = \"Species_name\") %&gt;%\n  relocate(., c(Common_name, Category), .after = Species_name)\n\n### print entries with no common name\nASV_table_taxID_edited %&gt;% dplyr::select(Species_name, Common_name) %&gt;% filter(is.na(Common_name)) %&gt;% distinct()\n</code></pre> <pre><code>## # A tibble: 2 \u00d7 2\n##   Species_name    Common_name\n##   &lt;chr&gt;           &lt;chr&gt;      \n## 1 Cololabis saira &lt;NA&gt;       \n## 2 unassigned      &lt;NA&gt;\n</code></pre> <p>Editing common names and category when needed.</p> <pre><code>### User edits:\n### 1. Add mutate cases using the format ifelse(grepl('', Species_name), \"\", Common_name\n### example: ifelse(grepl('unassigned', Species_name), \"unassigned\", Common_name)\n### 2. Add mutate cases using the format ifelse(grepl('', Species_name), \"\", Category\n\nASV_table_taxID_edited &lt;- ASV_table_taxID_edited %&gt;% \n  # changing specific entries for Common name\n  mutate(Common_name = ifelse(grepl('unassigned', Species_name), \"unassigned\", Common_name),\n\n         ## example from example dataset - change for your own data\n         Common_name = ifelse(grepl('Cololabis saira', Species_name), \"Pacific Saury\", Common_name)\n         ) %&gt;%\n\n  # changing specific entries for category\n  mutate(Category = ifelse(grepl('unassigned', Species_name), \"unassigned\", Category),\n\n         ## example from example dataset - change for your own data\n         Category = ifelse(grepl('Cololabis saira', Species_name), \"Teleost Fish\", Category)\n         )\n\n## printing list of species name without common names \n## after additions to mutate function above, this output should be zero \nASV_table_taxID_edited %&gt;% dplyr::select(Species_name, Common_name) %&gt;% filter(is.na(Common_name)) %&gt;% distinct()\n</code></pre> <pre><code>## # A tibble: 0 \u00d7 2\n## # \u2139 2 variables: Species_name &lt;chr&gt;, Common_name &lt;chr&gt;\n</code></pre>"},{"location":"eDNA/02-datatable_prep_12S_Riaz/#filtering-filter-asv-by-less-than-01-reads-and-then-collapse-by-group","title":"Filtering: Filter ASV by less than 0.1% reads and then collapse by group","text":""},{"location":"eDNA/02-datatable_prep_12S_Riaz/#filter-out-reads-that-are-less-than-01-of-asv-row-total-per-sample","title":"Filter out reads that are less than 0.1% of ASV (row) total per sample.","text":"<p>Create an output of what you\u2019re losing with filtering.</p> <p>No user edits.</p> <pre><code>ASV_table_taxID_filtered &lt;- ASV_table_taxID_edited %&gt;%\n  ## telling the df we are doing the following function by rows (ASVs)\n  rowwise() %&gt;%\n\n  ## filtering out any values that are less than 0.001 of the total ASV read # in each sample\n  mutate(across(.cols = (7:ncol(.)),            \n                .fns = ~ ifelse((.x/ASV_sum)&lt;0.001, NA, .x))) %&gt;% ungroup()\n\n## output of what we're losing\nASV_table_taxID_edited %&gt;% rowwise() %&gt;%\n  mutate(across(.cols = (7:ncol(.)),            \n                .fns = ~ ifelse((.x/ASV_sum)&gt;0.001, NA, .x))) %&gt;% ungroup() %&gt;% write_xlsx(\"example_output/ASV_reads_filtered_out.xlsx\")\n\n\n## Export ASV break-down for 03-data_quality.Rmd\nASV_table_taxID_filtered %&gt;% dplyr::select(ASV_ID, Species_name, Common_name, Category, ASV_sum, ASV_rank) %&gt;%\n  write_xlsx(\"example_output/ASV_breakdown.xlsx\")\n</code></pre>"},{"location":"eDNA/02-datatable_prep_12S_Riaz/#collapsing-read-counts-by-species-name","title":"Collapsing read counts by species name","text":"<p>No user edits.</p> <pre><code>ASV_table_taxID_collapsed &lt;- ASV_table_taxID_filtered %&gt;% \n  # removing original ASV_ID to collapse\n  dplyr::select(-ASV_ID) %&gt;%  \n\n  ## group by Species_name and sample\n  dplyr::group_by(Species_name, Common_name, Category) %&gt;%\n\n  ## sum down column by species name and sample to collapse\n  summarise(across(6:last_col(), ~ sum(., na.rm = TRUE))) %&gt;% ungroup()\n</code></pre> <pre><code>## `summarise()` has grouped output by 'Species_name', 'Common_name'. You can\n## override using the `.groups` argument.\n</code></pre>"},{"location":"eDNA/02-datatable_prep_12S_Riaz/#creating-results-output","title":"Creating results output","text":"<p>Raw reads results output. No user edits.</p> <pre><code>## Raw reads matrix (wide format)\nASV_table_taxID_collapsed %&gt;% write_xlsx(results_rawreads_matrix)\n\n## Raw reads long format and filtering out entries with zero reads\nASV_table_taxID_collapsed %&gt;% \n  gather(\"sampleID\", \"reads\", c(4:last_col())) %&gt;%\n  filter(reads &gt; 0) %&gt;% \n  left_join(., meta, by = \"sampleID\") %&gt;%\n  write_xlsx(results_rawreads_long)\n</code></pre> <p>Relative Abundance</p> <pre><code>## Creating matrix from edited collapsed df\ndata.matrix &lt;- as.matrix(ASV_table_taxID_collapsed %&gt;% \n                           dplyr::select(-Common_name, -Category) %&gt;% column_to_rownames(var = \"Species_name\"))\n\n## Calculating relative abundance\ndata_relativeab &lt;- as.data.frame(make_relative(data.matrix)) %&gt;%\n  ## moving rownames to a column\n  rownames_to_column(var = \"Species_name\") %&gt;%\n\n  ## adding common name and category back in\n  right_join(ASV_table_taxID_collapsed %&gt;% dplyr::select(Species_name, Common_name, Category), .)  \n</code></pre> <pre><code>## Joining with `by = join_by(Species_name)`\n</code></pre> <pre><code>## Exporting matrix\ndata_relativeab %&gt;% write_xlsx(results_relab_matrix)\n\n## Relative abundance long format with metadata \ndata_relativeab %&gt;%\n  gather(\"sampleID\", \"relab\", c(4:last_col())) %&gt;%\n  left_join(., meta, by = \"sampleID\") %&gt;%\n  write_xlsx(results_relab_long)\n</code></pre>"},{"location":"eDNA/03-data_quality/","title":"Metabarcoding data quality: eDNA metabarcoding base script","text":"<p>.Rmd script</p> <p>This script evaluates your sequence quality and taxonomic assignment quality. Figures produced in this script can go into supplemental data for a manuscript.</p>"},{"location":"eDNA/03-data_quality/#load-libraries","title":"Load libraries","text":"<pre><code>library(dplyr) # for data transformation\n</code></pre> <pre><code>## \n## Attaching package: 'dplyr'\n\n## The following objects are masked from 'package:stats':\n## \n##     filter, lag\n\n## The following objects are masked from 'package:base':\n## \n##     intersect, setdiff, setequal, union\n</code></pre> <pre><code>library(tidyverse) # for data transformation\n</code></pre> <pre><code>## \u2500\u2500 Attaching core tidyverse packages \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 tidyverse 2.0.0 \u2500\u2500\n## \u2714 forcats   1.0.0     \u2714 readr     2.1.5\n## \u2714 ggplot2   3.5.1     \u2714 stringr   1.5.1\n## \u2714 lubridate 1.9.3     \u2714 tibble    3.2.1\n## \u2714 purrr     1.0.2     \u2714 tidyr     1.3.1\n\n## \u2500\u2500 Conflicts \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 tidyverse_conflicts() \u2500\u2500\n## \u2716 dplyr::filter() masks stats::filter()\n## \u2716 dplyr::lag()    masks stats::lag()\n## \u2139 Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n</code></pre> <pre><code>library(ggplot2) # for plotting\nlibrary(readxl) ## for reading in excel files\nlibrary(viridis)\n</code></pre> <pre><code>## Loading required package: viridisLite\n</code></pre> <pre><code>library(hrbrthemes)\nlibrary(ggrepel)\nlibrary(cowplot)\n</code></pre> <pre><code>## \n## Attaching package: 'cowplot'\n## \n## The following object is masked from 'package:lubridate':\n## \n##     stamp\n</code></pre> <pre><code># removing scientific notation\n## remove line or comment out if not desired \noptions(scipen=999)\n</code></pre>"},{"location":"eDNA/03-data_quality/#load-data","title":"Load data","text":"<pre><code>### User edits:\n### 1. Replace the 3 paths below: edit example_input to your project specific path \n### 2. Confirm your sampleIDs match between metadata, results df, and filtering stats output\n\nfiltering_stats &lt;- read_tsv(\"example_input/overall_summary.tsv\", show_col_types = FALSE) %&gt;% dplyr::rename(sampleID = sample)\n\nmeta &lt;- read.csv(\"example_input/metadata.csv\", header=TRUE)\n\nresults &lt;- read_xlsx(\"example_output/Results_rawreads_long.xlsx\") %&gt;%\n  mutate(Category = factor(Category, levels = c(\"Human\", \"Livestock\", \"Other\", \"unassigned\", \"Bird\",\n                                                \"Elasmobranch\", \"Marine Mammal\", \"Sea Turtle\", \"Teleost Fish\")))\n\nASV_breakdown &lt;- read_xlsx(\"example_output/ASV_breakdown.xlsx\") %&gt;%\n  mutate(Category = factor(Category, levels = c(\"Human\", \"Livestock\", \"Other\", \"unassigned\", \"Bird\",\n                                                \"Elasmobranch\", \"Marine Mammal\", \"Sea Turtle\", \"Teleost Fish\")))\n</code></pre>"},{"location":"eDNA/03-data_quality/#sequence-data","title":"Sequence data","text":""},{"location":"eDNA/03-data_quality/#data-transformation","title":"Data Transformation","text":"<p>No user edits.</p> <pre><code>df &lt;- full_join(filtering_stats, meta, by = \"sampleID\") %&gt;%\n  # filtering out columns we don't need \n  dplyr::select(-cutadapt_reverse_complemented) %&gt;%\n\n  # removing percentage icon from cutadapt_passing_filters_percent\n  mutate(cutadapt_passing_filters_percent = gsub(\"%\", \"\", cutadapt_passing_filters_percent)) %&gt;%\n\n  # confirming that all columns of interest are numerical \n  mutate_at(vars(2:10), as.numeric) %&gt;%\n\n  # data transformation so all columns of interest are together \n  gather(\"measure\", \"value\", 2:10)  \n</code></pre>"},{"location":"eDNA/03-data_quality/#plotting","title":"Plotting","text":"<p>Suggested webpage to choose colors: https://coolors.co/</p> <pre><code>### User edits:\n### 1. Change paths of output to desired folder (data/figures is suggested data structure)\n### 2. Change x axis and color, size based on metadata desired \n### 3. Change custom colors and sizes if desired and number of colors and sizes based on metadata variable chosen \n\ndf %&gt;% \n  ## USER EDITS IN LINE BELOW \n  ggplot(., aes(x=Project, y=value, color=Site, shape=SampleType)) + \n\n  ## adding points in jitter format \n  geom_jitter(width=0.15, alpha=0.5) + \n\n  ## option for additional boxplots if desired (uncomment to add)\n  #geom_boxplot() +\n\n  ## using facet_wrap to create grid based on variables and factor() to order them in custom format\n  facet_wrap(~factor(measure, levels=c('cutadapt_total_processed', 'cutadapt_passing_filters', \n                                       'cutadapt_passing_filters_percent', 'DADA2_input',\n                                 'filtered', 'denoisedF', 'denoisedR', 'merged', 'nonchim')), scales = \"free\") +\n\n  ## graph asthetics \n  theme_bw() +\n  ylab(\"Number of reads\") + \n\n  ## USER EDITS IN MANUAL CODE BELOW \n  scale_color_manual(values = c(\"red3\", \"lightblue\", \"purple2\", \"gold\", \"green4\", \"black\")) +\n  scale_size_manual(values = c(21,17)) +\n\n\n  theme(panel.background=element_rect(fill='white', colour='black'),\n        strip.background=element_rect(fill='white', colour='black'),\n        strip.text = element_text(size = 10, face=\"bold\"),\n        legend.position = \"right\",\n        axis.text.y = element_text(size=7, color=\"grey30\"),\n        axis.text.x = element_text(angle = 45, hjust = 1),\n        axis.title.y = element_text(margin = margin(t = 0, r = 10, b = 0, l = 0), size=11, face=\"bold\"),\n        axis.title.x = element_text(margin = margin(t = 10, r = 0, b = 0, l = 0), size=11, face=\"bold\"))\n</code></pre> <p></p> <pre><code>ggsave(\"example_output/Figures/SampleReport_FilteringStats.png\", width = 10, height=8)\n</code></pre>"},{"location":"eDNA/03-data_quality/#plot-unassigned-taxonomy","title":"Plot unassigned taxonomy","text":""},{"location":"eDNA/03-data_quality/#data-transformation_1","title":"Data transformation","text":"<p>No user edits.</p> <pre><code>results_summary &lt;- results %&gt;% \n  group_by(Category, Project) %&gt;%\n  summarise(sum_reads = sum(reads))\n</code></pre> <pre><code>## `summarise()` has grouped output by 'Category'. You can override using the\n## `.groups` argument.\n</code></pre> <pre><code>general_stats &lt;- results %&gt;% \n  group_by(Category) %&gt;%\n  summarise(sum_reads = sum(reads)) %&gt;% ungroup() %&gt;%\n  mutate(total = sum(sum_reads),\n         percent = sum_reads/total*100) %&gt;% dplyr::select(Category, percent) %&gt;% distinct() %&gt;%\n  ## round to 2 decimal places \n  mutate(across(c('percent'), round, 4))\n</code></pre> <pre><code>## Warning: There was 1 warning in `mutate()`.\n## \u2139 In argument: `across(c(\"percent\"), round, 4)`.\n## Caused by warning:\n## ! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\n## Supply arguments directly to `.fns` through an anonymous function instead.\n## \n##   # Previously\n##   across(a:b, mean, na.rm = TRUE)\n## \n##   # Now\n##   across(a:b, \\(x) mean(x, na.rm = TRUE))\n</code></pre> <pre><code>ASV_summary &lt;- ASV_breakdown %&gt;%\n  group_by(Category) %&gt;%\n  summarise(count = n_distinct(ASV_ID))\n\nspecies_summary &lt;- results %&gt;%\n  group_by(Category) %&gt;%\n  summarise(count = n_distinct(Species_name))\n\n## species in addition to category\n## metadata option add-in\n</code></pre>"},{"location":"eDNA/03-data_quality/#raw-reads-plotting","title":"Raw Reads Plotting","text":"<p>With metadata</p> <pre><code>### User edits:\n### 1. Change paths of output to desired folder (data/figures is suggested data structure)\n### 2. Change x axis and color, size based on metadata desired \n### 3. Change custom colors and sizes if desired and number of colors and sizes based on metadata variable chosen \n### 4. Comment out any categories that do not show up in your dataset\n\n# Check how many categories \nunique(results_summary$Category)\n</code></pre> <pre><code>## [1] Human        Livestock    unassigned   Teleost Fish\n## 9 Levels: Human Livestock Other unassigned Bird Elasmobranch ... Teleost Fish\n</code></pre> <pre><code>## Based on this output, comment/uncomment the categories present for color \n\nggplot(results_summary, aes(fill=Category, y=sum_reads, x=Project)) + \n    geom_bar(position=\"stack\", stat=\"identity\") +\n    scale_fill_brewer(palette = \"RdYlBu\") +\n    # scale_fill_manual(values = c(\"#9f040e\", # Human\n    #                              \"#e30613\", # Livestock\n    #                              \"#fb747d\", # Other\n    #                              \"#ff0000\", # unassigned\n    #                              \"#03045e\", # Bird\n    #                              \"#023e8a\", # Elasmobranch\n    #                              \"#0077b6\", # Marine mammal\n    #                              \"#0096c7\", # Sea Turtle\n    #                              \"#48cae4\" # Teleost Fish \n    #                              )) +\n    labs(fill = \"Category\") +\n    theme_bw() + \n    xlab(\" Month\") + ylab(\"Raw reads\") +\n    theme(panel.background=element_rect(fill='white', colour='black'),\n        axis.text.y = element_text(size=7, color=\"grey30\"),\n        axis.text.x = element_text(angle = 45, hjust = 1),\n        axis.title.y = element_text(margin = margin(t = 0, r = 10, b = 0, l = 0), size=11, face=\"bold\"),\n        axis.title.x = element_text(margin = margin(t = 10, r = 0, b = 0, l = 0), size=11, face=\"bold\"))\n</code></pre> <p></p> <pre><code>## add % stacked as well\n\nggsave(\"example_output/Figures/Rawreads_percategory.png\", width = 8, height = 5)\n</code></pre> <p>Reads Piechart</p> <pre><code>### User edits:\n### 1. Change paths of output to desired folder (data/figures is suggested data structure)\n### 2. Change scale brewer color if desired \n\npiechart &lt;- general_stats %&gt;%  \n  mutate(csum = rev(cumsum(rev(percent))), \n         pos = percent/2 + lead(csum, 1),\n         pos = if_else(is.na(pos), percent/2, pos))\n\npiechart_reads &lt;- general_stats %&gt;% \n  ggplot(., aes(x=\"\", y = percent, fill = Category)) +\n  geom_col(color = \"black\") +\n  geom_label_repel(data = piechart,\n                   aes(y = pos, label = paste0(percent, \"%\")),\n                   size = 3, nudge_x = 1, show.legend = FALSE) +\n  coord_polar(theta = \"y\") +\n  scale_fill_brewer(palette = \"RdYlBu\") +\n  theme_bw() +\n  theme(\n    plot.background = element_rect(fill = \"white\", colour = NA),\n    panel.border = element_blank(),  # Remove panel border\n    panel.grid = element_blank(),    # Remove grid lines\n    axis.ticks = element_blank(),    # Remove axis ticks\n    axis.text = element_blank(),     # Remove axis text\n    axis.title = element_blank(),     # Remove axis titles\n    legend.position = \"none\"\n  ) +\n  ggtitle(\"% of raw reads\") +\n  xlab(\"\") + ylab(\"\") + labs(fill = \"Category\")\n</code></pre> <p>ASV Piechart</p> <pre><code>### User edits:\n### 1. Change paths of output to desired folder (data/figures is suggested data structure)\n### 2. Change scale brewer color if desired \n\npiechart_ASV &lt;- ASV_summary %&gt;%  \n  mutate(csum = rev(cumsum(rev(count))), \n         pos = count/2 + lead(csum, 1),\n         pos = if_else(is.na(pos), count/2, pos))\n\npiechart_ASV &lt;- ASV_summary %&gt;% \n  ggplot(., aes(x=\"\", y = count, fill = Category)) +\n  geom_col(color = \"black\") +\n  geom_label_repel(data = piechart_ASV,\n                   aes(y = pos, label = paste0(count)),\n                   size = 3, nudge_x = 1, show.legend = FALSE) +\n  coord_polar(theta = \"y\") +\n  scale_fill_brewer(palette = \"RdYlBu\") +\n  theme_bw() +\n  theme(\n    plot.background = element_rect(fill = \"white\", colour = NA),\n    panel.border = element_blank(),  # Remove panel border\n    panel.grid = element_blank(),    # Remove grid lines\n    axis.ticks = element_blank(),    # Remove axis ticks\n    axis.text = element_blank(),     # Remove axis text\n    axis.title = element_blank(),     # Remove axis titles\n    legend.position = \"none\"\n  ) +\n  ggtitle(\"# of ASVs\") +\n  xlab(\"\") + ylab(\"\") + labs(fill = \"Category\")\n</code></pre> <p>Number of species pie chart</p> <pre><code>piechart_spp &lt;- species_summary %&gt;%  \n  mutate(csum = rev(cumsum(rev(count))), \n         pos = count/2 + lead(csum, 1),\n         pos = if_else(is.na(pos), count/2, pos))\n\npiechart_spp &lt;- species_summary %&gt;% \n  ggplot(., aes(x=\"\", y = count, fill = Category)) +\n  geom_col(color = \"black\") +\n  geom_label_repel(data = piechart_spp,\n                   aes(y = pos, label = paste0(count)),\n                   size = 3, nudge_x = 1, show.legend = FALSE) +\n  coord_polar(theta = \"y\") +\n  scale_fill_brewer(palette = \"RdYlBu\") +\n  theme_bw() +\n  theme(\n    plot.background = element_rect(fill = \"white\", colour = NA),\n    panel.border = element_blank(),  # Remove panel border\n    panel.grid = element_blank(),    # Remove grid lines\n    axis.ticks = element_blank(),    # Remove axis ticks\n    axis.text = element_blank(),     # Remove axis text\n    axis.title = element_blank()     # Remove axis titles\n  ) +\n  ggtitle(\"# of species\") +\n  xlab(\"\") + ylab(\"\") + labs(fill = \"Category\")\n</code></pre> <p>Plot together and export</p> <pre><code>plot_grid(piechart_reads, piechart_ASV, piechart_spp, \n          ncol=3, \n          rel_widths = c(2,2,3.075) \n          #align = \"hv\"\n          )\n</code></pre> <p></p> <pre><code>ggsave(\"example_output/Category_breakdown.png\", width=10, height=4)\n</code></pre>"},{"location":"eDNA/04-relative_abundance_heatmap/","title":"Relative Abundance Heatmaps: eDNA metabarcoding base script","text":"<p>.Rmd script</p> <p>This script plots your relative abundance matrix as a heatmap. Figures produced are potentially part of the main figures of your manuscript/report.</p>"},{"location":"eDNA/04-relative_abundance_heatmap/#load-libraries","title":"Load libraries","text":"<pre><code>library(ggplot2) ## for plotting\nlibrary(dplyr) ## for data table manipulation\n</code></pre> <pre><code>## \n## Attaching package: 'dplyr'\n\n## The following objects are masked from 'package:stats':\n## \n##     filter, lag\n\n## The following objects are masked from 'package:base':\n## \n##     intersect, setdiff, setequal, union\n</code></pre> <pre><code>library(tidyr) ## for data table manipulation\nlibrary(readxl) ## for reading in excel files\nlibrary(stringr) ## for data transformation\nlibrary(strex) ## for data transformation\nlibrary(purrr) ## for data transformation\nlibrary(funrar) ## for make_relative()\nlibrary(tidyverse) ## for data transformation\n</code></pre> <pre><code>## \u2500\u2500 Attaching core tidyverse packages \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 tidyverse 2.0.0 \u2500\u2500\n## \u2714 forcats   1.0.0     \u2714 readr     2.1.5\n## \u2714 lubridate 1.9.3     \u2714 tibble    3.2.1\n\n## \u2500\u2500 Conflicts \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 tidyverse_conflicts() \u2500\u2500\n## \u2716 dplyr::filter() masks stats::filter()\n## \u2716 dplyr::lag()    masks stats::lag()\n## \u2139 Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n</code></pre> <pre><code>library(naniar) ## replace_with_na_all function\nlibrary(ggh4x) ## for facet wrap options\n</code></pre> <pre><code>## \n## Attaching package: 'ggh4x'\n## \n## The following object is masked from 'package:ggplot2':\n## \n##     guide_axis_logticks\n</code></pre> <pre><code>library(tidytext)\n</code></pre>"},{"location":"eDNA/04-relative_abundance_heatmap/#load-data","title":"Load data","text":"<pre><code>df &lt;- read_xlsx(\"example_output/Results_relab_long.xlsx\") %&gt;%\n  mutate(across(c(relab), ~ round(.x, 5)))\n</code></pre>"},{"location":"eDNA/04-relative_abundance_heatmap/#remove-categories","title":"Remove Categories","text":"<p>If you want to plot relative abundance without human, other, or livestock categories. As FYI/warning, relative abundance is calculated with these categories included. Relative abundance can also be thought of as proportion of total reads, which is calculated from the total reads for that sample.</p> <pre><code>df_filtered &lt;- df %&gt;% \n  filter(!Category == \"Other\" &amp; !Category == \"Livestock\" &amp; !Category == \"unassigned\" &amp; !Category == \"Human\") \n</code></pre>"},{"location":"eDNA/04-relative_abundance_heatmap/#plot","title":"Plot","text":"<p>reverse label order: scale y discrete limits reverse limits=rev</p> <p>https://coolors.co/ (hit tools on the top right hand side)</p> <pre><code>## if subset of categories is desired, replace df below with df_filtered\ndf %&gt;%\n\n  ## replace zeros with NAs for plotting\n  replace_with_na_all(condition = ~.x == 0.00000) %&gt;%\n\n  ## ggplot basic options (USER EDIT: X AND Y AXIS)\n  ggplot(., aes(x=Site, y=Common_name)) +\n  geom_tile(aes(fill = relab), color = \"black\") +\n\n  ## x, y, and legend labels (USER EDITS IF DESIRED)\n  ylab(\"Common name\") +\n  xlab(\"Site\") +\n  labs(fill = \"Relative Abundance (%)\") +\n\n  ## color of the tile options; direction=1 will flip the low/high (USER EDITS IF DESIRED)\n  scale_fill_gradient(na.value = \"white\", low = \"lightskyblue2\", high = \"#0C4D66\") + \n\n  ## facet grid with Category and project variables\n  facet_grid2(Category ~ SampleType, \n              scales = \"free\", space = \"free\", \n              labeller = labeller(Category = label_wrap_gen(width = 10))) +\n\n  ## graph theme options\n  theme_classic() +\n  theme(\n    ## axis text \n    axis.text.x = element_text(angle = 90, size=6, color=\"grey25\", hjust = 1),\n    axis.text.y = element_text(colour = 'black', size = 8),\n\n    ## legend text and title \n    legend.text = element_text(size = 8, color=\"black\"),\n    legend.title = element_text(margin = margin(t = 0, r = 0, b = 5, l = 0), size=10, color=\"black\", face=\"bold\"),\n    legend.position = c(-0.4, -0.05), \n    legend.key.height = unit(5, 'mm'),\n    legend.direction = \"horizontal\",\n    legend.key.width = unit(5, 'mm'),\n    legend.title.align = 0.5,\n    legend.title.position = \"top\",\n\n    ## axis titles \n    axis.title.y = element_text(margin = margin(t = 0, r = 10, b = 0, l = 0), size=14, face=\"bold\"),\n    axis.title.x = element_text(margin = margin(t = 10, r = 0, b = 0, l = 0), size=14, face=\"bold\"),\n\n    ## facet wrap labels\n    strip.text.x = element_text(color = \"black\", face = \"bold\", size = 12),\n    strip.text.y = element_text(color = \"black\", face = \"bold\", size = 12, angle=0),\n    strip.background.y = element_blank(),\n    strip.clip = \"off\"\n    )\n</code></pre> <pre><code>## Warning: The `legend.title.align` argument of `theme()` is deprecated as of ggplot2\n## 3.5.0.\n## \u2139 Please use theme(legend.title = element_text(hjust)) instead.\n## This warning is displayed once every 8 hours.\n## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was\n## generated.\n\n## Warning: A numeric `legend.position` argument in `theme()` was deprecated in ggplot2\n## 3.5.0.\n## \u2139 Please use the `legend.position.inside` argument of `theme()` instead.\n## This warning is displayed once every 8 hours.\n## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was\n## generated.\n</code></pre> <p></p> <pre><code>## USER EDITS WIDTH AND HEIGHT TO DESIRED   \nggsave(\"example_output/Figures/Relative_abundance.png\", width = 7, height = 10)  \n\n\n## Group by family or order from gmgi_db info? this would break down teleost fish more\n## ecological niche? functional groups? would probably require more metadata..\n</code></pre>"},{"location":"eDNA%20qPCR/qPCR-analysis/","title":"qPCR data sheets","text":"<p>.Rmd script</p>"},{"location":"eDNA%20qPCR/qPCR-analysis/#load-libraries","title":"Load libraries","text":"<pre><code>library(ggplot2) ## for plotting\nlibrary(dplyr) ## for data table manipulation\n</code></pre> <pre><code>## \n## Attaching package: 'dplyr'\n\n## The following objects are masked from 'package:stats':\n## \n##     filter, lag\n\n## The following objects are masked from 'package:base':\n## \n##     intersect, setdiff, setequal, union\n</code></pre> <pre><code>library(tidyr) ## for data table manipulation\nlibrary(readr) ## for reading in tsv files\nlibrary(readxl) ## for reading in excel files\nlibrary(stringr) ## for data transformation\nlibrary(strex) ## for data transformation\nlibrary(writexl) ## for excel output\nlibrary(purrr) ## for data transformation\nlibrary(tidyverse) ## for data table manipulation\n</code></pre> <pre><code>## \u2500\u2500 Attaching core tidyverse packages \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 tidyverse 2.0.0 \u2500\u2500\n## \u2714 forcats   1.0.0     \u2714 tibble    3.2.1\n## \u2714 lubridate 1.9.3\n\n## \u2500\u2500 Conflicts \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 tidyverse_conflicts() \u2500\u2500\n## \u2716 dplyr::filter() masks stats::filter()\n## \u2716 dplyr::lag()    masks stats::lag()\n## \u2139 Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n</code></pre> <pre><code>library(ggrepel)  # For geom_text_repel\n</code></pre>"},{"location":"eDNA%20qPCR/qPCR-analysis/#reading-in-datafiles-and-merging-into-one-large-dataframe","title":"Reading in datafiles and merging into one large dataframe","text":"<pre><code>### USER EDITS:\n## 1. Replace path of files \n## 2. In str_after_nth(file.ID, \"results/\", 1), make sure this matches the folder ending referenced in list.files\n\ndf &lt;- \n  # list files in directory following a particular pattern\n  list.files(path = 'example input/qPCR_results/', pattern = \".xlsx\", full.names = TRUE) %&gt;%\n\n  # get the column names\n  set_names(.) %&gt;% \n\n  # join all files together in one data frame by file ID, skipping first 19 rows\n  map_dfr(~read_xlsx(., skip = 19), .id = \"file.ID\") %&gt;% \n\n  # turn file.ID into just plate information (plate.ID)\n  mutate(file.ID = str_after_nth(file.ID, \"results/\", 1),\n         file.ID = str_before_nth(file.ID, \".xlsx\", 1)) %&gt;%\n  dplyr::rename(plate.ID = file.ID, Sample_ID = Sample) %&gt;%\n\n  ## RI STRIPED BASS SPECIFIC CODE FOR SAMPLE ID MISMATCHES\n  mutate(Sample_ID = gsub(\" #1\", \"_1\", Sample_ID),\n         Sample_ID = gsub(\" #2\", \"_2\", Sample_ID)) %&gt;%\n\n  mutate(Sample_ID = case_when(\n    str_detect(Well, \"4\") &amp; Sample_ID == \"2/21/24_ONE_NTR\" ~ paste0(Sample_ID, \"_1\"),\n    str_detect(Well, \"5\") &amp; Sample_ID == \"2/21/24_ONE_NTR\" ~ paste0(Sample_ID, \"_2\"),\n    TRUE ~ Sample_ID\n  ))\n\nhead(df)\n</code></pre> <pre><code>## # A tibble: 6 \u00d7 8\n##   plate.ID     Well  Fluor Target Content Sample_ID    Cq Starting Quantity (S\u2026\u00b9\n##   &lt;chr&gt;        &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt; &lt;lgl&gt;                 \n## 1 26 AUG 2024\u2026 A01   SYBR  Libra\u2026 Unkn    1/17/24_\u2026  NA   NA                    \n## 2 26 AUG 2024\u2026 A02   SYBR  Libra\u2026 Unkn    1/17/24_\u2026  NA   NA                    \n## 3 26 AUG 2024\u2026 A03   SYBR  Libra\u2026 Unkn    1/17/24_\u2026  NA   NA                    \n## 4 26 AUG 2024\u2026 A04   SYBR  Libra\u2026 Unkn    1/17/24_\u2026  34.8 NA                    \n## 5 26 AUG 2024\u2026 A05   SYBR  Libra\u2026 Unkn    1/17/24_\u2026  NA   NA                    \n## 6 26 AUG 2024\u2026 A06   SYBR  Libra\u2026 Unkn    1/17/24_\u2026  NA   NA                    \n## # \u2139 abbreviated name: \u00b9\u200b`Starting Quantity (SQ)`\n</code></pre>"},{"location":"eDNA%20qPCR/qPCR-analysis/#reading-in-meta-df","title":"Reading in meta df","text":"<p>Sample ID from meta needs to match the Sample ID from the qPCR output</p> <pre><code>meta &lt;- read_xlsx(\"example input/client_metadata_example.xlsx\") %&gt;%\n\n  ## RI STRIPED BASS SPECIFIC CODE FOR SAMPLE ID ISSUES\n  ## For other projects, address any Sample_ID differences (Sample_ID on qPCR output needs to match meta)\n  mutate(Sample_ID = gsub(\" #1\", \"_1\", Sample_ID),\n         Sample_ID = gsub(\" #2\", \"_2\", Sample_ID))\n</code></pre>"},{"location":"eDNA%20qPCR/qPCR-analysis/#evaluating-no-template-controls-ntcs-negatives","title":"Evaluating No Template Controls (NTCs; Negatives)","text":"<pre><code>NTC &lt;- df %&gt;% \n  filter(grepl(\"neg\", Sample_ID, ignore.case = TRUE))\n\n## Calculate number of negatives with Cq values \nsum(!is.na(NTC$Cq))\n</code></pre> <pre><code>## [1] 1\n</code></pre> <pre><code>## Calculate number of negatives total \nnrow(NTC)\n</code></pre> <pre><code>## [1] 120\n</code></pre> <pre><code>## Calculate number of plates\nlength(unique(NTC$plate.ID))\n</code></pre> <pre><code>## [1] 20\n</code></pre>"},{"location":"eDNA%20qPCR/qPCR-analysis/#spike-information","title":"Spike information","text":"<p>For each sample, calculate the distance from the Positive Spike-in value.</p> <pre><code>spike_samples &lt;- df %&gt;% \n  ## subset to spiked samples \n  filter(grepl(\"spike|pos\", Sample_ID, ignore.case = TRUE)) %&gt;%\n\n  ## remove spike from SampleID column only if 'pos' is NOT in the Sample_ID\n  mutate(Sample_ID = case_when(\n    !grepl(\"pos\", Sample_ID, ignore.case = TRUE) ~ str_before_nth(Sample_ID, \" spike\", 1),\n    TRUE ~ Sample_ID\n  )) %&gt;%\n\n  ## group by plate ID and sample ID \n  group_by(plate.ID, Sample_ID) %&gt;%\n\n  ## calculate mean of spikes per plate and sample \n  ## ungroup by Sample_ID so the next calculation is only done grouped by Plate \n  mutate(Filter_Cq = mean(Cq)) %&gt;% \n  ungroup(Sample_ID) %&gt;%\n\n  ## calculate difference from plate's Cq value \n  mutate(Cq_diff = Filter_Cq - Filter_Cq[grepl(\"pos\", Sample_ID, ignore.case = TRUE)]) %&gt;%\n  ungroup(); spike_samples\n</code></pre> <pre><code>## # A tibble: 480 \u00d7 10\n##    plate.ID    Well  Fluor Target Content Sample_ID    Cq Starting Quantity (S\u2026\u00b9\n##    &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt; &lt;lgl&gt;                 \n##  1 26 AUG 202\u2026 G01   SYBR  Libra\u2026 Unkn    1/17/24_\u2026  10.5 NA                    \n##  2 26 AUG 202\u2026 G02   SYBR  Libra\u2026 Unkn    1/17/24_\u2026  10.6 NA                    \n##  3 26 AUG 202\u2026 G03   SYBR  Libra\u2026 Unkn    1/17/24_\u2026  10.8 NA                    \n##  4 26 AUG 202\u2026 G04   SYBR  Libra\u2026 Unkn    1/17/24_\u2026  10.8 NA                    \n##  5 26 AUG 202\u2026 G05   SYBR  Libra\u2026 Unkn    1/17/24_\u2026  10.5 NA                    \n##  6 26 AUG 202\u2026 G06   SYBR  Libra\u2026 Unkn    1/17/24_\u2026  11.0 NA                    \n##  7 26 AUG 202\u2026 G07   SYBR  Libra\u2026 Unkn    1/17/24_\u2026  10.8 NA                    \n##  8 26 AUG 202\u2026 G08   SYBR  Libra\u2026 Unkn    1/17/24_\u2026  11.0 NA                    \n##  9 26 AUG 202\u2026 G09   SYBR  Libra\u2026 Unkn    1/17/24_\u2026  10.7 NA                    \n## 10 26 AUG 202\u2026 G10   SYBR  Libra\u2026 Unkn    1/18/24_\u2026  10.9 NA                    \n## # \u2139 470 more rows\n## # \u2139 abbreviated name: \u00b9\u200b`Starting Quantity (SQ)`\n## # \u2139 2 more variables: Filter_Cq &lt;dbl&gt;, Cq_diff &lt;dbl&gt;\n</code></pre> <pre><code>## Remove any lab error samples \n## RI Striped Bass only \n## removing 2/21/24 DI Blank 1 (pipetting issue) \nspike_samples &lt;- spike_samples %&gt;%\n  filter(!Sample_ID == \"2/21/24_Tap Blank_1\")\n</code></pre> <p>Plot those distance values and save to output folder</p> <pre><code># Create a jitter position object\njitter_pos &lt;- position_jitter(width = 0.45, seed = 123)\n\n## Plot samples \nspike_samples %&gt;% dplyr::select(Target, Sample_ID, Cq_diff) %&gt;% distinct() %&gt;%\n\n  ggplot(., aes(x=Target, y = Cq_diff)) + \n  geom_hline(yintercept = c(-2, 2), linetype = \"dotted\", color = \"grey50\") +\n\n  geom_jitter(aes(fill = abs(Cq_diff) &gt; 2), \n              size = 2, alpha = 0.5, color = 'black', shape = 21, #width = 0.45,\n              position = jitter_pos) +\n\n  scale_fill_manual(values = c(\"TRUE\" = \"red\", \"FALSE\" = \"white\")) +\n  geom_text_repel(\n    aes(label = ifelse(abs(Cq_diff) &gt; 2, Sample_ID, \"\")),  position = jitter_pos,\n    size = 3, box.padding = 0.5, point.padding = 0.2, force = 2\n  ) +\n\n  labs(\n    x = \"Sample\",\n    y = \"Distance from Positive Control\"\n  ) +\n\n  theme_bw() +\n\n  ## keep y axis at least -2,2 but extend to max \n  coord_cartesian(\n    ylim = c(\n      min(-2.5, min(spike_samples$Cq_diff, na.rm = TRUE)),\n      max(2.5, max(spike_samples$Cq_diff, na.rm = TRUE))\n    )) +\n\n  ## theme variables\n    theme(panel.background=element_rect(fill='white', colour='black'),\n        legend.position = \"none\",\n        axis.text.y = element_text(size=10, color=\"grey20\"),\n        axis.text.x = element_blank(),\n        axis.title.y = element_text(margin = margin(t = 0, r = 10, b = 0, l = 0), size=12, face=\"bold\"),\n        axis.title.x = element_text(margin = margin(t = 10, r = 0, b = 0, l = 0), size=12, face=\"bold\"))\n</code></pre> <p></p> <pre><code>ggsave(\"example output/Inhibition.png\", width=5, height=4)\n</code></pre>"},{"location":"eDNA%20qPCR/qPCR-analysis/#filter-data-cq-number-of-replicates-and-copy-num-calculations","title":"Filter data Cq, Number of Replicates, and Copy Num calculations","text":"<pre><code>## USER EDITS: Replace values below with species-specific assay values for copy number calculation\nyint &lt;- 38.183\nslope &lt;- -3.386\n\n## Complete calculations\nfilters_df &lt;- df %&gt;% \n  ## subset out the spiked samples \n  filter(!grepl(\"spike|pos|neg\", Sample_ID, ignore.case = TRUE)) %&gt;%\n\n  ## group by plate ID and sample ID \n  group_by(plate.ID, Sample_ID) %&gt;%\n\n  ## calculate mean of Cq per plate and sample, replacing NaN with NA\n  mutate(Filter_Cq = if_else(is.nan(mean(Cq, na.rm = TRUE)), NA_real_, mean(Cq, na.rm = TRUE))) %&gt;%\n\n  ## calculate # of replicates \n  mutate(Filter_Num_Replicates = sum(!is.na(Cq))) %&gt;%\n\n  ## calculate copy number \n  mutate(Filter_Copy_Num = 10^((Filter_Cq-yint)/slope)) %&gt;%\n\n  ## summarize df with specific columns \n  dplyr::select(plate.ID, Sample_ID, Filter_Cq, Filter_Num_Replicates, Filter_Copy_Num) %&gt;% \n  distinct() %&gt;% ungroup()\n</code></pre>"},{"location":"eDNA%20qPCR/qPCR-analysis/#addressing-duplicate-samples-aka-re-runs","title":"Addressing duplicate samples (a.k.a., re-runs)","text":"<p>This code assumes the user wants to always use the most recent plate if completing a sample for a 2nd time. If this is not the case, chat with Fisheries team to make sure code reflect user\u2019s needs.</p> <pre><code>filters_df &lt;- filters_df %&gt;%\n  ## Separating Plate.ID into Date and Plate Number\n  separate(plate.ID, c(\"Plate_date\", \"Plate_number\"), sep = \" PLATE\") %&gt;%\n\n  ## Change Date column to a Date format \n  mutate(Plate_date = dmy(Plate_date)) %&gt;%\n\n  ## Group by Sample_ID and keep only the row with the most recent date\n  group_by(Sample_ID) %&gt;%\n  slice_max(Plate_date, n = 1, with_ties = FALSE) %&gt;% ungroup()\n\n## confirm the number of rows matches the number of unique Sample IDs (output should be TRUE)\nnrow(filters_df) == length(unique(filters_df$Sample_ID))\n</code></pre> <pre><code>## [1] TRUE\n</code></pre>"},{"location":"eDNA%20qPCR/qPCR-analysis/#combing-with-meta-and-collapsing-by-sample","title":"Combing with meta and collapsing by sample","text":"<pre><code>samples_df &lt;- filters_df %&gt;% right_join(meta, ., by = \"Sample_ID\") %&gt;%\n  group_by(Date, Sample_Location) %&gt;%\n\n  ## mutate to mean Ct value\n  mutate(`Mean Ct` = if_else(is.nan(mean(Filter_Cq, na.rm = TRUE)), \n                                    NA_real_, mean(Filter_Cq, na.rm = TRUE)),\n\n         ## take highest value of number of replicates\n         `Number of Replicates` = max(Filter_Num_Replicates, na.rm=TRUE),\n\n         ## sum of 2 copy number values \n         `Mean Copy Number` = ifelse(sum(Filter_Copy_Num, na.rm=TRUE) == 0, NA, \n                            sum(Filter_Copy_Num, na.rm=TRUE))\n           ) %&gt;%\n\n  ungroup() %&gt;%\n\n  ## summarizing df \n  dplyr::select(Date, Sample_Location, Sample_Type, Number_of_Filters, \n                `Mean Ct`, `Number of Replicates`, `Mean Copy Number`) %&gt;%\n  distinct() %&gt;%\n\n  ## adding new SampleID back in\n  unite(Sample_ID, Date, Sample_Location, sep = \" \", remove=F)\n\nhead(samples_df)\n</code></pre> <pre><code>## # A tibble: 6 \u00d7 8\n##   Sample_ID    Date                Sample_Location Sample_Type Number_of_Filters\n##   &lt;chr&gt;        &lt;dttm&gt;              &lt;chr&gt;           &lt;chr&gt;       &lt;lgl&gt;            \n## 1 2024-01-17 \u2026 2024-01-17 00:00:00 DI Blank #1     Blank       NA               \n## 2 2024-01-17 \u2026 2024-01-17 00:00:00 DI Blank #2     Blank       NA               \n## 3 2024-01-17 \u2026 2024-01-17 00:00:00 GBP_OCA         Field       NA               \n## 4 2024-01-17 \u2026 2024-01-17 00:00:00 IPC_GCO         Field       NA               \n## 5 2024-01-17 \u2026 2024-01-17 00:00:00 IPC_BBC         Field       NA               \n## 6 2024-01-17 \u2026 2024-01-17 00:00:00 NAN_EDI         Field       NA               \n## # \u2139 3 more variables: `Mean Ct` &lt;dbl&gt;, `Number of Replicates` &lt;int&gt;,\n## #   `Mean Copy Number` &lt;dbl&gt;\n</code></pre> <pre><code>## Confirm that the number of samples output from qPCR matches the number of samples in the metadata  \nnrow(samples_df) == nrow(meta %&gt;% dplyr::select(-Sample_ID) %&gt;% distinct())\n</code></pre> <pre><code>## [1] TRUE\n</code></pre>"},{"location":"eDNA%20qPCR/qPCR-analysis/#normalizing-data","title":"Normalizing data","text":"<pre><code>## normalize data \nnormalized_df &lt;- samples_df %&gt;%\n  ## take log10 of copy number \n  mutate(`Mean Copy Number Normalized` = log10(`Mean Copy Number` + 1))\n\n## create an outlier cut-off \ncutoff &lt;- quantile(normalized_df$`Mean Copy Number Normalized`, na.rm = TRUE, probs=0.75) + \n  1.5*IQR(normalized_df$`Mean Copy Number Normalized`, na.rm=TRUE)\n\n## create an outlier cut-off \ncutoff_below &lt;- quantile(normalized_df$`Mean Copy Number Normalized`, na.rm = TRUE, probs=0.25) - \n  1.5*IQR(normalized_df$`Mean Copy Number Normalized`, na.rm=TRUE)\n\n## Output the values that outside the cutoff\nnormalized_df %&gt;% filter(`Mean Copy Number Normalized` &gt; cutoff) \n</code></pre> <pre><code>## # A tibble: 3 \u00d7 9\n##   Sample_ID    Date                Sample_Location Sample_Type Number_of_Filters\n##   &lt;chr&gt;        &lt;dttm&gt;              &lt;chr&gt;           &lt;chr&gt;       &lt;lgl&gt;            \n## 1 2024-01-18 \u2026 2024-01-18 00:00:00 POP_SEC         Field       NA               \n## 2 2024-01-18 \u2026 2024-01-18 00:00:00 POP_OUT         Field       NA               \n## 3 2024-01-18 \u2026 2024-01-18 00:00:00 PJP_CFL         Field       NA               \n## # \u2139 4 more variables: `Mean Ct` &lt;dbl&gt;, `Number of Replicates` &lt;int&gt;,\n## #   `Mean Copy Number` &lt;dbl&gt;, `Mean Copy Number Normalized` &lt;dbl&gt;\n</code></pre> <pre><code>normalized_df %&gt;% filter(`Mean Copy Number Normalized` &lt; cutoff_below) \n</code></pre> <pre><code>## # A tibble: 0 \u00d7 9\n## # \u2139 9 variables: Sample_ID &lt;chr&gt;, Date &lt;dttm&gt;, Sample_Location &lt;chr&gt;,\n## #   Sample_Type &lt;chr&gt;, Number_of_Filters &lt;lgl&gt;, Mean Ct &lt;dbl&gt;,\n## #   Number of Replicates &lt;int&gt;, Mean Copy Number &lt;dbl&gt;,\n## #   Mean Copy Number Normalized &lt;dbl&gt;\n</code></pre> <pre><code>## the cutoff we move forward with is the cutoff (high) but confirm no values are below the lower cutoff value \n</code></pre>"},{"location":"eDNA%20qPCR/qPCR-analysis/#adding-presentabsent-information","title":"Adding present/absent information","text":"<pre><code>normalized_df &lt;- normalized_df %&gt;%\n  mutate(Detection = case_when(\n    is.na(`Mean Copy Number Normalized`) ~ \"Absent\",\n    TRUE ~ \"Present\"\n  ))\n</code></pre>"},{"location":"eDNA%20qPCR/qPCR-analysis/#blank-specific-information","title":"Blank specific information","text":"<pre><code>blank_df &lt;- normalized_df %&gt;% subset(Sample_Type == \"Blank\")\n\ndetection_counts &lt;- blank_df %&gt;%\n  count(Detection) %&gt;%\n  mutate(percentage = n / sum(n) * 100,\n         label = paste0(Detection, \"\\n\", \"n=\", n, \"\\n\", round(percentage, 1), \"%\"))\n\nggplot(detection_counts, aes(x = \"\", y = n, fill = Detection)) +\n  geom_bar(stat = \"identity\", width = 1, color = \"black\", alpha=0.75) +\n  coord_polar(\"y\", start = 0) +\n  geom_text(aes(label = label), \n            position = position_stack(vjust = 0.5), \n            size = 5,\n            fontface = \"bold\") + \n  theme_void() +\n  theme(legend.position = \"none\",\n        #plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\")\n        ) +\n  scale_fill_manual(values = c(\"#D2D4D4\", \"#426999\"))\n</code></pre> <pre><code>ggsave(\"example output/Blanks_piechart.png\", width=4, height=4)\n</code></pre>"},{"location":"eDNA%20qPCR/qPCR-analysis/#sample-preliminary-data","title":"Sample preliminary data","text":"<p>Pie chart</p> <pre><code>fieldsamples_df &lt;- normalized_df %&gt;% subset(Sample_Type == \"Field\")\n\nfield_detection_counts &lt;- fieldsamples_df %&gt;%\n  count(Detection) %&gt;%\n  mutate(percentage = n / sum(n) * 100,\n         label = paste0(Detection, \"\\n\", \"n=\", n, \"\\n\", round(percentage, 1), \"%\"))\n\nggplot(field_detection_counts, aes(x = \"\", y = n, fill = Detection)) +\n  geom_bar(stat = \"identity\", width = 1, color = \"black\", alpha=0.75) +\n  coord_polar(\"y\", start = 0) +\n  geom_text(aes(label = label), \n            position = position_stack(vjust = 0.5), \n            size = 5,\n            fontface = \"bold\") + \n  theme_void() +\n  theme(legend.position = \"none\",\n        #plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\")\n        ) +\n  scale_fill_manual(values = c(\"#D2D4D4\", \"#426999\"))\n</code></pre> <p></p> <pre><code>ggsave(\"example output/Fields_piechart.png\", width=4, height=4)\n</code></pre> <p>Bar chart</p> <pre><code>fieldsamples_df %&gt;% \n  filter(!is.na(`Mean Copy Number Normalized`)) %&gt;%\n  ggplot(., aes(x=Sample_ID, y=`Mean Copy Number Normalized`, fill = `Mean Copy Number Normalized` &gt; cutoff)) + \n  geom_bar(stat = \"identity\", width = 0.7, alpha=0.75) +\n  scale_fill_manual(values = c(\"#426999\", \"#c1121f\"),\n                    labels = c(\"Normal\", \"Outlier\"),\n                    ) +\n  labs(\n    y=\"Log-Normalized Copy Number\",\n    x = \"Sample ID\",\n    fill = \"Outlier detection\",\n    #title = \"Just plotting log normalized value\"\n  ) +\n  theme_bw() +\n    ## theme variables\n    theme(panel.background=element_rect(fill='white', colour='black'),\n        legend.position = \"right\",\n        axis.text.y = element_text(size=8, color=\"grey20\"),\n        axis.text.x = element_text(size=6, color=\"grey20\", angle=90, hjust=1, vjust=0.5),\n        axis.title.y = element_text(margin = margin(t = 0, r = 10, b = 0, l = 0), size=10, face=\"bold\"),\n        axis.title.x = element_text(margin = margin(t = 10, r = 0, b = 0, l = 0), size=10, face=\"bold\"),\n        # New theme elements for strip appearance\n        strip.background = element_rect(fill = \"white\", color = \"black\"),\n        strip.text = element_text(face = \"bold\", size = 9)\n    ) +\n  guides(fill = guide_legend(override.aes = list(alpha = 1)))\n</code></pre> <p></p> <pre><code>ggsave(\"example output/Fieldsamples_barchart.png\", width = 9.5, height=5)\n</code></pre>"},{"location":"eDNA%20qPCR/qPCR-analysis/#exporting-data","title":"Exporting data","text":"<pre><code>blank_df %&gt;% mutate(Date = as.Date(Date)) %&gt;% \n  dplyr::select(Date, Sample_Location, `Mean Ct`, `Number of Replicates`, \n                `Mean Copy Number`, `Mean Copy Number Normalized`, Detection) %&gt;%\n\n  unite(Sample, Date, Sample_Location, sep = \" \", remove = TRUE) %&gt;%\n\n  # cut decimals down to 2\n  mutate(across(where(is.numeric), ~round(., 2))) %&gt;%\n\n  write_xlsx(\"example output/Results_Blanks.xlsx\")\n\n### adding some meta for Rich \nrich_meta &lt;- read_xlsx(\"C:/BoxDrive/Box/Science/Fisheries/Projects/eDNA/RI Striped Bass/metadata/eDNA_Data_RI_STB_2024.xlsx\") %&gt;%\n  dplyr::rename(Date = Sample_Date, Sample_Location = Station_Code) %&gt;%\n  mutate(Sample_Time = format(Sample_Time, format = \"%H:%M:%S\"))\n\n\nnormalized_df %&gt;% full_join(rich_meta, ., by = c(\"Date\", \"Sample_Location\")) %&gt;%\n  mutate(Date = as.Date(Date)) %&gt;%\n  dplyr::select(-Number_of_Filters) %&gt;% write_xlsx(\"example output/Results.xlsx\")\n\n\n  # dplyr::select(Date, Sample_Location, `Mean Ct`, `Number of Replicates`, \n  #               `Mean Copy Number`, `Mean Copy Number Normalized`, Detection) %&gt;%\n  # \n  # unite(Sample, Date, Sample_Location, sep = \" \", remove = FALSE) %&gt;%\n  # \n  # # cut decimals down to 2\n  # mutate(across(where(is.numeric), ~round(., 2))) %&gt;%\n  # \n  # write_xlsx(\"example output/Results.xlsx\")\n</code></pre>"}]}