{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the GMGI Fisheries Lab Handbook!","text":"<p>[team description]</p>"},{"location":"#gloucester-marine-genomics-institute","title":"Gloucester Marine Genomics Institute","text":"<p>[description and links]</p>"},{"location":"#laboratory-protocols","title":"Laboratory Protocols","text":""},{"location":"#bioinformatic-pipelines","title":"Bioinformatic Pipelines","text":""},{"location":"#recent-news-and-publications","title":"Recent News and Publications","text":""},{"location":"Computing_GMGI/","title":"GMGI in-house Computing Resources","text":"<p>Gloucester Marine Genomics Institute (GMGI) has 2 in-house servers that are used for bioinformatic analyses. </p> <p></p> <p>Ubuntu Linux operating system aka Humarus </p> <p>Humarus is primarily used for large-scale jobs (e.g., genome assemblies) and thus not the primary working area for Fisheries. </p> <p>Red Hat Enterprise Linux (RHEL) aka Gadus </p> <p>RHEL/Gadus is the primary working area, storage space, and is data is backed up daily to the Synology RackStation in-house. </p>"},{"location":"Computing_GMGI/#logging-in","title":"Logging in","text":"<p>Use ssh with username and the correct IP address that can be found on Lab Archives. Follow instructions for entering password. New users will need to get set-up with Jen while onboarding. </p> <pre><code>ssh username@123.456.7.8\n</code></pre>"},{"location":"Computing_GMGI/#server-structure","title":"Server Structure","text":"<p>Once logged in, users are directed to their home directory (<code>~/</code>) by default. This space has limited storage and is not intended for regular work. The Fisheries team primarily uses the NU Discovery Cluster for active projects and GMGI's in-house resources for long-term storage and data archiving. Consequently, team members typically use their home directory only for data transfers. </p> <p>General server structure: </p> <p>Do not edit any folder other than <code>data</code>. Only the RHEL main contact is responsible for downloading modules or setting up users. </p> <pre><code>[estrand@gadus ~]$ cd ../../\n[estrand@gadus /]$ ls\nbin  boot  data  dev  etc  home  lib  lib64  media  mnt  opt  proc  root  run  sbin  srv  sys  tmp  usr  var\n</code></pre> <p>Subdirectories within <code>data</code>: - <code>prj</code>: Each lab has their own folder (e.g., <code>prj/Fisheries</code>) that is a working area for data and bioinformatic analyses.   - <code>resources</code>: Shared resources like common databases, modules, and scripts live here. - <code>usr</code> and <code>var</code> are for the RHEL main contact only. </p> <pre><code>[estrand@gadus /]$ cd data/\n[estrand@gadus data]$ ls\nprj  resources  usr  var\n</code></pre> <p>Fisheries folders (<code>prj/Fisheries</code>):  </p> <p>We organize these folders by type of analyses or project. I.e., all eDNA projects should be nested within <code>edna</code>. </p> <pre><code>[estrand@gadus Fisheries]$ ls\n202402_negatives  edna  epiage  JonahCrabGenome  JonahCrabPopGen  lobster  SandLanceData\n</code></pre>"},{"location":"Computing_GMGI/#programs-and-modules","title":"Programs and modules","text":"<p>We run programs as 'modules' that are downloaded by the RHEL's main contact (Jen). If you need a program, send Jen a slack or email with the program name, download link, and if an R package, specify if it is a Bioconductor package or regular CRAN repository package. Once a program is downloaded as a module, this is available for all users. Global installation of programs and R packages helps keep the server uncluttered and not waste space with multiple installations. Do not install your own copies. </p> <p>Common commands:  - To find already installed programs: <code>module avail</code>  - To get information about a module: <code>module help [module/version]</code> or <code>module whatis [module/version]</code>. \"help\" will provide what the module is, package information including version and install date, and a link to the documentation/github. \"whatis\" will provide a short, one line description of the program.   - To load a module: <code>module load [module/version]</code> (e.g., <code>module load bamUtil/v1.0.15</code>). Loading a module will put all the necessary executables and dependencies for that program in your path so you can call the commands from any location (i.e. your working directory).    </p> <p>Replace \"[module/version]\" with the information for your module of interest, as it shows up in \"module avail\" list.</p>"},{"location":"Computing_GMGI/#running-bioinformatic-scripts","title":"Running bioinformatic scripts","text":"<p>The GMGI RHEL does not currently have a job scheduler program so each user needs to be extremely careful with how much memory and resources their scripts take up. </p> <p>Common commands: - Check all jobs that are running: <code>top</code> and to exit that screen, click Q - Check only our user: <code>top -u username</code> and to exit that screen, click Q  </p>"},{"location":"Computing_NU/","title":"Downloading sequencing data to servers","text":"<p>Goal: Download .fastq files from sequencing center to HPC and/or move data between HPCs and personal computers.</p> <p>Table of Contents: - GMGI in-house sequencing to HPC  - External sequencing to HPC  - HPC to Personal Computer - HPC to AWS back-up </p> <p>To transfer data from HPC to HPC (e.g., GMGI to NU), use Globus instructions outlined in External sequencing to HPC. </p>"},{"location":"Computing_NU/#illumina-basespace-to-nu-discovery-cluster-or-gmgi-in-house-hpc","title":"Illumina BaseSpace to NU Discovery Cluster or GMGI in-house HPC","text":"<p>Illumina BaseSpace CLI instructions</p> <p>Connecting your user to Illumina BaseSpace:  </p> <p>Each user only needs to complete this once to set-up. If completed for previous projects, skip to downloading data steps. </p> <ol> <li>Create folder called bin: <code>mkdir $HOME/bin</code> </li> <li>Download BaseSpace CLI: <code>wget \"https://launch.basespace.illumina.com/CLI/latest/amd64-linux/bs\" -O $HOME/bin/bs</code> </li> <li>Change the file permissions to make the downloaded binary executable: <code>chmod u+x $HOME/bin/bs</code> </li> <li>Authenticate your account: <code>bs auth</code> </li> <li>Navigate to the webpage provided and authenticate use of BaseSpace CLI.  </li> </ol> <p>Download data from each run to desired output path: </p> <ol> <li>Find the Run ID of desired download: Within Sequence Hub, navigate to Runs and select the desired run. The Run ID is in the webpage handle (e.g., https://basespace.illumina.com/run/123456789/details). </li> </ol> <p></p> <ol> <li>Navigate to <code>cd $HOME/bin</code> and download dataset: <code>bs download run -n run_name --extension=fastq.gz -o /local/output/path</code>. Replace <code>run_name</code> with the exact name of the run on BaseSpace.  </li> <li>Navigate to the output path <code>cd /local/output/path</code> and move all files out of subdirectories: <code>mv */* .</code> </li> </ol>"},{"location":"Computing_NU/#globus-to-nu-discovery-cluster-or-gmgi-in-house-hpc","title":"Globus to NU Discovery Cluster or GMGI in-house HPC","text":"<p>External sequencing centers (e.g., UConn) will share data via Globus. Instructions from NU on transfering data and using Globus. Globus works by transferring data between 'endpoints'. NU's endpoint is called Discovery Cluster which is searchable but our in-house GMGI endpoint needs to be created for each user. </p> <p>Globus instructions: [https://docs.globus.org/globus-connect-server/v5.4/quickstart/]. Create a Globus account prior to instructions below. If transferring to NU, user needs to connect their NU account to their Globus account (see above NU instructions for this step). </p> <p>GMGI endpoint set-up (only need to do this once): 1. Navigate to the globusconnectpersonal-3.2.2 module that is already downloaded on GMGI's in-house server: <code>cd /data/resources/app_modules/globusconnectpersonal-3.2.2</code>. 2. Set-up an endpoint: <code>./globusconnectpersonal -setup --no-gui</code> 3. This will then ask you to click on a log-in link. Once logged in, you receive a authorization code. Paste that in your terminal window where it asked for this code. 4. Name your endpoint with your own user (change this to your first and last name): <code>user.name</code> 5. If successfully, globus will output: </p> <pre><code>Input a value for the Endpoint Name: user.name\n\nregistered new endpoint, id: [unique ID to you]\n\nsetup completed successfully\n</code></pre> <p>Start Globus transfer: 1. [GMGI only] Navigate to the globusconnectpersonal-3.2.2 module that is already downloaded on GMGI's in-house server: <code>cd /data/resources/app_modules/globusconnectpersonal-3.2.2</code>. 2. [GMGI only] Activate personal endpoint: <code>./globusconnectpersonal -start &amp;</code> 3. [GMGI only] Your <code>user.name</code> endpoint will now appear as an option on the Globus online interface.  4. Log into Globus and Navigate to 'Collections' on the left hand panel. Confirm that your GMGI endpoint is activated (green icon):</p> <p></p> <ol> <li>Select the 'File Manager' on the left hand panel. Choose the sequencing center endpoint in the left side and the server end point on the right side. NU's Discovery Cluster is searchable but GMGI endpoint will the user.name set up in previous steps. </li> </ol> <p></p> <ol> <li>Select all files that you want to transfer.  </li> <li>Select Start to begin the transfer.  </li> <li>Check the status of a transfer by selecting 'Activity' on the left hand panel.  </li> <li>[GMGI only] Once transfer is complete, deactivate the endpoint: <code>./globusconnectpersonal -stop</code>. </li> </ol>"},{"location":"Computing_NU/#hpc-to-personal-computer-or-vice-versa","title":"HPC to personal computer or vice versa","text":"<p>Users can do this via Globus or 'scp' (secure copy paste) commands detailed below. NU instructions on transfer via terminal. Make sure you're using \"xfer.discovery.neu.edu\" for the discovery cluster and not login.discovery.neu.edu, or you'll get an email warning you that you're using too much CPU!</p> <p>For all the below code, change content in &lt;&gt; and then delete the &lt;&gt;. All commands need to be run in own terminal and not logged onto either server. </p> <p>Transfer a file: - To NU from personal computer: <code>scp &lt;filename and path&gt; &lt;username&gt;@xfer.discovery.neu.edu:/path/</code>  - To personal computer from NU: <code>scp &lt;username&gt;@xfer.discovery.neu.edu:/path/ /output/path/</code> </p> <p>Transfer a directory (a.k.a., repository) to personal computer from NU: <code>scp -r &lt;username&gt;@xfer.discovery.neu.edu:/path/ /output/path/</code> </p> <p>To transfer directly from GMGI to NU or vice versa, use Globus. </p>"},{"location":"Computing_NU/#aws-back-up","title":"AWS Back-up","text":"<p>AWS is our Amazon Web Services S3 cloud-based storage to backup data long-term data storage and back-up. GMGI uploads data from our in-house server to AWS.</p> <p>What should be backed up: - Raw data files such as fastq files directly from the sequencer - Final result data files (i.e. count table, assemblies, etc.)  </p> <ol> <li>Make sure all files are compressed by:  </li> <li>Gzip all fastq files (e.g., raw data, trimmed data), .fasta/.fa files (e.g., reference genomes), and large .txt files (e.g., intermediate files created during analysis): <code>gzip *.fastq</code> or create a slurm array with a sbatch script.  </li> <li>Genozip all .bam, .sam, .vcf files (e.g., intermediate files created during analysis). Genozip program is downloaded NU in <code>/work/gmgi/packages/</code> for general use.  </li> </ol> <p>Prior to AWS back-up, check with Tim or Emma for approval of files and compression. </p> <ol> <li>Create a new screen session called AWS_tar (user can change this name to desired): <code>tmux new -s AWS_tar</code> </li> <li>Create a txt file with file sizes of all desired input: <code>ls -l *gz &gt; file_size.txt</code> </li> <li>Edit this file to be only file sizes and names: <code>awk '{print $5,$9}' file_size.txt &gt; file_size_edited.txt</code> </li> <li>View this edited file: <code>head file_size_edited.txt</code></li> </ol> <pre><code>11400971821 Mae-263_S1_R1_001.fastq.gz\n\n12253428145 Mae-263_S1_R2_001.fastq.gz\n\n11962611469 Mae-266_S2_R1_001.fastq.gz\n\n12839131166 Mae-266_S2_R2_001.fastq.gz\n\n9691926610 Mae-274_S3_R1_001.fastq.gz\n</code></pre> <ol> <li>Sum the first column: <code>awk '{sum += $1} END {print sum}' file_size_edited.txt</code>. This value is in Byte (B), but convert to MB or TB for a more helpful value to work with. It's important to know the size of the data you are working for storage and cost purposes. Backing up to AWS costs $$/monthly based on TBs stored and our HPC systems have max TB storage limitations.  </li> <li>Tar all desired data to result in on zipped file: <code>tar -czvf HaddockEpiAge1_rawData_20231113.tar.gz ./*fastq.gz</code>. Tar file name needs to be a unique identifier that includes project name, date, and description of the files included.  </li> <li>Detach from a session: Press Ctrl+B, release, and then press D. This tar function will take awhile especially for large datasets.  </li> <li>Reopen/attach a detached session: <code>tmux attach-session -t AWS_tar</code>.  </li> <li>Check the tar file size: <code>ls -lha</code>. Example output: <code>-rw-rw-r--. 1 estrand science 1736366676523 Nov 15 17:54 HaddockEpiAge1_rawData_20231113.tar.gz</code>. This 1736366676523 value is the size in B which should match exactly the sum of all input file sizes calculated previously.   </li> <li>Once this tar function is complete, end a tmux session (forever - not just detached): In the attached session, type exit and press enter. Or press Ctrl+D.  </li> <li>Move the packed tar archive file to AWS transfer folder: <code>mv HaddockEpiAge1_rawData_20231113.tar.gz /data/prj/AWS-transfers</code>.  </li> <li>Notify Jen that there is a transfer waiting so she can move this to AWS services and then remove from the /AWS-transfers folder. </li> </ol>"},{"location":"Data_To_Server/","title":"Downloading sequencing data to servers","text":"<p>Goal: Download .fastq files from sequencing center to HPC and/or move data between HPCs and personal computers.</p> <p>Table of Contents: - GMGI in-house sequencing to HPC  - External sequencing to HPC  - HPC to Personal Computer - HPC to AWS back-up </p> <p>To transfer data from HPC to HPC (e.g., GMGI to NU), use Globus instructions outlined in External sequencing to HPC. </p>"},{"location":"Data_To_Server/#illumina-basespace-to-nu-discovery-cluster-or-gmgi-in-house-hpc","title":"Illumina BaseSpace to NU Discovery Cluster or GMGI in-house HPC","text":"<p>Illumina BaseSpace CLI instructions</p> <p>Connecting your user to Illumina BaseSpace:  </p> <p>Each user only needs to complete this once to set-up. If completed for previous projects, skip to downloading data steps. </p> <ol> <li>Create folder called bin: <code>mkdir $HOME/bin</code> </li> <li>Download BaseSpace CLI: <code>wget \"https://launch.basespace.illumina.com/CLI/latest/amd64-linux/bs\" -O $HOME/bin/bs</code> </li> <li>Change the file permissions to make the downloaded binary executable: <code>chmod u+x $HOME/bin/bs</code> </li> <li>Authenticate your account: <code>bs auth</code> </li> <li>Navigate to the webpage provided and authenticate use of BaseSpace CLI.  </li> </ol> <p>Download data from each run to desired output path: </p> <ol> <li>Find the Run ID of desired download: Within Sequence Hub, navigate to Runs and select the desired run. The Run ID is in the webpage handle (e.g., https://basespace.illumina.com/run/123456789/details). </li> </ol> <p></p> <ol> <li>Navigate to <code>cd $HOME/bin</code> and download dataset: <code>bs download run -n run_name --extension=fastq.gz -o /local/output/path</code>. Replace <code>run_name</code> with the exact name of the run on BaseSpace.  </li> <li>Navigate to the output path <code>cd /local/output/path</code> and move all files out of subdirectories: <code>mv */* .</code> </li> </ol>"},{"location":"Data_To_Server/#globus-to-nu-discovery-cluster-or-gmgi-in-house-hpc","title":"Globus to NU Discovery Cluster or GMGI in-house HPC","text":"<p>External sequencing centers (e.g., UConn) will share data via Globus. Instructions from NU on transfering data and using Globus. Globus works by transferring data between 'endpoints'. NU's endpoint is called Discovery Cluster which is searchable but our in-house GMGI endpoint needs to be created for each user. </p> <p>Globus instructions: [https://docs.globus.org/globus-connect-server/v5.4/quickstart/]. Create a Globus account prior to instructions below. If transferring to NU, user needs to connect their NU account to their Globus account (see above NU instructions for this step). </p> <p>GMGI endpoint set-up (only need to do this once): 1. Navigate to the globusconnectpersonal-3.2.2 module that is already downloaded on GMGI's in-house server: <code>cd /data/resources/app_modules/globusconnectpersonal-3.2.2</code>. 2. Set-up an endpoint: <code>./globusconnectpersonal -setup --no-gui</code> 3. This will then ask you to click on a log-in link. Once logged in, you receive a authorization code. Paste that in your terminal window where it asked for this code. 4. Name your endpoint with your own user (change this to your first and last name): <code>user.name</code> 5. If successfully, globus will output: </p> <pre><code>Input a value for the Endpoint Name: user.name\n\nregistered new endpoint, id: [unique ID to you]\n\nsetup completed successfully\n</code></pre> <p>Start Globus transfer: 1. [GMGI only] Navigate to the globusconnectpersonal-3.2.2 module that is already downloaded on GMGI's in-house server: <code>cd /data/resources/app_modules/globusconnectpersonal-3.2.2</code>. 2. [GMGI only] Activate personal endpoint: <code>./globusconnectpersonal -start &amp;</code> 3. [GMGI only] Your <code>user.name</code> endpoint will now appear as an option on the Globus online interface.  4. Log into Globus and Navigate to 'Collections' on the left hand panel. Confirm that your GMGI endpoint is activated (green icon):</p> <p></p> <ol> <li>Select the 'File Manager' on the left hand panel. Choose the sequencing center endpoint in the left side and the server end point on the right side. NU's Discovery Cluster is searchable but GMGI endpoint will the user.name set up in previous steps. </li> </ol> <p></p> <ol> <li>Select all files that you want to transfer.  </li> <li>Select Start to begin the transfer.  </li> <li>Check the status of a transfer by selecting 'Activity' on the left hand panel.  </li> <li>[GMGI only] Once transfer is complete, deactivate the endpoint: <code>./globusconnectpersonal -stop</code>. </li> </ol>"},{"location":"Data_To_Server/#hpc-to-personal-computer-or-vice-versa","title":"HPC to personal computer or vice versa","text":"<p>Users can do this via Globus or 'scp' (secure copy paste) commands detailed below. NU instructions on transfer via terminal. Make sure you're using \"xfer.discovery.neu.edu\" for the discovery cluster and not login.discovery.neu.edu, or you'll get an email warning you that you're using too much CPU!</p> <p>For all the below code, change content in &lt;&gt; and then delete the &lt;&gt;. All commands need to be run in own terminal and not logged onto either server. </p> <p>Transfer a file: - To NU from personal computer: <code>scp &lt;filename and path&gt; &lt;username&gt;@xfer.discovery.neu.edu:/path/</code>  - To personal computer from NU: <code>scp &lt;username&gt;@xfer.discovery.neu.edu:/path/ /output/path/</code> </p> <p>Transfer a directory (a.k.a., repository) to personal computer from NU: <code>scp -r &lt;username&gt;@xfer.discovery.neu.edu:/path/ /output/path/</code> </p> <p>To transfer directly from GMGI to NU or vice versa, use Globus. </p>"},{"location":"Data_To_Server/#aws-back-up","title":"AWS Back-up","text":"<p>AWS is our Amazon Web Services S3 cloud-based storage to backup data long-term data storage and back-up. GMGI uploads data from our in-house server to AWS.</p> <p>What should be backed up: - Raw data files such as fastq files directly from the sequencer - Final result data files (i.e. count table, assemblies, etc.)  </p> <ol> <li>Make sure all files are compressed by:  </li> <li>Gzip all fastq files (e.g., raw data, trimmed data), .fasta/.fa files (e.g., reference genomes), and large .txt files (e.g., intermediate files created during analysis): <code>gzip *.fastq</code> or create a slurm array with a sbatch script.  </li> <li>Genozip all .bam, .sam, .vcf files (e.g., intermediate files created during analysis). Genozip program is downloaded NU in <code>/work/gmgi/packages/</code> for general use.  </li> </ol> <p>Prior to AWS back-up, check with Tim or Emma for approval of files and compression. </p> <ol> <li>Create a new screen session called AWS_tar (user can change this name to desired): <code>tmux new -s AWS_tar</code> </li> <li>Create a txt file with file sizes of all desired input: <code>ls -l *gz &gt; file_size.txt</code> </li> <li>Edit this file to be only file sizes and names: <code>awk '{print $5,$9}' file_size.txt &gt; file_size_edited.txt</code> </li> <li>View this edited file: <code>head file_size_edited.txt</code></li> </ol> <pre><code>11400971821 Mae-263_S1_R1_001.fastq.gz\n\n12253428145 Mae-263_S1_R2_001.fastq.gz\n\n11962611469 Mae-266_S2_R1_001.fastq.gz\n\n12839131166 Mae-266_S2_R2_001.fastq.gz\n\n9691926610 Mae-274_S3_R1_001.fastq.gz\n</code></pre> <ol> <li>Sum the first column: <code>awk '{sum += $1} END {print sum}' file_size_edited.txt</code>. This value is in Byte (B), but convert to MB or TB for a more helpful value to work with. It's important to know the size of the data you are working for storage and cost purposes. Backing up to AWS costs $$/monthly based on TBs stored and our HPC systems have max TB storage limitations.  </li> <li>Tar all desired data to result in on zipped file: <code>tar -czvf HaddockEpiAge1_rawData_20231113.tar.gz ./*fastq.gz</code>. Tar file name needs to be a unique identifier that includes project name, date, and description of the files included.  </li> <li>Detach from a session: Press Ctrl+B, release, and then press D. This tar function will take awhile especially for large datasets.  </li> <li>Reopen/attach a detached session: <code>tmux attach-session -t AWS_tar</code>.  </li> <li>Check the tar file size: <code>ls -lha</code>. Example output: <code>-rw-rw-r--. 1 estrand science 1736366676523 Nov 15 17:54 HaddockEpiAge1_rawData_20231113.tar.gz</code>. This 1736366676523 value is the size in B which should match exactly the sum of all input file sizes calculated previously.   </li> <li>Once this tar function is complete, end a tmux session (forever - not just detached): In the attached session, type exit and press enter. Or press Ctrl+D.  </li> <li>Move the packed tar archive file to AWS transfer folder: <code>mv HaddockEpiAge1_rawData_20231113.tar.gz /data/prj/AWS-transfers</code>.  </li> <li>Notify Jen that there is a transfer waiting so she can move this to AWS services and then remove from the /AWS-transfers folder. </li> </ol>"},{"location":"test/","title":"Computing Best Practices","text":"<p>TEST</p>"},{"location":"EpiAge/eDNA_01/","title":"01-","text":"<p>TEST</p>"},{"location":"EpiAge/eDNA_02/","title":"eDNA 02","text":"<p>TEST</p>"},{"location":"PopGen/eDNA_01/","title":"01-","text":"<p>TEST</p>"},{"location":"PopGen/eDNA_02/","title":"eDNA 02","text":"<p>TEST</p>"},{"location":"eDNA/","title":"Environmental DNA (eDNA) Bioinformatic Workflow","text":"<p>Graphic from Szekely et al. 2022</p>"},{"location":"eDNA/#bony-fish-and-elasmobranch-targets","title":"Bony Fish and Elasmobranch Targets","text":""},{"location":"eDNA/#invertebrate-targets","title":"Invertebrate Targets","text":""},{"location":"eDNA/01-Metabarcoding%20ampliseq%2012S%20MiFish/","title":"Metabarcoding workflow for 12S amplicon sequencing with MiFish primers","text":"<p>page details in progress. </p> <p>The 12S rRNA gene region of the mitogenome is ~950 bp. There are two popular primer sets to amplify two different regions of 12S: Riaz and MiFish. The following workflow includes script specific to the MiFish Universal (U) or Elasmobranch (E) primer set.</p> <p></p> <p>Citation: Miya et al. 2015</p> <p>The MiFish-U and MiFish-E primers are two variants of universal PCR primers developed for metabarcoding environmental DNA (eDNA) from fishes. Here are the key differences between them: - Target species: MiFish-U (Universal) is designed to amplify DNA from a wide range of bony fishes (Osteichthyes). MiFish-E (Elasmobranch) is specifically optimized for cartilaginous fishes like sharks and rays (Elasmobranchii).  - Primer sequences: While both primer sets target the mitochondrial 12S rRNA gene, they have slightly different nucleotide sequences to accommodate the genetic variations between bony and cartilaginous fishes. - Amplicon length: MiFish-U typically produces amplicons around 170-180 base pairs long and MiFish-E amplicons are usually slightly shorter, around 160-170 base pairs. </p> <p>Scripts to run: </p> <ol> <li>00-fastqc.sh   </li> <li>00-multiqc.sh  </li> <li>01a-metadata.R</li> <li>01b-ampliseq.sh</li> <li>02-taxonomicID.sh  </li> </ol>"},{"location":"eDNA/01-Metabarcoding%20ampliseq%2012S%20MiFish/#step-1-conda-environment-fisheries-edna","title":"Step 1: Conda environment: Fisheries eDNA","text":"<p>Background information on Conda: https://docs.conda.io/projects/conda/en/latest/user-guide/getting-started.html. </p> <p>GMGI Fisheries has a conda environment set-up with all the packages needed for this workflow. Code below was used to create this conda environment. DO NOT REPEAT every time user is running this workflow.</p> <pre><code># Activate conda\nsource ~/../../work/gmgi/miniconda3/bin/activate\n\n# Creating conda \nconda create --name fisheries_eDNA\n\n# Installing packages needed for this workflow \nconda install -c bioconda fastqc \nconda install multiqc \nconda install bioconda::nextflow \nconda install conda-forge::singularity\nconda install bioconda::blast\nconda install nextflow\n</code></pre> <p>The conda environment is started within each slurm script, but to activate conda environment outside of the slurm script to update packages or check what is installed:</p> <pre><code># Activate conda\nsource ~/../../work/gmgi/miniconda3/bin/activate\n\n# Activate fisheries eDNA conda environment \nconda activate fisheries_eDNA\n\n# List all available environments \nconda env list \n\n# List all packages installed in fisheries_eDNA\nconda list \n\n# Update a package\nconda update [package name]\n\n# Update nextflow ampliseq workflow \nnextflow pull nf-core/ampliseq\n</code></pre>"},{"location":"eDNA/01-Metabarcoding%20ampliseq%2012S%20MiFish/#step-2-assess-quality-of-raw-data","title":"Step 2: Assess quality of raw data","text":"<p>Background information on FASTQC: https://hbctraining.github.io/Intro-to-rnaseq-hpc-salmon-flipped/lessons/05_qc_running_fastqc_interactively.html. </p> <p><code>00-fastqc.sh</code>: </p> <pre><code>#!/bin/bash\n#SBATCH --error=output/fastqc_output/\"%x_error.%j\" #if your job fails, the error report will be put in this file\n#SBATCH --output=output/fastqc_output/\"%x_output.%j\" #once your job is completed, any final job report comments will be put in this file\n#SBATCH --partition=short\n#SBATCH --nodes=1\n#SBATCH --time=5:00:00\n#SBATCH --job-name=fastqc\n#SBATCH --mem=3GB\n#SBATCH --ntasks=24\n#SBATCH --cpus-per-task=2\n\n### USER TO-DO ### \n## 1. Set paths for your project\n\n# Activate conda environment\nsource ~/../../work/gmgi/miniconda3/bin/activate\nconda activate fisheries_eDNA\n\n## SET PATHS \nraw_path=\"\" \nout_dir=\"\" \n\n## CREATE SAMPLE LIST FOR SLURM ARRAY\n### 1. Create list of all .gz files in raw data path\nls -d ${raw_path}/*.gz &gt; ${raw_path}/rawdata\n\n### 2. Create a list of filenames based on that list created in step 1\nmapfile -t FILENAMES &lt; ${raw_path}/rawdata\n\n### 3. Create variable i that will assign each row of FILENAMES to a task ID\ni=${FILENAMES[$SLURM_ARRAY_TASK_ID]}\n\n## RUN FASTQC PROGRAM \nfastqc ${i} --outdir ${out_dir}\n</code></pre> <p>To run:   - Start slurm array (e.g., with 138 files) = <code>sbatch --array=0-136 00-fastqc.sh</code>.</p> <p>Notes: - This is going to output many error and output files. After job completes, use <code>cat *output.* &gt; ../fastqc_output.txt</code> to create one file with all the output and <code>cat *error.* &gt; ../fastqc_error.txt</code> to create one file with all of the error message outputs.  - Within the <code>out_dir</code> output folder, use <code>ls *html | wc</code> to count the number of html output files (1st/2nd column values). This should be equal to the --array range used and the number of raw data files. If not, the script missed some input files so address this before moving on.  </p>"},{"location":"eDNA/01-Metabarcoding%20ampliseq%2012S%20MiFish/#step-3-visualize-quality-of-raw-data","title":"Step 3: Visualize quality of raw data","text":"<p>Background information on MULTIQC: https://multiqc.info/docs/#:~:text=MultiQC%20is%20a%20reporting%20tool%20that%20parses%20results,experiments%20containing%20multiple%20samples%20and%20multiple%20analysis%20steps.</p> <p><code>00-multiqc.sh</code> </p> <pre><code>#!/bin/bash\n#SBATCH --error=output/\"%x_error.%j\" #if your job fails, the error report will be put in this file\n#SBATCH --output=output/\"%x_output.%j\" #once your job is completed, any final job report comments will be put in this file\n#SBATCH --partition=short\n#SBATCH --nodes=1\n#SBATCH --time=10:00:00\n#SBATCH --job-name=multiqc\n#SBATCH --mem=8GB\n#SBATCH --ntasks=24\n#SBATCH --cpus-per-task=2\n\n### USER TO-DO ### \n## 1. Set paths for your project\n## 2. Optional: change file name (multiqc_raw.html) as desired\n\n# Activate conda environment\nsource ~/../../work/gmgi/miniconda3/bin/activate\nconda activate fisheries_eDNA\n\n## SET PATHS \n## fastqc_output = output from 00-fastqc.sh; fastqc program\nfastqc_output=\"\"\nmultiqc_dir=\"\"\n\n## RUN MULTIQC \nmultiqc --interactive ${fastqc_output} -o ${multiqc_dir} --filename multiqc_raw.html\n</code></pre> <p>To run: - <code>sbatch 00-multiqc.sh</code> </p> <p>Notes: - Depending on the number of files per project, multiqc can be quick to run without a slurm script. To do this, run each line separately in the command line after activating the conda environment.  </p>"},{"location":"eDNA/01-Metabarcoding%20ampliseq%2012S%20MiFish/#step-4-nf-coreampliseq","title":"Step 4: nf-core/ampliseq","text":"<p>Nf-core: A community effort to collect a curated set of analysis pipelines built using Nextflow. Nextflow: scalable and reproducible scientific workflows using software containers, used to build wrapper programs like the one we use here.  </p> <p>https://nf-co.re/ampliseq/2.11.0: nfcore/ampliseq is a bioinformatics analysis pipeline used for amplicon sequencing, supporting denoising of any amplicon and supports a variety of taxonomic databases for taxonomic assignment including 16S, ITS, CO1 and 18S. </p> <p></p> <p>We use ampliseq for the following programs: - Cutadapt is trimming primer sequences from sequencing reads. Primer sequences are non-biological sequences that often introduce point mutations that do not reflect sample sequences. This is especially true for degenerated PCR primer. If primer trimming would be omitted, artifactual amplicon sequence variants might be computed by the denoising tool or sequences might be lost due to become labelled as PCR chimera. - DADA2 performs fast and accurate sample inference from amplicon data with single-nucleotide resolution. It infers exact amplicon sequence variants (ASVs) from amplicon data with fewer false positives than many other methods while maintaining high sensitivity.  </p> <p>We skip the taxonomic assignment because we use 3-db approach described in the next section. </p>"},{"location":"eDNA/01-Metabarcoding%20ampliseq%2012S%20MiFish/#12s-primer-sequences-required","title":"12S primer sequences (required)","text":"<p>Below is what we used for 12S amplicon sequencing. Ampliseq will automatically calculate and include the reverse compliment sequence. </p> <p>MiFish-U 12S amplicon F: GTCGGTAAAACTCGTGCCAGC MiFish-U 12S amplicon R: CATAGTGGGGTATCTAATCCCAGTTTG       </p> <p>MiFish-E 12S amplicon F: GTTGGTAAATCTCGTGCCAGC   MiFish-E 12S amplicon R: CATAGTGGGGTATCTAATCCTAGTTTG    </p>"},{"location":"eDNA/01-Metabarcoding%20ampliseq%2012S%20MiFish/#metadata-sheet-optional","title":"Metadata sheet (optional)","text":"<p>The metadata file has to follow the QIIME2 specifications (https://docs.qiime2.org/2021.2/tutorials/metadata/). Below is a preview of the sample sheet used for this test. Keep the column headers the same for future use. The first column needs to be \"ID\" and can only contain numbers, letters, or \"-\". This is different than the sample sheet. NAs should be empty cells rather than \"NA\". </p>"},{"location":"eDNA/01-Metabarcoding%20ampliseq%2012S%20MiFish/#create-samplesheet-sheet-for-ampliseq","title":"Create samplesheet sheet for ampliseq","text":"<p>This file indicates the sample ID and the path to R1 and R2 files. Below is a preview of the sample sheet used in this test. File created on RStudio Interactive on Discovery Cluster using (<code>create_metadatasheets.R</code>).  </p> <ul> <li>sampleID (required): Unique sample IDs, must start with a letter, and can only contain letters, numbers or underscores (no hyphons!).  </li> <li>forwardReads (required): Paths to (forward) reads zipped FastQ files  </li> <li>reverseReads (optional): Paths to reverse reads zipped FastQ files, required if the data is paired-end  </li> <li>run (optional): If the data was produced by multiple sequencing runs, any string  </li> </ul> sampleID forwardReads reverseReads run sample1 ./data/S1_R1_001.fastq.gz ./data/S1_R2_001.fastq.gz A sample2 ./data/S2_fw.fastq.gz ./data/S2_rv.fastq.gz A sample3 ./S4x.fastq.gz ./S4y.fastq.gz B sample4 ./a.fastq.gz ./b.fastq.gz B <p>This is an R script, not slurm script. Open RStudio interactive on Discovery Cluster to run this script.</p> <p>Prior to running R script, use the <code>rawdata</code> file created for the fastqc slurm array from within the raw data folder to create a list of files. Below is an example from our Offshore Wind project but the specifics of the sampleID will be project dependent. This project had four sequencing runs with different file names. </p> <p><code>01a-metadata.R</code></p> <pre><code>## Load libraries \n\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(strex) \n\n### Read in sample sheet \n\nsample_list &lt;- read.delim2(\"/work/gmgi/Fisheries/eDNA/offshore_wind2023/raw_data/rawdata\", header=F) %&gt;% \n  dplyr::rename(forwardReads = V1) %&gt;%\n  mutate(sampleID = str_after_nth(forwardReads, \"data/\", 1),\n         sampleID = str_before_nth(sampleID, \"_R\", 1))\n\n# creating sample ID \nsample_list$sampleID &lt;- gsub(\"-\", \"_\", sample_list$sampleID)\n\n# keeping only rows with R1\nsample_list &lt;- filter(sample_list, grepl(\"R1\", forwardReads, ignore.case = TRUE))\n\n# duplicating column \nsample_list$reverseReads &lt;- sample_list$forwardReads\n\n# replacing R1 with R2 in only one column \nsample_list$reverseReads &lt;- gsub(\"R1\", \"R2\", sample_list$reverseReads)\n\n# rearranging columns \nsample_list &lt;- sample_list[,c(2,1,3)]\n\nsample_list %&gt;% write.csv(\"/work/gmgi/Fisheries/eDNA/offshore_wind2023/metadata/samplesheet.csv\", \n                          row.names=FALSE, quote = FALSE)\n</code></pre>"},{"location":"eDNA/01-Metabarcoding%20ampliseq%2012S%20MiFish/#run-nf-coreampliseq-cutadapt-dada2","title":"Run nf-core/ampliseq (Cutadapt &amp; DADA2)","text":"<p>Update ampliseq workflow if needed: <code>nextflow pull nf-core/ampliseq</code>. </p> <p><code>01b-ampliseq.sh</code>:</p> <pre><code>#!/bin/bash\n#SBATCH --error=output/\"%x_error.%j\" #if your job fails, the error report will be put in this file\n#SBATCH --output=output/\"%x_output.%j\" #once your job is completed, any final job report comments will be put in this file\n#SBATCH --partition=short\n#SBATCH --nodes=1\n#SBATCH --time=20:00:00\n#SBATCH --job-name=ampliseq\n#SBATCH --mem=70GB\n#SBATCH --ntasks=24\n#SBATCH --cpus-per-task=2\n\n### USER TO-DO ### \n## 1. Set paths for project \n## 2. Adjust SBATCH options above (time, mem, ntasks, etc.) as desired  \n## 3. Fill in F and R primer information (no reverse compliment)\n## 4. Adjust parameters as needed (below is Fisheries team default for 12S)\n\n# LOAD MODULES\n# module load singularity/3.10.3\n# module load nextflow/23.10.1\n\n# Activate conda environment\nsource ~/../../work/gmgi/miniconda3/bin/activate\nconda activate fisheries_eDNA\n\n# SET PATHS \nmetadata=\"\" \noutput_dir=\"\"\n\nnextflow run nf-core/ampliseq -resume \\\n   -profile singularity \\\n   --input ${metadata}/samplesheet.csv \\\n   --FW_primer \"\" \\\n   --RV_primer \"\" \\\n   --outdir ${output_dir} \\\n   --trunclenf 100 \\\n   --trunclenr 100 \\\n   --trunc_qmin 25 \\\n   --max_len 200 \\\n   --max_ee 2 \\\n   --min_len_asv 100 \\\n   --max_len_asv 115 \\\n   --sample_inference pseudo \\\n   --skip_taxonomy \\\n   --ignore_failed_trimming\n</code></pre> <p>To run:  - <code>sbatch 01b-ampliseq.sh</code> </p>"},{"location":"eDNA/01-Metabarcoding%20ampliseq%2012S%20MiFish/#files-generated-by-ampliseq","title":"Files generated by ampliseq","text":"<p>Pipeline summary reports: - <code>summary_report/</code> - <code>summary_report.html</code>: pipeline summary report as standalone HTML file that can be viewed in your web browser. - <code>*.svg*</code>: plots that were produced for (and are included in) the report. - <code>versions.yml</code>: software versions used to produce this report.</p> <p>Preprocessing: - FastQC: <code>fastqc/</code> and <code>*_fastqc.html</code>: FastQC report containing quality metrics for your untrimmed raw fastq files. - Cutadapt: <code>cutadapt/</code> and <code>cutadapt_summary.tsv</code>: summary of read numbers that pass cutadapt - MultiQC: <code>multiqc</code>, <code>multiqc_data/</code>, <code>multiqc_plots/</code> with <code>multiqc_report.html</code>: a standalone HTML file that can be viewed in your web browser; </p> <p>ASV inferrence with DADA2: - <code>dada2/</code>, <code>dada2/args/</code>, <code>data2/log/</code>     - <code>ASV_seqs.fasta</code>: Fasta file with ASV sequences.    - <code>ASV_table.tsv</code>: Counts for each ASV sequence.    - <code>DADA2_stats.tsv</code>: Tracking read numbers through DADA2 processing steps, for each sample.    - <code>DADA2_table.rds</code>: DADA2 ASV table as R object.    - <code>DADA2_table.tsv</code>: DADA2 ASV table. - <code>dada2/QC/</code>    - <code>*.err.convergence.txt</code>: Convergence values for DADA2's dada command, should reduce over several magnitudes and approaching 0.    - <code>*.err.pdf</code>: Estimated error rates for each possible transition. The black line shows the estimated error rates after convergence of the machine-learning algorithm. The red line shows the error rates expected under the nominal definition of the Q-score. The estimated error rates (black line) should be a good fit to the observed rates (points), and the error rates should drop with increased quality.    - <code>*_qual_stats.pdf</code>: Overall read quality profiles: heat map of the frequency of each quality score at each base position. The mean quality score at each position is shown by the green line, and the quartiles of the quality score distribution by the orange lines. The red line shows the scaled proportion of reads that extend to at least that position.    - <code>*_preprocessed_qual_stats.pdf</code>: Same as above, but after preprocessing.  </p> <p>We add an ASV length filter that will output <code>asv_length_filter/</code> with: - <code>ASV_seqs.len.fasta</code>: Fasta file with filtered ASV sequences. - <code>ASV_table.len.tsv</code>: Counts for each filtered ASV sequence. - <code>ASV_len_orig.tsv</code>: ASV length distribution before filtering. - <code>ASV_len_filt.tsv</code>: ASV length distribution after filtering. - <code>stats.len.tsv</code>: Tracking read numbers through filtering, for each sample.  </p>"},{"location":"eDNA/01-Metabarcoding%20ampliseq%2012S%20MiFish/#step-5-blast-asv-sequences-output-from-dada2-against-our-3-databases","title":"Step 5: Blast ASV sequences (output from DADA2) against our 3 databases","text":""},{"location":"eDNA/01-Metabarcoding%20ampliseq%2012S%20MiFish/#populating-workgmgidatabases-folder","title":"Populating /work/gmgi/databases folder","text":"<p>We use NCBI, Mitofish, and GMGI-12S databases. </p>"},{"location":"eDNA/01-Metabarcoding%20ampliseq%2012S%20MiFish/#download-nbci","title":"Download NBCI","text":"<p>Option 1: Download ncbi-blast+ to <code>/work/gmgi/packages</code> using <code>wget ftp://ftp.ncbi.nlm.nih.gov/blast/executables/blast+/LATEST/ncbi-blast-2.16.0+-x64-linux.tar.gz</code> and then <code>tar -zxvf ncbi-blast-2.16.0+-x64-linux.tar.gz</code>. NCBI latest: https://ftp.ncbi.nlm.nih.gov/blast/executables/blast+/LATEST/. Once downloaded, user does not need to repeat this. I was struggling with the remote flag here within a slurm script.</p> <p>Option 2: Download 12S sequences from NCBI via CRABS: Creating Reference databases for Amplicon-Based Sequencing.</p> <p>CRABS requires python 3.9 so I created a new conda environment with this version: <code>conda create --name Env_py3.9 python=3.9</code>. Fisheries eDNA uses python 3.12. I was struggling to download CRABS in either conda environment... come back to this.</p> <p>https://github.com/gjeunen/reference_database_creator</p> <pre><code>conda activate Env_py3.9\nconda install -c bioconda crabs\n\ncd /work/gmgi/databases/12S/ncbi\n\ncrabs db_download --source ncbi --database nucleotide --query '12S[All Fields]' --output 12S_ncbi_[date].fasta --keep_original no --batchsize 5000\n\nmakeblastdb -in 12S_ncbi_[date].fasta -dbtype nucl -out 12S_ncbi_[date] -parse_seqids\n</code></pre> <p>Option 3: <code>blast</code> package is downloaded in the fisheries_eDNA conda environment with <code>conda install blast</code>. Install nt database <code>update_blastdb.pl --decompress nt</code> once inside the <code>work/gmgi/databases/ncbi/nt</code> folder. This is not ideal because it will take up more space and need to be updated.</p>"},{"location":"eDNA/01-Metabarcoding%20ampliseq%2012S%20MiFish/#download-mitofish","title":"Download Mitofish","text":"<p>Check Mitofish webpage (https://mitofish.aori.u-tokyo.ac.jp/download/) for the most recent database version number. Compare to the <code>work/gmgi/databases/12S/reference_fasta/12S/Mitofish/</code> folder. If needed, update Mitofish database:</p> <pre><code>## download db \nwget https://mitofish.aori.u-tokyo.ac.jp/species/detail/download/?filename=download%2F/complete_partial_mitogenomes.zip  \n\n## unzip \nunzip 'index.html?filename=download%2F%2Fcomplete_partial_mitogenomes.zip'\n\n## clean headers \nawk '/^&gt;/ {print $1} !/^&gt;/ {print}' mito-all &gt; Mitofish_v4.02.fasta\n\n## remove excess files \nrm mito-all* \nrm index*\n\n## make NCBI db \n## make sure fisheries_eDNA conda environment is activated or module load ncbi-blast+/2.13.0\nmakeblastdb -in Mitofish_v4.02.fasta -dbtype nucl -out Mitofish_v4.02.fasta -parse_seqids\n</code></pre> <p>Alternate option: Download Mitofish db with CRABS. This program and will download and format the db accordingly.   </p> <pre><code>git clone https://github.com/gjeunen/reference_database_creator.git\n\n## Download Mitofish \ncrabs db_download --source mitofish --output /work/gmgi/databases/12S/Mitofish/mitofish.fasta --keep_original yes\n### I couldn't get the function crabs to work \n</code></pre>"},{"location":"eDNA/01-Metabarcoding%20ampliseq%2012S%20MiFish/#running-taxonomic-id-script","title":"Running taxonomic ID script","text":"<p><code>02-taxonomicID.sh</code>: </p> <pre><code>#!/bin/bash\n#SBATCH --error=output/\"%x_error.%j\" #if your job fails, the error report will be put in this file\n#SBATCH --output=output/\"%x_output.%j\" #once your job is completed, any final job report comments will be put in this file\n#SBATCH --partition=short\n#SBATCH --nodes=1\n#SBATCH --time=20:00:00\n#SBATCH --job-name=tax_ID\n#SBATCH --mem=30GB\n#SBATCH --ntasks=24\n#SBATCH --cpus-per-task=2\n\n### USER TO-DO ### \n## 1. Set paths for project; change db path if not 12S\n\n## LOAD MODULES \n## can use module on NU cluster or own ncbi-blast+\n# module load ncbi-blast+/2.13.0\nncbi_program=\"/work/gmgi/packages/ncbi-blast-2.16.0+\"\n\n# Activate conda environment\nsource ~/../../work/gmgi/miniconda3/bin/activate\nconda activate fisheries_eDNA\n\n# SET PATHS \nASV_fasta=\"\"\nout=\"\"\n\ngmgi=\"/work/gmgi/databases/12S/GMGI\"\nmito=\"/work/gmgi/databases/12S/Mitofish\"\ntaxonkit=\"/work/gmgi/databases/taxonkit\"\n\n#### DATABASE QUERY ####\n### NCBI database \nblastn -remote -db nt \\\n   -query ${ASV_fasta}/ASV_seqs.len.fasta \\\n   -out ${out}/BLASTResults_NCBI.txt \\\n   -max_target_seqs 10 -perc_identity 100 -qcov_hsp_perc 95 \\\n   -outfmt '6  qseqid   sseqid   sscinames   staxid pident   length   mismatch gapopen  qstart   qend  sstart   send  evalue   bitscore' \\\n   -verbose\n\n## Mitofish database \nblastn -db ${mito}/*.fasta \\\n   -query ${ASV_fasta}/ASV_seqs.len.fasta \\\n   -out ${out}/BLASTResults_Mito.txt \\\n   -max_target_seqs 10 -perc_identity 100 -qcov_hsp_perc 95 \\\n   -outfmt '6  qseqid   sseqid  pident   length   mismatch gapopen  qstart   qend  sstart   send  evalue   bitscore'\n\n############################\n\n#### TAXONOMIC CLASSIFICATION #### \n## creating list of staxids from all three files \nawk -F $'\\t' '{ print $4}' ${out}/BLASTResults_NCBI.txt | sort -u &gt; ${out}/NCBI_sp.txt\n\n## annotating taxid with full taxonomic classification\ncat ${out}/NCBI_sp.txt | ${taxonkit}/taxonkit reformat -I 1 -r \"Unassigned\" &gt; ${out}/NCBI_taxassigned.txt\n</code></pre> <p>To run: - <code>sbatch 02-taxonomicID.sh</code> </p>"},{"location":"eDNA/01-Metabarcoding%20ampliseq%2012S%20Riaz/","title":"Metabarcoding workflow for 12S amplicon sequencing with Riaz primers","text":"<p>page details in progress. </p> <p>The 12S rRNA gene region of the mitogenome is ~950 bp. There are two popular primer sets to amplify two different regions of 12S: Riaz and MiFish. The following workflow includes script specific to the Riaz primer set.</p> <p></p> <p>Citation: Riaz et al. 2011</p> <p>Workflow done on HPC. Scripts to run: </p> <ol> <li>00-fastqc.sh   </li> <li>00-multiqc.sh  </li> <li>01a-metadata.R</li> <li>01b-ampliseq.sh</li> <li>02-taxonomicID.sh  </li> </ol>"},{"location":"eDNA/01-Metabarcoding%20ampliseq%2012S%20Riaz/#step-1-conda-environment-fisheries-edna","title":"Step 1: Conda environment: Fisheries eDNA","text":"<p>Background information on Conda. </p> <p>GMGI Fisheries has a conda environment set-up with all the packages needed for this workflow. Code below was used to create this conda environment. DO NOT REPEAT every time user is running this workflow.</p> <pre><code># Activate conda\nsource ~/../../work/gmgi/miniconda3/bin/activate\n\n# Creating conda \nconda create --name fisheries_eDNA\n\n# Installing packages needed for this workflow \nconda install -c bioconda fastqc \nconda install multiqc \nconda install bioconda::nextflow \nconda install conda-forge::singularity\nconda install bioconda::blast\nconda install nextflow\nconda install blast\nconda install singularity\n</code></pre> <p>The conda environment is started within each slurm script, but to activate conda environment outside of the slurm script to update packages or check what is installed:</p> <pre><code># Activate conda\nsource ~/../../work/gmgi/miniconda3/bin/activate\n\n# Activate fisheries eDNA conda environment \nconda activate fisheries_eDNA\n\n# List all available environments \nconda env list \n\n# List all packages installed in fisheries_eDNA\nconda list \n\n# Update a package\nconda update [package name]\n\n# Update nextflow ampliseq workflow \nnextflow pull nf-core/ampliseq\n</code></pre>"},{"location":"eDNA/01-Metabarcoding%20ampliseq%2012S%20Riaz/#step-2-assess-quality-of-raw-data","title":"Step 2: Assess quality of raw data","text":"<p>Background information on FASTQC. </p> <p><code>00-fastqc.sh</code>: </p> <pre><code>#!/bin/bash\n#SBATCH --error=output/fastqc_output/\"%x_error.%j\" #if your job fails, the error report will be put in this file\n#SBATCH --output=output/fastqc_output/\"%x_output.%j\" #once your job is completed, any final job report comments will be put in this file\n#SBATCH --partition=short\n#SBATCH --nodes=1\n#SBATCH --time=20:00:00\n#SBATCH --job-name=fastqc\n#SBATCH --mem=3GB\n#SBATCH --ntasks=24\n#SBATCH --cpus-per-task=2\n\n### USER TO-DO ### \n## 1. Set paths for your project\n\n# Activate conda environment\nsource ~/../../work/gmgi/miniconda3/bin/activate\nconda activate fisheries_eDNA\n\n## SET PATHS \nraw_path=\"\"\nout_dir=\"\"\n\n## CREATE SAMPLE LIST FOR SLURM ARRAY\n### 1. Create list of all .gz files in raw data path\nls -d ${raw_path}/*.gz &gt; ${raw_path}/rawdata\n\n### 2. Create a list of filenames based on that list created in step 1\nmapfile -t FILENAMES &lt; ${raw_path}/rawdata\n\n### 3. Create variable i that will assign each row of FILENAMES to a task ID\ni=${FILENAMES[$SLURM_ARRAY_TASK_ID]}\n\n## RUN FASTQC PROGRAM \nfastqc ${i} --outdir ${out_dir}\n</code></pre> <p>To run:   - Start slurm array (e.g., with 138 files) = <code>sbatch --array=0-137 00-fastqc.sh</code>.</p> <p>Notes: - This is going to output many error and output files. After job completes, use <code>cat *output.* &gt; ../fastqc_output.txt</code> to create one file with all the output and <code>cat *error.* &gt; ../fastqc_error.txt</code> to create one file with all of the error message outputs.  - Within the <code>out_dir</code> output folder, use <code>ls *html | wc</code> to count the number of html output files (1st/2nd column values). This should be equal to the --array range used and the number of raw data files. If not, the script missed some input files so address this before moving on.  </p>"},{"location":"eDNA/01-Metabarcoding%20ampliseq%2012S%20Riaz/#step-3-visualize-quality-of-raw-data","title":"Step 3: Visualize quality of raw data","text":"<p>Background information on MULTIQC.</p> <p><code>00-multiqc.sh</code> </p> <pre><code>#!/bin/bash\n#SBATCH --error=output/\"%x_error.%j\" #if your job fails, the error report will be put in this file\n#SBATCH --output=output/\"%x_output.%j\" #once your job is completed, any final job report comments will be put in this file\n#SBATCH --partition=short\n#SBATCH --nodes=1\n#SBATCH --time=10:00:00\n#SBATCH --job-name=multiqc\n#SBATCH --mem=8GB\n#SBATCH --ntasks=24\n#SBATCH --cpus-per-task=2\n\n### USER TO-DO ### \n## 1. Set paths for your project\n## 2. Optional: change file name (multiqc_raw.html) as desired\n\n# Activate conda environment\nsource ~/../../work/gmgi/miniconda3/bin/activate\nconda activate fisheries_eDNA\n\n## SET PATHS \n## fastqc_output = output from 00-fastqc.sh; fastqc program\nfastqc_output=\"\" \nmultiqc_dir=\"\" \n\n## RUN MULTIQC \nmultiqc --interactive ${fastqc_output} -o ${multiqc_dir} --filename multiqc_raw.html\n</code></pre> <p>To run: - <code>sbatch 00-multiqc.sh</code> </p> <p>Notes: - Depending on the number of files per project, multiqc can be quick to run without a slurm script. To do this, run each line separately in the command line after activating the conda environment.  </p>"},{"location":"eDNA/01-Metabarcoding%20ampliseq%2012S%20Riaz/#step-4-nf-coreampliseq","title":"Step 4: nf-core/ampliseq","text":"<p>Nf-core: A community effort to collect a curated set of analysis pipelines built using Nextflow. Nextflow: scalable and reproducible scientific workflows using software containers, used to build wrapper programs like the one we use here.  </p> <p>[https://nf-co.re/ampliseq/2.11.0]: nfcore/ampliseq is a bioinformatics analysis pipeline used for amplicon sequencing, supporting denoising of any amplicon and supports a variety of taxonomic databases for taxonomic assignment including 16S, ITS, CO1 and 18S. </p> <p></p> <p>We use ampliseq for the following programs: - Cutadapt is trimming primer sequences from sequencing reads. Primer sequences are non-biological sequences that often introduce point mutations that do not reflect sample sequences. This is especially true for degenerated PCR primer. If primer trimming would be omitted, artifactual amplicon sequence variants might be computed by the denoising tool or sequences might be lost due to become labelled as PCR chimera. - DADA2 performs fast and accurate sample inference from amplicon data with single-nucleotide resolution. It infers exact amplicon sequence variants (ASVs) from amplicon data with fewer false positives than many other methods while maintaining high sensitivity.  </p> <p>We skip the taxonomic assignment because we use 3-db approach described in the next section. </p>"},{"location":"eDNA/01-Metabarcoding%20ampliseq%2012S%20Riaz/#12s-primer-sequences-required","title":"12S primer sequences (required)","text":"<p>Below is what we used for 12S amplicon sequencing. Ampliseq will automatically calculate the reverse compliment and include this for us.</p> <p>Riaz 12S amplicon F Original: ACTGGGATTAGATACCCC Riaz 12S amplicon F Degenerate: ACTGGGATTAGATACCCY    Riaz 12S amplicon R: TAGAACAGGCTCCTCTAG     </p>"},{"location":"eDNA/01-Metabarcoding%20ampliseq%2012S%20Riaz/#metadata-sheet-optional","title":"Metadata sheet (optional)","text":"<p>The metadata file has to follow the QIIME2 specifications. Below is a preview of the sample sheet used for this test. Keep the column headers the same for future use. The first column needs to be \"ID\" and can only contain numbers, letters, or \"-\". This is different than the sample sheet. NAs should be empty cells rather than \"NA\". </p>"},{"location":"eDNA/01-Metabarcoding%20ampliseq%2012S%20Riaz/#create-samplesheet-sheet-for-ampliseq","title":"Create samplesheet sheet for ampliseq","text":"<p>This file indicates the sample ID and the path to R1 and R2 files. Below is a preview of the sample sheet used in this test. File created on RStudio Interactive on Discovery Cluster using (<code>create_metadatasheets.R</code>).  </p> <ul> <li>sampleID (required): Unique sample IDs, must start with a letter, and can only contain letters, numbers or underscores (no hyphons!).  </li> <li>forwardReads (required): Paths to (forward) reads zipped FastQ files  </li> <li>reverseReads (optional): Paths to reverse reads zipped FastQ files, required if the data is paired-end  </li> <li>run (optional): If the data was produced by multiple sequencing runs, any string  </li> </ul> sampleID forwardReads reverseReads run sample1 ./data/S1_R1_001.fastq.gz ./data/S1_R2_001.fastq.gz A sample2 ./data/S2_fw.fastq.gz ./data/S2_rv.fastq.gz A sample3 ./S4x.fastq.gz ./S4y.fastq.gz B sample4 ./a.fastq.gz ./b.fastq.gz B <p>This is an R script, not slurm script. Open RStudio interactive on Discovery Cluster to run this script.</p> <p>Prior to running R script, use the <code>rawdata</code> file created for the fastqc slurm array from within the raw data folder to create a list of files. Below is an example from our Offshore Wind project but the specifics of the sampleID will be project dependent. This project had four sequencing runs with different file names. </p> <p><code>01a-metadata.R</code></p> <pre><code>## Load libraries \n\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(strex) \n\n### Read in sample sheet \n\nsample_list &lt;- read.delim2(\"/work/gmgi/Fisheries/eDNA/offshore_wind2023/raw_data/rawdata\", header=F) %&gt;% \n  dplyr::rename(forwardReads = V1) %&gt;%\n  mutate(sampleID = str_after_nth(forwardReads, \"data/\", 1),\n         sampleID = str_before_nth(sampleID, \"_R\", 1))\n\n# creating sample ID \nsample_list$sampleID &lt;- gsub(\"-\", \"_\", sample_list$sampleID)\n\n# keeping only rows with R1\nsample_list &lt;- filter(sample_list, grepl(\"R1\", forwardReads, ignore.case = TRUE))\n\n# duplicating column \nsample_list$reverseReads &lt;- sample_list$forwardReads\n\n# replacing R1 with R2 in only one column \nsample_list$reverseReads &lt;- gsub(\"R1\", \"R2\", sample_list$reverseReads)\n\n# rearranging columns \nsample_list &lt;- sample_list[,c(2,1,3)]\n\nsample_list %&gt;% write.csv(\"/work/gmgi/Fisheries/eDNA/offshore_wind2023/metadata/samplesheet.csv\", \n                          row.names=FALSE, quote = FALSE)\n</code></pre>"},{"location":"eDNA/01-Metabarcoding%20ampliseq%2012S%20Riaz/#run-nf-coreampliseq-cutadapt-dada2","title":"Run nf-core/ampliseq (Cutadapt &amp; DADA2)","text":"<p>Update ampliseq workflow if needed: <code>nextflow pull nf-core/ampliseq</code>. </p> <p><code>01b-ampliseq.sh</code>:</p> <pre><code>#!/bin/bash\n#SBATCH --error=output/\"%x_error.%j\" #if your job fails, the error report will be put in this file\n#SBATCH --output=output/\"%x_output.%j\" #once your job is completed, any final job report comments will be put in this file\n#SBATCH --partition=short\n#SBATCH --nodes=1\n#SBATCH --time=20:00:00\n#SBATCH --job-name=ampliseq\n#SBATCH --mem=70GB\n#SBATCH --ntasks=24\n#SBATCH --cpus-per-task=2\n\n### USER TO-DO ### \n## 1. Set paths for project \n## 2. Adjust SBATCH options above (time, mem, ntasks, etc.) as desired  \n## 3. Fill in F primer information based on primer type (no reverse compliment needed)\n## 4. Adjust parameters as needed (below is Fisheries team default for 12S)\n\n# LOAD MODULES\nmodule load singularity/3.10.3\nmodule load nextflow/23.10.1\n\n# SET PATHS \nmetadata=\"\" \noutput_dir=\"\"\n\nnextflow run nf-core/ampliseq -resume \\\n   -profile singularity \\\n   --input ${metadata}/samplesheet.csv \\\n   --FW_primer \"\" \\\n   --RV_primer \"TAGAACAGGCTCCTCTAG\" \\\n   --outdir ${output_dir} \\\n   --trunclenf 100 \\\n   --trunclenr 100 \\\n   --trunc_qmin 25 \\\n   --max_len 200 \\\n   --max_ee 2 \\\n   --min_len_asv 100 \\\n   --max_len_asv 115 \\\n   --sample_inference pseudo \\\n   --skip_taxonomy \\\n   --ignore_failed_trimming\n</code></pre> <p>To run:  - <code>sbatch 01b-ampliseq.sh</code> </p>"},{"location":"eDNA/01-Metabarcoding%20ampliseq%2012S%20Riaz/#files-generated-by-ampliseq","title":"Files generated by ampliseq","text":"<p>Pipeline summary reports: - <code>summary_report/</code> - <code>summary_report.html</code>: pipeline summary report as standalone HTML file that can be viewed in your web browser. - <code>*.svg*</code>: plots that were produced for (and are included in) the report. - <code>versions.yml</code>: software versions used to produce this report.</p> <p>Preprocessing: - FastQC: <code>fastqc/</code> and <code>*_fastqc.html</code>: FastQC report containing quality metrics for your untrimmed raw fastq files. - Cutadapt: <code>cutadapt/</code> and <code>cutadapt_summary.tsv</code>: summary of read numbers that pass cutadapt - MultiQC: <code>multiqc</code>, <code>multiqc_data/</code>, <code>multiqc_plots/</code> with <code>multiqc_report.html</code>: a standalone HTML file that can be viewed in your web browser; </p> <p>ASV inferrence with DADA2: - <code>dada2/</code>, <code>dada2/args/</code>, <code>data2/log/</code>     - <code>ASV_seqs.fasta</code>: Fasta file with ASV sequences.    - <code>ASV_table.tsv</code>: Counts for each ASV sequence.    - <code>DADA2_stats.tsv</code>: Tracking read numbers through DADA2 processing steps, for each sample.    - <code>DADA2_table.rds</code>: DADA2 ASV table as R object.    - <code>DADA2_table.tsv</code>: DADA2 ASV table. - <code>dada2/QC/</code>    - <code>*.err.convergence.txt</code>: Convergence values for DADA2's dada command, should reduce over several magnitudes and approaching 0.    - <code>*.err.pdf</code>: Estimated error rates for each possible transition. The black line shows the estimated error rates after convergence of the machine-learning algorithm. The red line shows the error rates expected under the nominal definition of the Q-score. The estimated error rates (black line) should be a good fit to the observed rates (points), and the error rates should drop with increased quality.    - <code>*_qual_stats.pdf</code>: Overall read quality profiles: heat map of the frequency of each quality score at each base position. The mean quality score at each position is shown by the green line, and the quartiles of the quality score distribution by the orange lines. The red line shows the scaled proportion of reads that extend to at least that position.    - <code>*_preprocessed_qual_stats.pdf</code>: Same as above, but after preprocessing.  </p> <p>We add an ASV length filter that will output <code>asv_length_filter/</code> with: - <code>ASV_seqs.len.fasta</code>: Fasta file with filtered ASV sequences. - <code>ASV_table.len.tsv</code>: Counts for each filtered ASV sequence. - <code>ASV_len_orig.tsv</code>: ASV length distribution before filtering. - <code>ASV_len_filt.tsv</code>: ASV length distribution after filtering. - <code>stats.len.tsv</code>: Tracking read numbers through filtering, for each sample.  </p>"},{"location":"eDNA/01-Metabarcoding%20ampliseq%2012S%20Riaz/#step-5-blast-asv-sequences-output-from-dada2-against-our-3-databases","title":"Step 5: Blast ASV sequences (output from DADA2) against our 3 databases","text":""},{"location":"eDNA/01-Metabarcoding%20ampliseq%2012S%20Riaz/#populating-workgmgidatabases-folder","title":"Populating /work/gmgi/databases folder","text":"<p>We use NCBI, Mitofish, and GMGI-12S databases. </p>"},{"location":"eDNA/01-Metabarcoding%20ampliseq%2012S%20Riaz/#download-andor-update-nbci-blast-nt-database","title":"Download and/or update NBCI blast nt database","text":"<p>NCBI is updated daily and therefore needs to be updated each time a project is analyzed. This is the not the most ideal method but we were struggling to get the <code>-remote</code> flag to work within slurm because I don't think NU slurm is connected to the internet? NU help desk was helping for awhile but we didn't get anywhere.</p> <p>Within <code>/work/gmgi/databases/ncbi</code>, there is a <code>update_nt.sh</code> script with the following code. To run <code>sbatch update_nt.sh</code>. This won't take long as it will check for updates rather than re-downloading every time. </p> <pre><code>#!/bin/bash\n#SBATCH --partition=short\n#SBATCH --nodes=1\n#SBATCH --time=24:00:00\n#SBATCH --job-name=update_ncbi_nt\n#SBATCH --mem=50G\n#SBATCH --output=%x_%j.out\n#SBATCH --error=%x_%j.err\n\n# Activate conda environment\nsource /work/gmgi/miniconda3/bin/activate fisheries_eDNA\n\n# Create output directory if it doesn't exist\ncd /work/gmgi/databases/ncbi/nt\n\n# Update BLAST nt database\nupdate_blastdb.pl --decompress nt\n\n# Print completion message\necho \"BLAST nt database update completed\"\n</code></pre> <p>View the <code>update_ncbi_nt.out</code> file to confirm the echo printed at the end.</p> <p>Emma is still troubleshooting the -remote flag to also avoid storing the nt db within our /work/gmgi folder. </p>"},{"location":"eDNA/01-Metabarcoding%20ampliseq%2012S%20Riaz/#download-andor-update-mitofish-database","title":"Download and/or update Mitofish database","text":"<p>Check Mitofish webpage for the most recent database version number. Compare to the <code>work/gmgi/databases/12S/reference_fasta/12S/Mitofish/</code> folder. If needed, update Mitofish database:</p> <pre><code>## download db \nwget https://mitofish.aori.u-tokyo.ac.jp/species/detail/download/?filename=download%2F/complete_partial_mitogenomes.zip  \n\n## unzip \nunzip 'index.html?filename=download%2F%2Fcomplete_partial_mitogenomes.zip'\n\n## clean headers \nawk '/^&gt;/ {print $1} !/^&gt;/ {print}' mito-all &gt; Mitofish_v4.02.fasta\n\n## remove excess files \nrm mito-all* \nrm index*\n\n## make NCBI db \n## make sure fisheries_eDNA conda environment is activated or module load ncbi-blast+/2.13.0\nmakeblastdb -in Mitofish_v4.02.fasta -dbtype nucl -out Mitofish_v4.02.fasta -parse_seqids\n</code></pre>"},{"location":"eDNA/01-Metabarcoding%20ampliseq%2012S%20Riaz/#download-gmgi-12s","title":"Download GMGI 12S","text":"<p>This is our in-house GMGI database that will include version numbers. Check <code>/work/gmgi/databases/12S/GMGI/</code> for current uploaded version number and check our Box folder for the most recent version number. </p> <p>On OOD portal, click the Interactive Apps dropdown. Select Home Directory under the HTML Viewer section. Navigate to the <code>/work/gmgi/databases/12S/GMGI/</code> folder. In the top right hand corner of the portal, select Upload and add the most recent .fasta file from our Box folder. </p> <p>To create a blast db from this reference fasta file (if updated): </p> <pre><code>cd /work/gmgi/databases/12S/GMGI/ \n\n## make NCBI db \n## make sure fisheries_eDNA conda environment is activated \n### CHANGE THE VERSION NUMBER BELOW TO LATEST\nmakeblastdb -in GMGI_Vert_Ref_2024v1.fasta -dbtype nucl -out GMGI_Vert_Ref_2024v1.fasta\n</code></pre>"},{"location":"eDNA/01-Metabarcoding%20ampliseq%2012S%20Riaz/#running-taxonomic-id-script","title":"Running taxonomic ID script","text":"<p><code>02-taxonomicID.sh</code>: </p> <pre><code>#!/bin/bash\n#SBATCH --error=output/\"%x_error.%j\" #if your job fails, the error report will be put in this file\n#SBATCH --output=output/\"%x_output.%j\" #once your job is completed, any final job report comments will be put in this file\n#SBATCH --partition=short\n#SBATCH --nodes=1\n#SBATCH --time=20:00:00\n#SBATCH --job-name=tax_ID\n#SBATCH --mem=30GB\n#SBATCH --ntasks=24\n#SBATCH --cpus-per-task=2\n\n### USER TO-DO ### \n## 1. Set paths for project; change db path if not 12S\n\n# Activate conda environment\nsource /work/gmgi/miniconda3/bin/activate\nconda activate fisheries_eDNA\n\n# SET PATHS \nASV_fasta=\"\"\nout=\"\"\n\ngmgi=\"/work/gmgi/databases/12S/GMGI\"\nmito=\"/work/gmgi/databases/12S/Mitofish\"\nncbi=\"/work/gmgi/databases/ncbi/nt\"\ntaxonkit=\"/work/gmgi/databases/taxonkit\"\n\n#### DATABASE QUERY ####\n### NCBI database \nblastn -db ${ncbi}/*.fasta \\\n   -query ${ASV_fasta}/ASV_seqs.len.fasta \\\n   -out ${out}/BLASTResults_NCBI.txt \\\n   -max_target_seqs 10 -perc_identity 100 -qcov_hsp_perc 95 \\\n   -outfmt '6  qseqid   sseqid   sscinames   staxid pident   length   mismatch gapopen  qstart   qend  sstart   send  evalue   bitscore' \\\n   -verbose\n\n## Mitofish database \nblastn -db ${mito}/*.fasta \\\n   -query ${ASV_fasta}/ASV_seqs.len.fasta \\\n   -out ${out}/BLASTResults_Mito.txt \\\n   -max_target_seqs 10 -perc_identity 100 -qcov_hsp_perc 95 \\\n   -outfmt '6  qseqid   sseqid  pident   length   mismatch gapopen  qstart   qend  sstart   send  evalue   bitscore'\n\n## GMGI database \nblastn -db ${gmgi}/*.fasta \\\n   -query ${ASV_fasta}/ASV_seqs.len.fasta \\\n   -out ${out}/BLASTResults_GMGI.txt \\\n   -max_target_seqs 10 -perc_identity 98 -qcov_hsp_perc 95 \\\n   -outfmt '6  qseqid   sseqid   pident   length   mismatch gapopen  qstart   qend  sstart   send  evalue   bitscore'\n\n############################\n\n#### TAXONOMIC CLASSIFICATION #### \n## creating list of staxids from all three files \nawk -F $'\\t' '{ print $4}' ${out}/BLASTResults_NCBI.txt | sort -u &gt; ${out}/NCBI_sp.txt\n\n## annotating taxid with full taxonomic classification\ncat ${out}/NCBI_sp.txt | ${taxonkit}/taxonkit reformat -I 1 -r \"Unassigned\" &gt; ${out}/NCBI_taxassigned.txt\n</code></pre> <p>To run: - <code>sbatch 02-taxonomicID.sh</code> </p>"},{"location":"eDNA/02-datatable_prep_12S_Riaz/","title":"Datatable preparation base script for eDNA metabarcoding","text":"<p>.Rmd script</p> <p>This script takes your Blast output from the GMGI database, Mitofish database, and NCBI database to create one datatable with read counts and taxonomic assignment.</p> <p>Workflow summary: 1. Load libraries 2. Load metadata 3. Load BLAST output from GMGI, Mitofish, and NCBI 4. Load DADA2 ASV Table 5. Taxonomic Assignment - 5a. Identify ASVs with multiple hits from GMGI\u2019s database - 5b. Identify entries that mismatch between GMGI, Mitofish, and NCBI databases - 5c. Assign taxonomy based on hierarchical approach - 5d. Edit taxonomy annotations based on mismatch table choices - 5e. Adjusting common name and category for those entries that don\u2019t have one (from Mito or NCBI) 6. Filtering: Filter ASV by less than 0.1% reads and then collapse by group 7. Collapsing read counts by species name 8. Creating results output</p>"},{"location":"eDNA/02-datatable_prep_12S_Riaz/#load-libraries","title":"Load libraries","text":"<pre><code>library(ggplot2) ## for plotting\nlibrary(dplyr) ## for data table manipulation\n</code></pre> <pre><code>## \n## Attaching package: 'dplyr'\n\n## The following objects are masked from 'package:stats':\n## \n##     filter, lag\n\n## The following objects are masked from 'package:base':\n## \n##     intersect, setdiff, setequal, union\n</code></pre> <pre><code>library(tidyr) ## for data table manipulation\nlibrary(readr) ## for reading in tsv files\nlibrary(readxl) ## for reading in excel files\nlibrary(stringr) ## for data transformation\nlibrary(strex) ## for data transformation\nlibrary(writexl) ## for excel output\nlibrary(purrr) ## for data transformation\nlibrary(funrar) ## for make_relative()\nlibrary(tidyverse) ## for data table manipulation\n</code></pre> <pre><code>## \u2500\u2500 Attaching core tidyverse packages \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 tidyverse 2.0.0 \u2500\u2500\n## \u2714 forcats   1.0.0     \u2714 tibble    3.2.1\n## \u2714 lubridate 1.9.3\n\n## \u2500\u2500 Conflicts \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 tidyverse_conflicts() \u2500\u2500\n## \u2716 dplyr::filter() masks stats::filter()\n## \u2716 dplyr::lag()    masks stats::lag()\n## \u2139 Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n</code></pre>"},{"location":"eDNA/02-datatable_prep_12S_Riaz/#metadata-input","title":"Metadata input","text":""},{"location":"eDNA/02-datatable_prep_12S_Riaz/#identify-paths-for-metadata-and-project-data","title":"Identify paths for metadata and project data","text":"<p>Each user needs to write in their specific directory outputs prior to the file name. The default working directory is this document so the folder where this script is saved for the user. To change the workign directory to the Rproject directory, select \u2018Knit\u2019 and \u2018Knit Directory\u2019 &gt; \u2018Project Directory\u2019.</p> <pre><code>### User edits:\n### 1. change paths of input and output as desired \n\n## GMGI Fish database\npath_GMGIdb = \"../../../../Projects/eDNA/Metabarcoding Lab Resources/Reference Databases/GMGI_Vert_Ref.xlsx\"\npath_fishbase_tax = \"../../../../Projects/eDNA/Metabarcoding Lab Resources/Reference Databases/taxonomic_classification_fishbase.csv\"\npath_mitofish_tax = \"../../../../Projects/eDNA/Metabarcoding Lab Resources/Reference Databases/taxonomic_classification_mitofish.csv\"\n\n## BLAST results\npath_blast_gmgi = \"example_input/BLASTResults_GMGI.txt\"\npath_blast_mito = \"example_input/BLASTResults_Mito.txt\"\npath_blast_ncbi_taxassigned = \"example_input/NCBI_taxassigned.txt\"\npath_blast_ncbi = \"example_input/BLASTResults_NCBI.txt\"\n\n## ASV table results \n## confirm that the ASV_table.len.tsv name is correct for user's project\npath_asv_table = \"example_input/ASV_table.len.tsv\"\npath_output_summary = \"example_input/overall_summary.tsv\"\n\n# output paths \npath_choice_required = \"example_output/Taxonomic_assignments/Choice_required_GMGI_multiplehits.xlsx\"\npath_disagree_list = \"example_output/Taxonomic_assignments/SampleReport_taxonomic_ID.xlsx\"\n\nresults_rawreads_matrix = \"example_output/Results_rawreads_matrix.xlsx\"\nresults_rawreads_long = \"example_output/Results_rawreads_long.xlsx\"\nresults_relab_matrix = \"example_output/Results_relab_matrix.xlsx\"\nresults_relab_long = \"example_output/Results_relab_long.xlsx\"\n</code></pre>"},{"location":"eDNA/02-datatable_prep_12S_Riaz/#load-project-metadata","title":"Load project metadata","text":"<p>Metadata specific to each project. This contains information about each sample (e.g., month, site, time, sample type, etc.). Confirm that sample IDs match those used in the ASV_table.len.tsv file.</p> <pre><code>### User edits:\n### 1. change path of metadata file\n\n## EXCEL\n# meta &lt;- read_excel(\"example_input/metadata.xlsx\")\n## CSV \nmeta &lt;- read.csv(\"example_input/metadata.csv\", header = TRUE)\n</code></pre>"},{"location":"eDNA/02-datatable_prep_12S_Riaz/#load-database-metadata","title":"Load database metadata","text":"<p>No user edits in this section because paths have already been set above.</p> <pre><code># Load GMGI database information (common name, species name, etc.)\ngmgi_db &lt;- read_xlsx(path_GMGIdb, sheet = 1) %&gt;% dplyr::rename(sseqid = Ref) %&gt;%\n  ## removing &gt; from beginning of entires within Ref column\n  mutate(sseqid = gsub(\"&gt;\", \"\", sseqid))\n</code></pre>"},{"location":"eDNA/02-datatable_prep_12S_Riaz/#blast-data-input","title":"BLAST data input","text":"<p>No user edits unless user changed blastn parameters from fisheries team default.</p> <pre><code>## Setting column header names and classes\nblast_col_headers = c(\"ASV_ID\", \"sseqid\", \"pident\", \"length\", \"mismatch\", \"gapopen\",\n                                        \"qstart\", \"qend\", \"sstart\", \"send\", \"evalue\", \"bitscore\")\nblast_col_classes = c(rep(\"character\", 2), rep(\"numeric\", 10))\n</code></pre>"},{"location":"eDNA/02-datatable_prep_12S_Riaz/#gmgi-database","title":"GMGI database","text":"<p>No user edits.</p> <pre><code>Blast_GMGI &lt;- read.table(path_blast_gmgi, header=F, col.names = blast_col_headers, colClasses = blast_col_classes) %&gt;%\n  ## blast changes spaces to hyphons so we need to change that back to match our metadata\n  mutate(sseqid = gsub(\"-\", \" \", sseqid)) %&gt;%\n  ## join with GMGI database information\n  left_join(., gmgi_db, by = \"sseqid\")\n\n## Check how many ASVs were identified with the GMGI Database\nlength(unique(Blast_GMGI$ASV_ID)) \n</code></pre> <pre><code>## [1] 61\n</code></pre>"},{"location":"eDNA/02-datatable_prep_12S_Riaz/#mitofish-database","title":"Mitofish database","text":"<p>No user edits.</p> <pre><code>Blast_Mito &lt;- read.table(path_blast_mito, header=F, col.names = blast_col_headers, colClasses = blast_col_classes) %&gt;%\n  # renaming sseqid to species name\n  dplyr::rename(Species_name = sseqid) %&gt;%\n\n  # replacing _ with spaces\n  mutate(Species_name = gsub(\"_\", \" \", Species_name))\n</code></pre>"},{"location":"eDNA/02-datatable_prep_12S_Riaz/#ncbi-database","title":"NCBI database","text":"<p>No user edits.</p> <pre><code>NCBI_taxassigned &lt;- read.delim2(path_blast_ncbi_taxassigned, header=F, col.names = c(\"staxid\", \"Phylo\")) %&gt;%\n  ## creating taxonomic assignment columns\n  separate(Phylo, c(\"Kingdom\", \"Phylum\", \"Class\", \"Order\", \"Family\", \"Genus\", \"Species_name\"), sep = \";\") %&gt;%\n  ## creating species column based on Species_name\n  mutate(., species = str_after_nth(Species_name, \" \", 1))\n\nBlast_NCBI &lt;- read.table(path_blast_ncbi, header=F,\n                           col.names = c(\"ASV_ID\", \"sseqid\", \"sscinames\", \"staxid\", \"pident\", \"length\", \"mismatch\",\n                                         \"gapopen\", \"qstart\", \"qend\", \"sstart\", \"send\", \"evalue\", \"bitscore\"),\n                           colClasses = c(rep(\"character\", 3), \"integer\", rep(\"numeric\", 9))) %&gt;%\n  left_join(., NCBI_taxassigned, by = \"staxid\")\n</code></pre>"},{"location":"eDNA/02-datatable_prep_12S_Riaz/#load-dada2-asv-table","title":"Load DADA2 ASV Table","text":"<p>The column headers will be the Sample IDs and the first column is the ASV ID. ASVs are given a \u201crank\u201d based on sum of reads from that ASV (pre-filtering). \u2018Random\u2019 indicates that if ASVs are tied, then the code will randomly assign a rank for those tied. Because we don\u2019t need an exact rank here, \u2018random\u2019 will do for a tie-breaker.</p> <p>No user edits.</p> <pre><code>ASV_table &lt;- read_tsv(path_asv_table, show_col_types = FALSE) %&gt;%\n  ## calculate the sum of all reads for each ASV\n  mutate(., ASV_sum = rowSums(across(where(is.numeric)))) %&gt;% \n\n  ## calculate a ranking based on those sum calculated above\n  mutate(ASV_rank = rank(-ASV_sum, ties.method='random')) %&gt;%\n\n  ## move the sum and rank columns to after ASV_ID and arrange by rank\n  relocate(c(ASV_sum,ASV_rank), .after = ASV_ID) %&gt;% arrange((ASV_rank))\n\n## creating list of rankings\nASV_rank_list &lt;- ASV_table %&gt;% dplyr::select(ASV_ID, ASV_sum, ASV_rank)\n</code></pre>"},{"location":"eDNA/02-datatable_prep_12S_Riaz/#taxonomic-assignment","title":"Taxonomic Assignment","text":"<p>Identifying where NCBI, Mito, and GMGI disagree on tax assignment. With the hierarchial approach, ASVs that match to GMGI and several other databases will only result in GMGI assignment. By reviewing this df, we can be sure we aren\u2019t missing an assignment in our GMGI curated database.</p> <p>Sub-workflow: 1. Identify any ASVs that contain multiple hits within the GMGI database. 2. Identify entries that mismatch between GMGI, Mitofish, and NCBI databases. 3. Assign taxonomy based on hierarchical approach. 4. Edit taxonomy annotations based on mismatch table. 5. Adjusting common name for those entries that don\u2019t have one (from Mito or GMGI).</p>"},{"location":"eDNA/02-datatable_prep_12S_Riaz/#identify-any-asvs-that-contain-multiple-hits-within-the-gmgi-database","title":"Identify any ASVs that contain multiple hits within the GMGI database","text":"<p>At this point, a fisheries team member needs to make choices about which taxonomic assignment to accept.</p>"},{"location":"eDNA/02-datatable_prep_12S_Riaz/#create-list-of-those-asvs-with-multiple-hits","title":"Create list of those ASVs with multiple hits","text":"<p>No user edits.</p> <pre><code>multiple_hit_choice &lt;- Blast_GMGI %&gt;% group_by(ASV_ID) %&gt;%\n  ## take top percent identity hit, count the number of top hits, and filter to those with more than 1 top hit \n  slice_max(pident, n=1) %&gt;% count() %&gt;% filter(n&gt;1) %&gt;%\n\n  ## adding BLAST_GMGI information with these ASVs and ASV rank and sum\n  left_join(., Blast_GMGI, by = \"ASV_ID\") %&gt;%\n  left_join(., ASV_rank_list, by = \"ASV_ID\") %&gt;%\n\n  ## moving database percent ID to be next to Blast percent ID\n  relocate(c(db_percent_ID, ASV_sum, ASV_rank), .after = pident) %&gt;%\n\n  ## adding choice column for next steps \n  mutate(Choice = NA)\n\n## export this data frame as excel sheet \nmultiple_hit_choice %&gt;% write_xlsx(path_choice_required)\n</code></pre> <p>Based on the output above, user needs to make some choices. In the excel spreadsheet, user needs to mark \u2018x\u2019 on the choices desired while leaving the other entries blank.</p>"},{"location":"eDNA/02-datatable_prep_12S_Riaz/#choosing-one-of-several-hits","title":"Choosing one of several hits.","text":"<p>Load choice edited dataset. No user edits.</p> <pre><code>multiple_hit_choice_edited &lt;- read_xlsx(\"example_output/Taxonomic_assignments/Choice_required_GMGI_multiplehits_edited.xlsx\") %&gt;%\n  ## selecting the choices made\n  filter(!is.na(Choice)) %&gt;%\n  ## selecting only columns needed \n  dplyr::select(ASV_ID, sseqid, Choice)\n</code></pre> <p>A for loop will filter Blast_GMGI df based on these choices. No user edits.</p> <pre><code># Create a new edited df\nBlast_GMGI_edited &lt;- Blast_GMGI \n\n# Loop through each row of the dataframe\nfor (i in multiple_hit_choice_edited$ASV_ID) {\n  # Extract the current row (will do this for each ASV_ID in the choice df)\n  current_row &lt;- multiple_hit_choice_edited %&gt;% subset(ASV_ID==i)\n\n  # Apply filter based on the current row's condition\n  Blast_GMGI_edited &lt;- Blast_GMGI_edited %&gt;%\n    filter(case_when(ASV_ID == current_row$ASV_ID ~ sseqid == current_row$sseqid,\n           TRUE ~ TRUE))\n}\n</code></pre>"},{"location":"eDNA/02-datatable_prep_12S_Riaz/#confirming-that-all-entries-have-been-dealth-with","title":"Confirming that all entries have been dealth with","text":"<p>No user edits.</p> <pre><code>### Check the below output to confirm the filtering steps above worked (if it worked, it won't be in output)\nBlast_GMGI_edited %&gt;% group_by(ASV_ID) %&gt;% slice_max(pident, n=1) %&gt;% count() %&gt;% filter(n&gt;1)\n</code></pre> <pre><code>## # A tibble: 0 \u00d7 2\n## # Groups:   ASV_ID [0]\n## # \u2139 2 variables: ASV_ID &lt;chr&gt;, n &lt;int&gt;\n</code></pre>"},{"location":"eDNA/02-datatable_prep_12S_Riaz/#identify-entries-that-mismatch-between-gmgi-mitofish-and-ncbi-databases","title":"Identify entries that mismatch between GMGI, Mitofish, and NCBI databases","text":"<p>Creating a df called \u201cDisagree\u201d. Review the output before moving onto the next section.</p> <p>No user edits.</p> <pre><code>Disagree &lt;- Blast_GMGI_edited %&gt;% group_by(ASV_ID) %&gt;% \n  dplyr::rename(., GMGI_db_ID = db_percent_ID, GMGI_pident = pident) %&gt;%\n  ## Creating new columns with species name based on pident information\n  mutate(\n    GMGI_100 = if_else(GMGI_pident == 100, Species_name, NA_character_),\n    GMGI_lessthan100 = if_else(GMGI_pident &lt; 100, Species_name, NA_character_)) %&gt;%\n\n  ## taking only the top hit per ASV ID\n  slice_max(GMGI_pident, n = 1, with_ties = FALSE) %&gt;% ungroup() %&gt;%\n\n  ## filtering to distinct rows with selected columns\n  distinct(ASV_ID, GMGI_db_ID, GMGI_pident, GMGI_100, GMGI_lessthan100) %&gt;%\n\n  ## adding Mitofish and editing the Blast_Mito df in the process\n  full_join(Blast_Mito %&gt;% dplyr::select(ASV_ID, Species_name) %&gt;%\n              dplyr::rename(Mitofish = Species_name) %&gt;%\n              distinct() %&gt;% group_by(ASV_ID) %&gt;%\n              mutate(Mitofish = paste0(Mitofish, collapse = \";\")),\n            by = \"ASV_ID\") %&gt;%\n\n  ## adding NCBI and editing the Blast_NCBI df in the process\n  full_join(Blast_NCBI %&gt;% dplyr::select(ASV_ID, Species_name) %&gt;%\n              dplyr::rename(NCBI = Species_name) %&gt;%\n              distinct() %&gt;% group_by(ASV_ID) %&gt;%\n              mutate(NCBI = paste0(NCBI, collapse = \";\")),\n            by = \"ASV_ID\") %&gt;%\n\n  ## adding ASV rank and sum information\n  left_join(., ASV_rank_list, by = \"ASV_ID\") %&gt;%\n\n  ## filtering out duplicate rows\n  distinct() %&gt;%\n\n  ## filtering to those entries that mismatch between GMGI, Mitofish, and NCBI\n  filter((GMGI_100 != GMGI_lessthan100 | GMGI_100 != Mitofish | GMGI_100 != NCBI | is.na(GMGI_100))) %&gt;%\n\n  ## adding choice column for next steps \n  mutate(Choice = NA)\n</code></pre> <pre><code>## Warning in full_join(., Blast_NCBI %&gt;% dplyr::select(ASV_ID, Species_name) %&gt;% : Detected an unexpected many-to-many relationship between `x` and `y`.\n## \u2139 Row 3 of `x` matches multiple rows in `y`.\n## \u2139 Row 10 of `y` matches multiple rows in `x`.\n## \u2139 If a many-to-many relationship is expected, set `relationship =\n##   \"many-to-many\"` to silence this warning.\n</code></pre> <pre><code>## export this data frame as excel sheet \nDisagree %&gt;% write_xlsx(path_disagree_list)\n</code></pre>"},{"location":"eDNA/02-datatable_prep_12S_Riaz/#assign-taxonomy-based-on-hierarchical-approach","title":"Assign taxonomy based on hierarchical approach","text":"<p>Taxonomic identification is taken from GMGI 100%, then GMGI \\&lt;100%, then Mitofish 100%, and finally NCBI 100%.</p> <p>No user edits.</p> <pre><code>ASV_table_taxID &lt;- ASV_table %&gt;% \n\n  ## 1. Top hit from GMGI's database\n  left_join(Blast_GMGI_edited %&gt;%  group_by(ASV_ID) %&gt;%\n              slice_max(pident, n = 1) %&gt;%\n                            dplyr::select(ASV_ID, Species_name),\n            by = join_by(ASV_ID)) %&gt;%\n\n  ## 2. Mitofish database\n  ### join df, select ASV_ID and Species_name columns, rename Species_name to Mito, call only distinct rows\n  left_join(., Blast_Mito %&gt;% dplyr::select(ASV_ID, Species_name) %&gt;% dplyr::rename(Mito = Species_name) %&gt;% distinct() %&gt;%\n\n              ### group by ASV_ID, and collapse all species names separated by ;, then take only distinct rows\n              group_by(ASV_ID) %&gt;% mutate(Mito = paste0(Mito, collapse = \";\")) %&gt;% distinct(), by = \"ASV_ID\") %&gt;%\n\n  ### if GMGI annotation is NA, then replace with Mitofish \n  mutate(., Species_name = ifelse(is.na(Species_name), Mito, Species_name)) %&gt;%\n\n  ## 3. NCBI database; same functions as above\n  left_join(., Blast_NCBI %&gt;% dplyr::select(ASV_ID, Species_name) %&gt;% dplyr::rename(NCBI = Species_name) %&gt;% distinct() %&gt;%\n              group_by(ASV_ID) %&gt;% mutate(NCBI = paste0(NCBI, collapse = \";\")) %&gt;% distinct(), by = \"ASV_ID\") %&gt;%\n  mutate(., Species_name = ifelse(is.na(Species_name), NCBI, Species_name)) %&gt;%\n\n  ## 4. if Species name is STILL not filled, call it \"unassigned\"\n  mutate(., Species_name = ifelse(is.na(Species_name), \"unassigned\", Species_name)) %&gt;%  \n\n  ## removing Mito spp and NCBI spp\n  dplyr::select(-Mito, -NCBI) %&gt;%\n\n  ## move species name to be after ASV_ID\n  relocate(., c(Species_name), .after = ASV_ID)\n</code></pre>"},{"location":"eDNA/02-datatable_prep_12S_Riaz/#edit-taxonomy-annotations-based-on-mismatch-table","title":"Edit taxonomy annotations based on mismatch table","text":"<p>Override any annotations with edited taxonomic identification table. No user edits.</p> <pre><code>## read in edited df \ntaxonomic_choice &lt;- read_xlsx(\"example_output/Taxonomic_assignments/SampleReport_taxonomic_ID_edited.xlsx\") %&gt;%\n    ## selecting only columns needed \n  dplyr::select(ASV_ID, Choice)  \n\n# Create a new edited df\nASV_table_taxID_edited &lt;- ASV_table_taxID \n\n# Loop through each row of the dataframe\nfor (i in taxonomic_choice$ASV_ID) {\n  # Extract the current row (will do this for each ASV_ID in the choice df)\n  current_row &lt;- taxonomic_choice %&gt;% subset(ASV_ID==i)\n\n  # Apply filter based on the current row's condition\n  ASV_table_taxID_edited &lt;- ASV_table_taxID_edited %&gt;%\n    mutate(Species_name = case_when(\n          ASV_ID == current_row$ASV_ID ~ current_row$Choice,\n           TRUE ~ Species_name))\n}\n</code></pre>"},{"location":"eDNA/02-datatable_prep_12S_Riaz/#confirm-all-entries-are-dealt-with","title":"Confirm all entries are dealt with","text":"<p>No user edits.</p> <pre><code>## Output will be blank\nASV_table_taxID_edited %&gt;% dplyr::select(Species_name) %&gt;% distinct() %&gt;% \n  filter(., grepl(\";\", Species_name)) %&gt;% arrange(Species_name) \n</code></pre> <pre><code>## # A tibble: 0 \u00d7 1\n## # \u2139 1 variable: Species_name &lt;chr&gt;\n</code></pre>"},{"location":"eDNA/02-datatable_prep_12S_Riaz/#adjusting-common-name-for-those-entries-that-dont-have-one-from-mito-or-ncbi","title":"Adjusting common name for those entries that don\u2019t have one (from Mito or NCBI)","text":"<p>No user edits.</p> <pre><code>### add common name column to df\nASV_table_taxID_edited &lt;- ASV_table_taxID_edited %&gt;%\n  left_join(., gmgi_db %&gt;% dplyr::select(Species_name, Common_name, Category) %&gt;% distinct(), by = \"Species_name\") %&gt;%\n  relocate(., c(Common_name, Category), .after = Species_name)\n\n### print entries with no common name\nASV_table_taxID_edited %&gt;% dplyr::select(Species_name, Common_name) %&gt;% filter(is.na(Common_name)) %&gt;% distinct()\n</code></pre> <pre><code>## # A tibble: 2 \u00d7 2\n##   Species_name    Common_name\n##   &lt;chr&gt;           &lt;chr&gt;      \n## 1 Cololabis saira &lt;NA&gt;       \n## 2 unassigned      &lt;NA&gt;\n</code></pre> <p>Editing common names and category when needed.</p> <pre><code>### User edits:\n### 1. Add mutate cases using the format ifelse(grepl('', Species_name), \"\", Common_name\n### example: ifelse(grepl('unassigned', Species_name), \"unassigned\", Common_name)\n### 2. Add mutate cases using the format ifelse(grepl('', Species_name), \"\", Category\n\nASV_table_taxID_edited &lt;- ASV_table_taxID_edited %&gt;% \n  # changing specific entries for Common name\n  mutate(Common_name = ifelse(grepl('unassigned', Species_name), \"unassigned\", Common_name),\n\n         ## example from example dataset - change for your own data\n         Common_name = ifelse(grepl('Cololabis saira', Species_name), \"Pacific Saury\", Common_name)\n         ) %&gt;%\n\n  # changing specific entries for category\n  mutate(Category = ifelse(grepl('unassigned', Species_name), \"unassigned\", Category),\n\n         ## example from example dataset - change for your own data\n         Category = ifelse(grepl('Cololabis saira', Species_name), \"Teleost Fish\", Category)\n         )\n\n## printing list of species name without common names \n## after additions to mutate function above, this output should be zero \nASV_table_taxID_edited %&gt;% dplyr::select(Species_name, Common_name) %&gt;% filter(is.na(Common_name)) %&gt;% distinct()\n</code></pre> <pre><code>## # A tibble: 0 \u00d7 2\n## # \u2139 2 variables: Species_name &lt;chr&gt;, Common_name &lt;chr&gt;\n</code></pre>"},{"location":"eDNA/02-datatable_prep_12S_Riaz/#filtering-filter-asv-by-less-than-01-reads-and-then-collapse-by-group","title":"Filtering: Filter ASV by less than 0.1% reads and then collapse by group","text":""},{"location":"eDNA/02-datatable_prep_12S_Riaz/#filter-out-reads-that-are-less-than-01-of-asv-row-total-per-sample","title":"Filter out reads that are less than 0.1% of ASV (row) total per sample.","text":"<p>Create an output of what you\u2019re losing with filtering.</p> <p>No user edits.</p> <pre><code>ASV_table_taxID_filtered &lt;- ASV_table_taxID_edited %&gt;%\n  ## telling the df we are doing the following function by rows (ASVs)\n  rowwise() %&gt;%\n\n  ## filtering out any values that are less than 0.001 of the total ASV read # in each sample\n  mutate(across(.cols = (7:ncol(.)),            \n                .fns = ~ ifelse((.x/ASV_sum)&lt;0.001, NA, .x))) %&gt;% ungroup()\n\n## output of what we're losing\nASV_table_taxID_edited %&gt;% rowwise() %&gt;%\n  mutate(across(.cols = (7:ncol(.)),            \n                .fns = ~ ifelse((.x/ASV_sum)&gt;0.001, NA, .x))) %&gt;% ungroup() %&gt;% write_xlsx(\"example_output/ASV_reads_filtered_out.xlsx\")\n\n\n## Export ASV break-down for 03-data_quality.Rmd\nASV_table_taxID_filtered %&gt;% dplyr::select(ASV_ID, Species_name, Common_name, Category, ASV_sum, ASV_rank) %&gt;%\n  write_xlsx(\"example_output/ASV_breakdown.xlsx\")\n</code></pre>"},{"location":"eDNA/02-datatable_prep_12S_Riaz/#collapsing-read-counts-by-species-name","title":"Collapsing read counts by species name","text":"<p>No user edits.</p> <pre><code>ASV_table_taxID_collapsed &lt;- ASV_table_taxID_filtered %&gt;% \n  # removing original ASV_ID to collapse\n  dplyr::select(-ASV_ID) %&gt;%  \n\n  ## group by Species_name and sample\n  dplyr::group_by(Species_name, Common_name, Category) %&gt;%\n\n  ## sum down column by species name and sample to collapse\n  summarise(across(6:last_col(), ~ sum(., na.rm = TRUE))) %&gt;% ungroup()\n</code></pre> <pre><code>## `summarise()` has grouped output by 'Species_name', 'Common_name'. You can\n## override using the `.groups` argument.\n</code></pre>"},{"location":"eDNA/02-datatable_prep_12S_Riaz/#creating-results-output","title":"Creating results output","text":"<p>Raw reads results output. No user edits.</p> <pre><code>## Raw reads matrix (wide format)\nASV_table_taxID_collapsed %&gt;% write_xlsx(results_rawreads_matrix)\n\n## Raw reads long format and filtering out entries with zero reads\nASV_table_taxID_collapsed %&gt;% \n  gather(\"sampleID\", \"reads\", c(4:last_col())) %&gt;%\n  filter(reads &gt; 0) %&gt;% \n  left_join(., meta, by = \"sampleID\") %&gt;%\n  write_xlsx(results_rawreads_long)\n</code></pre> <p>Relative Abundance</p> <pre><code>## Creating matrix from edited collapsed df\ndata.matrix &lt;- as.matrix(ASV_table_taxID_collapsed %&gt;% \n                           dplyr::select(-Common_name, -Category) %&gt;% column_to_rownames(var = \"Species_name\"))\n\n## Calculating relative abundance\ndata_relativeab &lt;- as.data.frame(make_relative(data.matrix)) %&gt;%\n  ## moving rownames to a column\n  rownames_to_column(var = \"Species_name\") %&gt;%\n\n  ## adding common name and category back in\n  right_join(ASV_table_taxID_collapsed %&gt;% dplyr::select(Species_name, Common_name, Category), .)  \n</code></pre> <pre><code>## Joining with `by = join_by(Species_name)`\n</code></pre> <pre><code>## Exporting matrix\ndata_relativeab %&gt;% write_xlsx(results_relab_matrix)\n\n## Relative abundance long format with metadata \ndata_relativeab %&gt;%\n  gather(\"sampleID\", \"relab\", c(4:last_col())) %&gt;%\n  left_join(., meta, by = \"sampleID\") %&gt;%\n  write_xlsx(results_relab_long)\n</code></pre>"},{"location":"eDNA/03-data_quality/","title":"Metabarcoding data quality: eDNA metabarcoding base script","text":"<p>.Rmd script</p> <p>This script evaluates your sequence quality and taxonomic assignment quality. Figures produced in this script can go into supplemental data for a manuscript.</p>"},{"location":"eDNA/03-data_quality/#load-libraries","title":"Load libraries","text":"<pre><code>library(dplyr) # for data transformation\n</code></pre> <pre><code>## \n## Attaching package: 'dplyr'\n\n## The following objects are masked from 'package:stats':\n## \n##     filter, lag\n\n## The following objects are masked from 'package:base':\n## \n##     intersect, setdiff, setequal, union\n</code></pre> <pre><code>library(tidyverse) # for data transformation\n</code></pre> <pre><code>## \u2500\u2500 Attaching core tidyverse packages \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 tidyverse 2.0.0 \u2500\u2500\n## \u2714 forcats   1.0.0     \u2714 readr     2.1.5\n## \u2714 ggplot2   3.5.1     \u2714 stringr   1.5.1\n## \u2714 lubridate 1.9.3     \u2714 tibble    3.2.1\n## \u2714 purrr     1.0.2     \u2714 tidyr     1.3.1\n\n## \u2500\u2500 Conflicts \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 tidyverse_conflicts() \u2500\u2500\n## \u2716 dplyr::filter() masks stats::filter()\n## \u2716 dplyr::lag()    masks stats::lag()\n## \u2139 Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n</code></pre> <pre><code>library(ggplot2) # for plotting\nlibrary(readxl) ## for reading in excel files\nlibrary(viridis)\n</code></pre> <pre><code>## Loading required package: viridisLite\n</code></pre> <pre><code>library(hrbrthemes)\nlibrary(ggrepel)\nlibrary(cowplot)\n</code></pre> <pre><code>## \n## Attaching package: 'cowplot'\n## \n## The following object is masked from 'package:lubridate':\n## \n##     stamp\n</code></pre> <pre><code># removing scientific notation\n## remove line or comment out if not desired \noptions(scipen=999)\n</code></pre>"},{"location":"eDNA/03-data_quality/#load-data","title":"Load data","text":"<pre><code>### User edits:\n### 1. Replace the 3 paths below: edit example_input to your project specific path \n### 2. Confirm your sampleIDs match between metadata, results df, and filtering stats output\n\nfiltering_stats &lt;- read_tsv(\"example_input/overall_summary.tsv\", show_col_types = FALSE) %&gt;% dplyr::rename(sampleID = sample)\n\nmeta &lt;- read.csv(\"example_input/metadata.csv\", header=TRUE)\n\nresults &lt;- read_xlsx(\"example_output/Results_rawreads_long.xlsx\") %&gt;%\n  mutate(Category = factor(Category, levels = c(\"Human\", \"Livestock\", \"Other\", \"unassigned\", \"Bird\",\n                                                \"Elasmobranch\", \"Marine Mammal\", \"Sea Turtle\", \"Teleost Fish\")))\n\nASV_breakdown &lt;- read_xlsx(\"example_output/ASV_breakdown.xlsx\") %&gt;%\n  mutate(Category = factor(Category, levels = c(\"Human\", \"Livestock\", \"Other\", \"unassigned\", \"Bird\",\n                                                \"Elasmobranch\", \"Marine Mammal\", \"Sea Turtle\", \"Teleost Fish\")))\n</code></pre>"},{"location":"eDNA/03-data_quality/#sequence-data","title":"Sequence data","text":""},{"location":"eDNA/03-data_quality/#data-transformation","title":"Data Transformation","text":"<p>No user edits.</p> <pre><code>df &lt;- full_join(filtering_stats, meta, by = \"sampleID\") %&gt;%\n  # filtering out columns we don't need \n  dplyr::select(-cutadapt_reverse_complemented) %&gt;%\n\n  # removing percentage icon from cutadapt_passing_filters_percent\n  mutate(cutadapt_passing_filters_percent = gsub(\"%\", \"\", cutadapt_passing_filters_percent)) %&gt;%\n\n  # confirming that all columns of interest are numerical \n  mutate_at(vars(2:10), as.numeric) %&gt;%\n\n  # data transformation so all columns of interest are together \n  gather(\"measure\", \"value\", 2:10)  \n</code></pre>"},{"location":"eDNA/03-data_quality/#plotting","title":"Plotting","text":"<p>Suggested webpage to choose colors: https://coolors.co/</p> <pre><code>### User edits:\n### 1. Change paths of output to desired folder (data/figures is suggested data structure)\n### 2. Change x axis and color, size based on metadata desired \n### 3. Change custom colors and sizes if desired and number of colors and sizes based on metadata variable chosen \n\ndf %&gt;% \n  ## USER EDITS IN LINE BELOW \n  ggplot(., aes(x=Project, y=value, color=Site, shape=SampleType)) + \n\n  ## adding points in jitter format \n  geom_jitter(width=0.15, alpha=0.5) + \n\n  ## option for additional boxplots if desired (uncomment to add)\n  #geom_boxplot() +\n\n  ## using facet_wrap to create grid based on variables and factor() to order them in custom format\n  facet_wrap(~factor(measure, levels=c('cutadapt_total_processed', 'cutadapt_passing_filters', \n                                       'cutadapt_passing_filters_percent', 'DADA2_input',\n                                 'filtered', 'denoisedF', 'denoisedR', 'merged', 'nonchim')), scales = \"free\") +\n\n  ## graph asthetics \n  theme_bw() +\n  ylab(\"Number of reads\") + \n\n  ## USER EDITS IN MANUAL CODE BELOW \n  scale_color_manual(values = c(\"red3\", \"lightblue\", \"purple2\", \"gold\", \"green4\", \"black\")) +\n  scale_size_manual(values = c(21,17)) +\n\n\n  theme(panel.background=element_rect(fill='white', colour='black'),\n        strip.background=element_rect(fill='white', colour='black'),\n        strip.text = element_text(size = 10, face=\"bold\"),\n        legend.position = \"right\",\n        axis.text.y = element_text(size=7, color=\"grey30\"),\n        axis.text.x = element_text(angle = 45, hjust = 1),\n        axis.title.y = element_text(margin = margin(t = 0, r = 10, b = 0, l = 0), size=11, face=\"bold\"),\n        axis.title.x = element_text(margin = margin(t = 10, r = 0, b = 0, l = 0), size=11, face=\"bold\"))\n</code></pre> <p></p> <pre><code>ggsave(\"example_output/Figures/SampleReport_FilteringStats.png\", width = 10, height=8)\n</code></pre>"},{"location":"eDNA/03-data_quality/#plot-unassigned-taxonomy","title":"Plot unassigned taxonomy","text":""},{"location":"eDNA/03-data_quality/#data-transformation_1","title":"Data transformation","text":"<p>No user edits.</p> <pre><code>results_summary &lt;- results %&gt;% \n  group_by(Category, Project) %&gt;%\n  summarise(sum_reads = sum(reads))\n</code></pre> <pre><code>## `summarise()` has grouped output by 'Category'. You can override using the\n## `.groups` argument.\n</code></pre> <pre><code>general_stats &lt;- results %&gt;% \n  group_by(Category) %&gt;%\n  summarise(sum_reads = sum(reads)) %&gt;% ungroup() %&gt;%\n  mutate(total = sum(sum_reads),\n         percent = sum_reads/total*100) %&gt;% dplyr::select(Category, percent) %&gt;% distinct() %&gt;%\n  ## round to 2 decimal places \n  mutate(across(c('percent'), round, 4))\n</code></pre> <pre><code>## Warning: There was 1 warning in `mutate()`.\n## \u2139 In argument: `across(c(\"percent\"), round, 4)`.\n## Caused by warning:\n## ! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\n## Supply arguments directly to `.fns` through an anonymous function instead.\n## \n##   # Previously\n##   across(a:b, mean, na.rm = TRUE)\n## \n##   # Now\n##   across(a:b, \\(x) mean(x, na.rm = TRUE))\n</code></pre> <pre><code>ASV_summary &lt;- ASV_breakdown %&gt;%\n  group_by(Category) %&gt;%\n  summarise(count = n_distinct(ASV_ID))\n\nspecies_summary &lt;- results %&gt;%\n  group_by(Category) %&gt;%\n  summarise(count = n_distinct(Species_name))\n\n## species in addition to category\n## metadata option add-in\n</code></pre>"},{"location":"eDNA/03-data_quality/#raw-reads-plotting","title":"Raw Reads Plotting","text":"<p>With metadata</p> <pre><code>### User edits:\n### 1. Change paths of output to desired folder (data/figures is suggested data structure)\n### 2. Change x axis and color, size based on metadata desired \n### 3. Change custom colors and sizes if desired and number of colors and sizes based on metadata variable chosen \n### 4. Comment out any categories that do not show up in your dataset\n\n# Check how many categories \nunique(results_summary$Category)\n</code></pre> <pre><code>## [1] Human        Livestock    unassigned   Teleost Fish\n## 9 Levels: Human Livestock Other unassigned Bird Elasmobranch ... Teleost Fish\n</code></pre> <pre><code>## Based on this output, comment/uncomment the categories present for color \n\nggplot(results_summary, aes(fill=Category, y=sum_reads, x=Project)) + \n    geom_bar(position=\"stack\", stat=\"identity\") +\n    scale_fill_brewer(palette = \"RdYlBu\") +\n    # scale_fill_manual(values = c(\"#9f040e\", # Human\n    #                              \"#e30613\", # Livestock\n    #                              \"#fb747d\", # Other\n    #                              \"#ff0000\", # unassigned\n    #                              \"#03045e\", # Bird\n    #                              \"#023e8a\", # Elasmobranch\n    #                              \"#0077b6\", # Marine mammal\n    #                              \"#0096c7\", # Sea Turtle\n    #                              \"#48cae4\" # Teleost Fish \n    #                              )) +\n    labs(fill = \"Category\") +\n    theme_bw() + \n    xlab(\" Month\") + ylab(\"Raw reads\") +\n    theme(panel.background=element_rect(fill='white', colour='black'),\n        axis.text.y = element_text(size=7, color=\"grey30\"),\n        axis.text.x = element_text(angle = 45, hjust = 1),\n        axis.title.y = element_text(margin = margin(t = 0, r = 10, b = 0, l = 0), size=11, face=\"bold\"),\n        axis.title.x = element_text(margin = margin(t = 10, r = 0, b = 0, l = 0), size=11, face=\"bold\"))\n</code></pre> <p></p> <pre><code>## add % stacked as well\n\nggsave(\"example_output/Figures/Rawreads_percategory.png\", width = 8, height = 5)\n</code></pre> <p>Reads Piechart</p> <pre><code>### User edits:\n### 1. Change paths of output to desired folder (data/figures is suggested data structure)\n### 2. Change scale brewer color if desired \n\npiechart &lt;- general_stats %&gt;%  \n  mutate(csum = rev(cumsum(rev(percent))), \n         pos = percent/2 + lead(csum, 1),\n         pos = if_else(is.na(pos), percent/2, pos))\n\npiechart_reads &lt;- general_stats %&gt;% \n  ggplot(., aes(x=\"\", y = percent, fill = Category)) +\n  geom_col(color = \"black\") +\n  geom_label_repel(data = piechart,\n                   aes(y = pos, label = paste0(percent, \"%\")),\n                   size = 3, nudge_x = 1, show.legend = FALSE) +\n  coord_polar(theta = \"y\") +\n  scale_fill_brewer(palette = \"RdYlBu\") +\n  theme_bw() +\n  theme(\n    plot.background = element_rect(fill = \"white\", colour = NA),\n    panel.border = element_blank(),  # Remove panel border\n    panel.grid = element_blank(),    # Remove grid lines\n    axis.ticks = element_blank(),    # Remove axis ticks\n    axis.text = element_blank(),     # Remove axis text\n    axis.title = element_blank(),     # Remove axis titles\n    legend.position = \"none\"\n  ) +\n  ggtitle(\"% of raw reads\") +\n  xlab(\"\") + ylab(\"\") + labs(fill = \"Category\")\n</code></pre> <p>ASV Piechart</p> <pre><code>### User edits:\n### 1. Change paths of output to desired folder (data/figures is suggested data structure)\n### 2. Change scale brewer color if desired \n\npiechart_ASV &lt;- ASV_summary %&gt;%  \n  mutate(csum = rev(cumsum(rev(count))), \n         pos = count/2 + lead(csum, 1),\n         pos = if_else(is.na(pos), count/2, pos))\n\npiechart_ASV &lt;- ASV_summary %&gt;% \n  ggplot(., aes(x=\"\", y = count, fill = Category)) +\n  geom_col(color = \"black\") +\n  geom_label_repel(data = piechart_ASV,\n                   aes(y = pos, label = paste0(count)),\n                   size = 3, nudge_x = 1, show.legend = FALSE) +\n  coord_polar(theta = \"y\") +\n  scale_fill_brewer(palette = \"RdYlBu\") +\n  theme_bw() +\n  theme(\n    plot.background = element_rect(fill = \"white\", colour = NA),\n    panel.border = element_blank(),  # Remove panel border\n    panel.grid = element_blank(),    # Remove grid lines\n    axis.ticks = element_blank(),    # Remove axis ticks\n    axis.text = element_blank(),     # Remove axis text\n    axis.title = element_blank(),     # Remove axis titles\n    legend.position = \"none\"\n  ) +\n  ggtitle(\"# of ASVs\") +\n  xlab(\"\") + ylab(\"\") + labs(fill = \"Category\")\n</code></pre> <p>Number of species pie chart</p> <pre><code>piechart_spp &lt;- species_summary %&gt;%  \n  mutate(csum = rev(cumsum(rev(count))), \n         pos = count/2 + lead(csum, 1),\n         pos = if_else(is.na(pos), count/2, pos))\n\npiechart_spp &lt;- species_summary %&gt;% \n  ggplot(., aes(x=\"\", y = count, fill = Category)) +\n  geom_col(color = \"black\") +\n  geom_label_repel(data = piechart_spp,\n                   aes(y = pos, label = paste0(count)),\n                   size = 3, nudge_x = 1, show.legend = FALSE) +\n  coord_polar(theta = \"y\") +\n  scale_fill_brewer(palette = \"RdYlBu\") +\n  theme_bw() +\n  theme(\n    plot.background = element_rect(fill = \"white\", colour = NA),\n    panel.border = element_blank(),  # Remove panel border\n    panel.grid = element_blank(),    # Remove grid lines\n    axis.ticks = element_blank(),    # Remove axis ticks\n    axis.text = element_blank(),     # Remove axis text\n    axis.title = element_blank()     # Remove axis titles\n  ) +\n  ggtitle(\"# of species\") +\n  xlab(\"\") + ylab(\"\") + labs(fill = \"Category\")\n</code></pre> <p>Plot together and export</p> <pre><code>plot_grid(piechart_reads, piechart_ASV, piechart_spp, \n          ncol=3, \n          rel_widths = c(2,2,3.075) \n          #align = \"hv\"\n          )\n</code></pre> <p></p> <pre><code>ggsave(\"example_output/Category_breakdown.png\", width=10, height=4)\n</code></pre>"},{"location":"eDNA/04-relative_abundance_heatmap/","title":"Relative Abundance Heatmaps: eDNA metabarcoding base script","text":"<p>.Rmd script</p> <p>This script plots your relative abundance matrix as a heatmap. Figures produced are potentially part of the main figures of your manuscript/report.</p>"},{"location":"eDNA/04-relative_abundance_heatmap/#load-libraries","title":"Load libraries","text":"<pre><code>library(ggplot2) ## for plotting\nlibrary(dplyr) ## for data table manipulation\n</code></pre> <pre><code>## \n## Attaching package: 'dplyr'\n\n## The following objects are masked from 'package:stats':\n## \n##     filter, lag\n\n## The following objects are masked from 'package:base':\n## \n##     intersect, setdiff, setequal, union\n</code></pre> <pre><code>library(tidyr) ## for data table manipulation\nlibrary(readxl) ## for reading in excel files\nlibrary(stringr) ## for data transformation\nlibrary(strex) ## for data transformation\nlibrary(purrr) ## for data transformation\nlibrary(funrar) ## for make_relative()\nlibrary(tidyverse) ## for data transformation\n</code></pre> <pre><code>## \u2500\u2500 Attaching core tidyverse packages \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 tidyverse 2.0.0 \u2500\u2500\n## \u2714 forcats   1.0.0     \u2714 readr     2.1.5\n## \u2714 lubridate 1.9.3     \u2714 tibble    3.2.1\n\n## \u2500\u2500 Conflicts \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 tidyverse_conflicts() \u2500\u2500\n## \u2716 dplyr::filter() masks stats::filter()\n## \u2716 dplyr::lag()    masks stats::lag()\n## \u2139 Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n</code></pre> <pre><code>library(naniar) ## replace_with_na_all function\nlibrary(ggh4x) ## for facet wrap options\n</code></pre> <pre><code>## \n## Attaching package: 'ggh4x'\n## \n## The following object is masked from 'package:ggplot2':\n## \n##     guide_axis_logticks\n</code></pre> <pre><code>library(tidytext)\n</code></pre>"},{"location":"eDNA/04-relative_abundance_heatmap/#load-data","title":"Load data","text":"<pre><code>df &lt;- read_xlsx(\"example_output/Results_relab_long.xlsx\") %&gt;%\n  mutate(across(c(relab), ~ round(.x, 5)))\n</code></pre>"},{"location":"eDNA/04-relative_abundance_heatmap/#remove-categories","title":"Remove Categories","text":"<p>If you want to plot relative abundance without human, other, or livestock categories. As FYI/warning, relative abundance is calculated with these categories included. Relative abundance can also be thought of as proportion of total reads, which is calculated from the total reads for that sample.</p> <pre><code>df_filtered &lt;- df %&gt;% \n  filter(!Category == \"Other\" &amp; !Category == \"Livestock\" &amp; !Category == \"unassigned\" &amp; !Category == \"Human\") \n</code></pre>"},{"location":"eDNA/04-relative_abundance_heatmap/#plot","title":"Plot","text":"<p>reverse label order: scale y discrete limits reverse limits=rev</p> <p>https://coolors.co/ (hit tools on the top right hand side)</p> <pre><code>## if subset of categories is desired, replace df below with df_filtered\ndf %&gt;%\n\n  ## replace zeros with NAs for plotting\n  replace_with_na_all(condition = ~.x == 0.00000) %&gt;%\n\n  ## ggplot basic options (USER EDIT: X AND Y AXIS)\n  ggplot(., aes(x=Site, y=Common_name)) +\n  geom_tile(aes(fill = relab), color = \"black\") +\n\n  ## x, y, and legend labels (USER EDITS IF DESIRED)\n  ylab(\"Common name\") +\n  xlab(\"Site\") +\n  labs(fill = \"Relative Abundance (%)\") +\n\n  ## color of the tile options; direction=1 will flip the low/high (USER EDITS IF DESIRED)\n  scale_fill_gradient(na.value = \"white\", low = \"lightskyblue2\", high = \"#0C4D66\") + \n\n  ## facet grid with Category and project variables\n  facet_grid2(Category ~ SampleType, \n              scales = \"free\", space = \"free\", \n              labeller = labeller(Category = label_wrap_gen(width = 10))) +\n\n  ## graph theme options\n  theme_classic() +\n  theme(\n    ## axis text \n    axis.text.x = element_text(angle = 90, size=6, color=\"grey25\", hjust = 1),\n    axis.text.y = element_text(colour = 'black', size = 8),\n\n    ## legend text and title \n    legend.text = element_text(size = 8, color=\"black\"),\n    legend.title = element_text(margin = margin(t = 0, r = 0, b = 5, l = 0), size=10, color=\"black\", face=\"bold\"),\n    legend.position = c(-0.4, -0.05), \n    legend.key.height = unit(5, 'mm'),\n    legend.direction = \"horizontal\",\n    legend.key.width = unit(5, 'mm'),\n    legend.title.align = 0.5,\n    legend.title.position = \"top\",\n\n    ## axis titles \n    axis.title.y = element_text(margin = margin(t = 0, r = 10, b = 0, l = 0), size=14, face=\"bold\"),\n    axis.title.x = element_text(margin = margin(t = 10, r = 0, b = 0, l = 0), size=14, face=\"bold\"),\n\n    ## facet wrap labels\n    strip.text.x = element_text(color = \"black\", face = \"bold\", size = 12),\n    strip.text.y = element_text(color = \"black\", face = \"bold\", size = 12, angle=0),\n    strip.background.y = element_blank(),\n    strip.clip = \"off\"\n    )\n</code></pre> <pre><code>## Warning: The `legend.title.align` argument of `theme()` is deprecated as of ggplot2\n## 3.5.0.\n## \u2139 Please use theme(legend.title = element_text(hjust)) instead.\n## This warning is displayed once every 8 hours.\n## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was\n## generated.\n\n## Warning: A numeric `legend.position` argument in `theme()` was deprecated in ggplot2\n## 3.5.0.\n## \u2139 Please use the `legend.position.inside` argument of `theme()` instead.\n## This warning is displayed once every 8 hours.\n## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was\n## generated.\n</code></pre> <p></p> <pre><code>## USER EDITS WIDTH AND HEIGHT TO DESIRED   \nggsave(\"example_output/Figures/Relative_abundance.png\", width = 7, height = 10)  \n\n\n## Group by family or order from gmgi_db info? this would break down teleost fish more\n## ecological niche? functional groups? would probably require more metadata..\n</code></pre>"}]}