{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the GMGI Fisheries Lab Handbook!","text":"<p>This is the bioinformatic handbook for the Fisheries team at Gloucester Marine Genomics Institute.</p> <p></p> <p>Team:    </p> <ul> <li>Research Scientist, Tim O'Donnell </li> <li>Postdoctoral Scientist, Emma Strand; github </li> <li>Research Associate, Dylan Comb </li> <li>Research Associate, Nicole Cubba </li> </ul>"},{"location":"#recent-news","title":"Recent News","text":"<p>2024:    </p> <ul> <li>Tim and the offshore wind eDNA project were featured in the Gloucester Times </li> <li>Tim joined OceanX - GMGI collaboration in Indonesia </li> <li>Our epigenetic aging project was featured in NOAA \u2018Omics Q3 report </li> <li>Emma was featured in Seattle Aquarium's alumni stories </li> <li>Emma was featured in the Journal of Experimental Biology's Early Career Spotlight </li> </ul>"},{"location":"#recent-presentations","title":"Recent Presentations","text":"<p>Upcoming in 2025:  </p> <ul> <li>Emma will present our epigenetic aging work at the Plant and Animal Genome Conference in San Diego, CA  </li> </ul> <p>2024:  </p> <ul> <li>Emma along with Research Scientist Shelly Wanamaker served on a panel for Coastal Acidification in Massachusetts: A Community Conversation in Gloucester, MA  </li> <li>Tim attended the Mid Atlantic/Northeastern Division American Fisheries Society Meeting in New Brunswick, NJ and presented a talk  </li> <li>Tim presented the team's eDNA work at the University of Connecticut's coastal campus Avery Point    </li> <li>Tim was the featured GMGI speaker at the 2024 GMGI Science Forum     </li> <li>Emma gave the biology department seminar at Woods Hole Oceanographic Institute (WHOI)   </li> <li>Tim and Emma attended the 3rd National Workshop on Marine eDNA hosted by Johns Hopkins Applied Physics Laboratory in Laurel, MD and the Smithsonian National Museum of Natural History in Washington DC  </li> <li>Emma participated in NOAA\u2019s Women in Science Leadership Workshop in Princeton, NJ  </li> <li>Emma presented our epigenetic aging work at the Society for Integrative and Comparative Biology Conference in Seattle, WA </li> </ul>"},{"location":"#publications","title":"Publications","text":"<p>O\u2019Donnell, T.P., &amp; T.J. Sullivan. 2021. Low-coverage whole-genome sequencing reveals molecular markers for spawning season and sex identification in Gulf of Maine Atlantic cod (Gadus morhua, Linnaeus 1758). Ecology and Evolution 11: 10659-10671. doi:10.1002/ece3.7878</p>"},{"location":"#gloucester-marine-genomics-institute","title":"Gloucester Marine Genomics Institute","text":"<p>https://gmgi.org/ </p>"},{"location":"00-creating_eDNA_conda_env/","title":"Creating a conda environment for eDNA projects","text":""},{"location":"00-creating_eDNA_conda_env/#edna-general","title":"eDNA general","text":"<p>Background information on Conda. </p> <p>GMGI Fisheries has a conda environment set-up with all the packages needed for this workflow. Code below was used to create this conda environment.</p> <p>DO NOT REPEAT every time user is running this workflow.</p> <pre><code># Activate conda\nsource /work/gmgi/miniconda3/bin/activate\n\n# Creating conda \nconda create --name fisheries_eDNA\n\n# Installing packages needed for this workflow \nconda install -c bioconda fastqc \nconda install multiqc \nconda install bioconda::nextflow \nconda install conda-forge::singularity\nconda install bioconda::blast\nconda install nextflow\nconda install blast\nconda install singularity\nconda install -c bioconda vsearch -y\npip install nsdpy\nconda install wget\nconda install -c bioconda coidb\nconda install bioconda::mothur\n</code></pre> <p>The conda environment is started within each slurm script, but to activate conda environment outside of the slurm script to update packages or check what is installed:</p> <pre><code># Activate conda\nsource /work/gmgi/miniconda3/bin/activate\n\n# Activate fisheries eDNA conda environment \nconda activate fisheries_eDNA\n\n# List all available environments \nconda env list \n\n# List all packages installed in fisheries_eDNA\nconda list\n\n# Update a package\nconda update [package name]\n\n# Update nextflow ampliseq workflow \nnextflow pull nf-core/ampliseq\n</code></pre>"},{"location":"00-creating_eDNA_conda_env/#edna-coi","title":"eDNA COI","text":"<p>I had issues when installing mothur on the eDNA general conda environment. I got the following error: <code>mothur: error while loading shared libraries: libgsl.so.25: cannot open shared object file: No such file or directory</code>. I then installed gsl with <code>conda install -c conda-forge gsl</code>. This was already installed and is version 2.7 but I think I need 2.5.. Tried <code>conda install -c conda-forge gsl=2.5</code>. This downgraded mothur from 1.48.0-h9f4bb92_2 --&gt; 1.36.1-0. gsl 2.7 will be superseded by higher priority channgel as 2.5. This resulted in a similar error so I updated gsl. Mothur wouldn't update.</p> <p>Tried to start a different conda environment for mothur </p> <p>DO NOT REPEAT every time user is running this workflow.</p> <pre><code># Activate conda\nsource /work/gmgi/miniconda3/bin/activate\n\n# Creating conda \nconda create --name eDNA_COI mothur\n\n# Installing different versions of packages\nconda install -c conda-forge gsl=2.5\nconda install -c bioconda mafft\n</code></pre> <p>The gsl version downgraded mothur (1.48 -&gt; 1.44) and vsearch (2.15 -&gt; 2.13) as well.</p>"},{"location":"Computing_Begin_Resources/","title":"Computing Beginner Resources","text":""},{"location":"Computing_Begin_Resources/#shell-linux-and-bash","title":"Shell, Linux, and Bash","text":"<p>Shell: A shell is a command-line interface program that provides a user interface for interacting with an operating system Bash: short for Bourne-Again SHell, is a command-line shell and scripting language widely used in Unix-like operating systems, including Linux.    Linux/Unix: both operating systems that are used through a command line but Linux is an open-source version of Unix.  </p> <ul> <li>Bioinformatics | PNNL </li> <li>An Introduction to Linux Basics | DigitalOcean </li> <li>The Bash Guide </li> <li>Introducing the Shell </li> <li>Introduction to the Command Line for Genomics </li> </ul>"},{"location":"Computing_Begin_Resources/#rstudio","title":"RStudio","text":"<ul> <li>A Installing R and RStudio | Hands-On Programming with R (rstudio-education.github.io) </li> <li>Hands-On Programming with R (rstudio-education.github.io) </li> <li>R-Crash-Course </li> <li>Intro2R </li> <li>A crash-course in using a project-oriented workflow with Git + GitHub in scientific research </li> </ul>"},{"location":"Computing_Begin_Resources/#metabarcoding-bioinformatics","title":"Metabarcoding bioinformatics","text":"<ul> <li>Bioinformatic Methods for Biodiversity Metabarcoding \u2014 Bioinformatic Methods for Biodiversity Metabarcoding documentation (learnmetabarcoding.github.io) </li> <li>Introduction to the bioinformatic analysis of eDNA metabarcoding data \u2013 eDNA (otagobioinformaticsspringschool.github.io) </li> </ul>"},{"location":"Computing_Begin_Resources/#recommended-courses","title":"Recommended courses","text":"<ul> <li>FISH546 - Bioinformatics for Environmental Sciences </li> <li>FISH274 - Introduction to Data Analysis for Aquatic Sciences </li> <li>Data Carpentry for Biologists </li> </ul>"},{"location":"Computing_GMGI/","title":"GMGI in-house Computing Resources","text":"<p>Gloucester Marine Genomics Institute (GMGI) has 2 in-house servers that are used for bioinformatic analyses. </p> <p></p> <p>Ubuntu Linux operating system aka Humarus </p> <p>Humarus is primarily used for large-scale jobs (e.g., genome assemblies) and thus not the primary working area for Fisheries. </p> <p>Red Hat Enterprise Linux (RHEL) aka Gadus </p> <p>RHEL/Gadus is the primary working area, storage space, and is data is backed up daily to the Synology RackStation in-house. </p>"},{"location":"Computing_GMGI/#logging-in","title":"Logging in","text":"<p>Use ssh with username and the correct IP address that can be found on Lab Archives. Follow instructions for entering password. New users will need to get set-up with Jen while onboarding. </p> <pre><code>ssh username@123.456.7.8\n</code></pre>"},{"location":"Computing_GMGI/#server-structure","title":"Server Structure","text":"<p>Once logged in, users are directed to their home directory (<code>~/</code>) by default. This space has limited storage and is not intended for regular work. The Fisheries team primarily uses the NU Discovery Cluster for active projects and GMGI's in-house resources for long-term storage and data archiving. Consequently, team members typically use their home directory only for data transfers. </p> <p>General server structure: </p> <p>Do not edit any folder other than <code>data</code>. Only the RHEL main contact is responsible for downloading modules or setting up users. </p> <pre><code>[estrand@gadus ~]$ cd ../../\n[estrand@gadus /]$ ls\nbin  boot  data  dev  etc  home  lib  lib64  media  mnt  opt  proc  root  run  sbin  srv  sys  tmp  usr  var\n</code></pre> <p>Subdirectories within <code>data</code>:  </p> <ul> <li><code>prj</code>: Each lab has their own folder (e.g., <code>prj/Fisheries</code>) that is a working area for data and bioinformatic analyses.    </li> <li><code>resources</code>: Shared resources like common databases, modules, and scripts live here.  </li> <li><code>usr</code> and <code>var</code> are for the RHEL main contact only. </li> </ul> <pre><code>[estrand@gadus /]$ cd data/\n[estrand@gadus data]$ ls\nprj  resources  usr  var\n</code></pre> <p>Fisheries folders (<code>prj/Fisheries</code>):  </p> <p>We organize these folders by type of analyses or project. I.e., all eDNA projects should be nested within <code>edna</code>. </p> <pre><code>[estrand@gadus Fisheries]$ ls\n202402_negatives  edna  epiage  JonahCrabGenome  JonahCrabPopGen  lobster  SandLanceData\n</code></pre>"},{"location":"Computing_GMGI/#programs-and-modules","title":"Programs and modules","text":"<p>We run programs as 'modules' that are downloaded by the RHEL's main contact (Jen). If you need a program, send Jen a slack or email with the program name, download link, and if an R package, specify if it is a Bioconductor package or regular CRAN repository package. Once a program is downloaded as a module, this is available for all users. Global installation of programs and R packages helps keep the server uncluttered and not waste space with multiple installations. Do not install your own copies. </p> <p>Common commands:   </p> <ul> <li>To find already installed programs: <code>module avail</code> </li> <li>To get information about a module: <code>module help [module/version]</code> or <code>module whatis [module/version]</code>. \"help\" will provide what the module is, package information including version and install date, and a link to the documentation/github. \"whatis\" will provide a short, one line description of the program.    </li> <li>To load a module: <code>module load [module/version]</code> (e.g., <code>module load bamUtil/v1.0.15</code>). Loading a module will put all the necessary executables and dependencies for that program in your path so you can call the commands from any location (i.e. your working directory).    </li> </ul> <p>Replace \"[module/version]\" with the information for your module of interest, as it shows up in \"module avail\" list.</p>"},{"location":"Computing_GMGI/#resource-usage","title":"Resource usage","text":"<p>The GMGI RHEL does not currently have a job scheduler program so each user needs to be extremely careful with how much memory and resources their scripts take up. RHEL has 128 processors (CPUs) that are available total so users need to split this. Users should use CPU usage between 1-32 threads max at a time to allow other teams to use the server as well. </p> <p>Common commands:    </p> <ul> <li>Check all jobs that are running: <code>top</code> and to exit that screen, click Q  </li> <li>Check only our user: <code>top -u username</code> and to exit that screen, click Q  </li> </ul> <p>The most important aspects to watch are Job %CPU and %MEM, server %CPU, and load average. The load average is the average number of processes that are either running on the CPU or waiting for CPU time over the last 1, 5, and 15 minutes. </p> <p>In the example below, user #1 is using a program called 'cd-hit' that is currently using 1598% CPU, which is the equivalent of using 16 CPU cores or processors. When running a job, this is the value that I would check on the most to make sure I'm not taking up the entire server. Programs will have different default CPU maximums so check default flags prior to running scripts. </p> <p></p>"},{"location":"Computing_GMGI/#running-a-bioinformatic-script","title":"Running a bioinformatic script","text":"<p>Using \"tmux\" terminal multiplexer will allow you to runs scripts in multiple windows within a single terminal window, and to jump back and forth between them. This also allows a user to start a script, log off and have that continue to run while the user's computer isn't connected to internet. This is also called using a 'screen' on other servers but screen was deprecated after RHEL7, and our system was upgraded from RHEL7 to RHEL8 OS in Sept. 2023.</p> <p>Common commands:     </p> <ul> <li>Create a new session named \"test\": <code>tmux new -s test</code> </li> <li>Detach from a session: Press Ctrl+B, release, and then press D  </li> <li>Reopen/attach a detached session: <code>tmux attach-session -t test</code> </li> <li>View and/or switch between sessions without detaching from tmux: Prese Ctrl+B, release, than press W. A list will appear and you can toggle between options using the up and down arrows. The select one, make sure it is highlighted and press Enter.  </li> <li>End a tmux session (forever - not just detached): In the attached session, type <code>exit</code> and press enter. Or press Ctrl+D  </li> </ul> <p>Write everything in the tmux session to a text file: </p> <ul> <li>Output the history limit: <code>tmux display-message -p -F \"#{history_limit}\" -t test</code> </li> <li>Capture output to text file: <code>tmux capture-pane -Jp -S -### -t test &gt; test.txt</code>. Replace the ### with the history limit above.  </li> </ul> <p>Note: the automatic limit is 2000 lines. If you know you're going to run verbose commands and want to be able to capture it all in a log file run the command below right before starting a new session (note must already have another session open): <code>tmux set-option -g history-limit 99999</code></p>"},{"location":"Computing_NU/","title":"Northeastern University Computing Resources","text":"<p>A high-performance computing resource for the Northeastern research community, the Discovery cluster is located in the Massachusetts Green High Performance Computing Center in Holyoke, Massachusetts.</p> <p>The Discovery cluster provides Northeastern researchers with access to more than 45,000 CPU cores and more than 400 GPUs. Connected to the university network over 10 Gbps Ethernet (GbE) for high-speed data transfer, Discovery provides 5 PB of available storage on a high-performance GPFS parallel file system. Compute nodes are connected with either 10 GbE or a high-performance HDR100 InfiniBand (IB) interconnect running at 200 Gbps, supporting all types and scales of computational workloads.</p> <p>As GMGI's researchers, we have access to Northeastern's HPC resources through an MOU established in Fall 2023. </p> <p>Read the HPC resource documentation prior to getting started: Research Computing - NURC RTD (northeastern.edu).</p> <p></p>"},{"location":"Computing_NU/#logging-in","title":"Logging in","text":"<p>Before creating an account with the NU Discovery Cluster, claim your Northeastern email and sponsored account.</p> <ol> <li>Log into (with the northeastern email and pw previously claimed): Home - Northeastern Tech Service Portal.  </li> <li>Navigate to High Performance Computing - Northeastern Tech Service Portal.    </li> <li>Request an account (Research Computing Access Request - Northeastern Tech Service Portal). Fill out the form with your Northeastern email, and following options:  </li> <li>Select: I do not have access to Discovery. I am requesting a new account.  </li> <li>Affiliation with Northeastern University: Visiting Researcher (Greg says this answer doesn't really matter).   </li> <li>University Sponsor: Geoffrey Trusell  </li> <li>Gaussian: No (This is a specific program, if you don't know what it is, you don't need it).  </li> <li>Select: By clicking here, I acknowledge that I have read and agree to the following.  </li> <li>This triggers an email to Geoff Trusell to approve your account. Send an email to Geoff letting him know that you are activating your account that needs his approval.  </li> <li>Once Geoff approves the account sponsorship, then Greg and the computing team will finish setting up your account.</li> </ol> <p>You can operate on the Discovery Cluster in two ways: 1. via Linux operating system on your computer or ssh client 2. NU's Open On Demand (Open OnDemand (OOD) - RC RTD (northeastern.edu)) GUI. Accessing Open OnDemand - RC RTD (northeastern.edu). With OOD interface, you can access plug-ins (i.e. RStudio) and launch the server from the web. Serena recommended using incognito window because OOD usually works better without the caching.</p> <pre><code>ssh username@login.discovery.neu.edu\n</code></pre>"},{"location":"Computing_NU/#server-structure","title":"Server structure","text":"<p>We have a 30 TB maximum in our working space <code>/work/gmgi</code>:</p> <ul> <li>Each lab has their own subfolder that serves as their storage and working space (e.g., <code>work/gmgi/Fisheries/</code>).  </li> <li><code>databases/</code>: Shared folder for common databases for amplicon sequencing (i.e., 12S, 16S, 18S, COI) and NCBI nt database. View the README file for databases sources.</li> <li><code>containers/</code>: Shared folder for custom built containers.  </li> <li><code>packages/</code>: Shared folder for programs downloaded for all users.  </li> <li><code>miniconda3/</code> and <code>Miniconda3-latest-Linux-x86_64.sh</code>: Shared resource for building conda environments. Environments built here can be used by everyone. Do not edit. </li> <li><code>check_storage.sh</code>: Bash script built to calculate TB usage from each lab's folder and gmgi's work and output is <code>storage_summary_2024-09-23.txt</code> with the date calculated.</li> </ul> <pre><code>[e.strand@login-00 ~]$ cd /work/gmgi\n[e.strand@login-00 gmgi]$ ls\ncheck_storage.sh  containers  databases  ecosystem-diversity  Fisheries  miniconda3  Miniconda3-latest-Linux-x86_64.sh  packages  storage_summary_2024-09-23.txt\n</code></pre> <p>Fisheries folder (<code>work/gmgi/Fisheries/</code>) is split by the type of project. <code>reference_genomes</code> includes .fasta reference files for organisms rather than a database (<code>Haddock_ref.fasta</code>). </p> <pre><code>[e.strand@login-00 Fisheries]$ ls\neDNA  epiage  reference_genomes\n</code></pre>"},{"location":"Computing_NU/#storage-rules","title":"Storage rules","text":"<p>While analyzing data, NOT just at the end of a project!</p> <p>Raw data files are backed up on AWS services and on GMGI RHEL Gadus immediately upon receiving data. If working on NU cluster, once user is happy with data analysis pipeline, raw and final data is to be removed from NU cluster and only kept on AWS services or GMGI's RHEL server. </p> <p>Compressing files:  </p> <ul> <li>Gzip all fastq files (e.g., raw data, trimmed data), .fasta/.fa files (e.g., reference genomes), and large .txt files (e.g., intermediate files created during analysis): <code>gzip *.fastq</code> or create a slurm array with a sbatch script.    </li> <li>Genozip all .bam, .sam, .vcf files (e.g., intermediate files created during analysis). Genozip has been downloaded in /work/gmgi/packages for general use.   </li> </ul> <p>Space-related commands:  </p> <ul> <li>List all files within a directory and human-readable sizes (folder size is not total size of folder contents): <code>ls -lha</code> </li> <li>Calculate total storage taken up by one directory (change path as needed): <code>du -shc .[^.]* /work/gmgi/fisheries</code> </li> <li>In /work/gmgi/, there is a <code>check_storage.sh</code> bash script that will use the above commands to create a summary .txt file with the storage use of each team.  </li> </ul>"},{"location":"Computing_NU/#nu-contacts-and-research-computing-help","title":"NU Contacts and Research Computing Help","text":"<p>GMGI's two main contacts are: Greg Shomo (g.shomo@northeastern.edu) and Serena Caplins (s.caplins@northeastern.edu). </p> <p>If you need assistance, NU's support team is available at rchelp@northeastern.edu or consult the Frequently Asked Questions (FAQs). Emailing the rchelp team will create a help ticket one of NU's team members will be assigned to your case. When creating a help ticket, CC Geoff Trussell (g.trussell@northeastern.edu) and Jon Grabowski (j.grabowski@northeastern.edu). </p> <p>RC Help hosts office hours to connect with RC staff and Graduate Research Assistants (GRAs) to ask questions about any RC-related questions that you might have (e.g., Setting up conda environments, Installing software, Optimizing the runtime of your sbatch scripts, Effectively using the Open onDemand website): - Wednesdays 3 pm - 4 pm: zoom link - Thursdays 11 am - 12 pm: zoom link </p> <p>Ask 2-3 others at GMGI for assistance and/or attend office hours prior to creating a help ticket. We do not want to overwhelm the rchelp desk if we can troubleshoot internally first. GMGI has a #bioinformatics slack channel for this purpose.</p>"},{"location":"Computing_NU/#running-a-bioinformatic-script","title":"Running a bioinformatic script","text":"<p>Read through: Running Jobs - RC RTD (northeastern.edu) before starting. </p> <p>Jobs are run either through: 1. Interactive mode (immediate execution and feedback): <code>srun --pty bash</code> to claim a node and then utilize <code>bash scriptname.sh</code> to run a script.   2. Batch jobs: using scripts to manage longer-running jobs: <code>sbatch scriptname.sh</code> to run a script.  </p> <p>Interactive mode would be equivalent to running a job directly in your terminal window without starting a tmux session. Using batch jobs and shell scripts would be similar to tmux session where you can turn off wifi, walk away, etc. Interactive requires you to stay connected. </p> <p>Introduction to Slurm scripts:    </p> <ul> <li>Slurm - RC RTD (northeastern.edu) </li> <li>Best Practices - RC RTD (northeastern.edu) </li> </ul>"},{"location":"Computing_NU/#packages-and-modules","title":"Packages and modules","text":"<p>NU has some modules downloaded that are accessible for all users. Otherwise, larger packages should be installed in a conda environment by the user or simple packages can be downloaded to the <code>/work/gmgi/packages/</code> folder. Instructions for downloading to conda env: [https://rc-docs.northeastern.edu/en/latest/software/index.html]</p> <p>Common commands:   </p> <ul> <li>To find already installed programs: <code>module avail</code> </li> <li>To get information about a module: <code>module help [module/version]</code> or <code>module whatis [module/version]</code>. \"help\" will provide what the module is, package information including version and install date, and a link to the documentation/github. \"whatis\" will provide a short, one line description of the program.    </li> <li>To load a module: <code>module load [module/version]</code> (e.g., <code>module load bamUtil/v1.0.15</code>). Loading a module will put all the necessary executables and dependencies for that program in your path so you can call the commands from any location (i.e. your working directory).   </li> </ul>"},{"location":"Data%20Visualization%20in%20R/","title":"Data Visualization and Analysis in R","text":""},{"location":"Data%20Visualization%20in%20R/#data-wrangling","title":"Data wrangling","text":"<ul> <li>Top 10 Must-Know {dplyr} Commands for Data Wrangling in R</li> </ul>"},{"location":"Data%20Visualization%20in%20R/#visualization","title":"Visualization","text":"<p>Picking color:   </p> <ul> <li>Coolors palette generator</li> </ul> <p>Basic plots: - Tidyplots  - R graph gallery</p> <p>Heatmaps:    </p> <ul> <li>R package for using pheatmap with minimal coding: Tidy Heatmaps </li> <li>R package for aligning heatmaps: ggalign and twitter post with example</li> </ul>"},{"location":"Data%20Visualization%20in%20R/#data-analysis","title":"Data Analysis","text":"<p>Generalized Linear Models:     </p> <ul> <li>Workshop 6: Generalized linear models in R</li> </ul> <p>Functional Gene Annotation: - GOSemSim and clusterProfiler with visualization suggestions</p> <p>General: - Computation Genomics in R </p>"},{"location":"Data%20Visualization%20in%20R/#tips-and-tricks","title":"Tips and Tricks","text":"<p>Export an image as ggsave(\"file.svg\") and open in powerpoint. The image is vectorized and edit-able.  </p>"},{"location":"Data_Management/","title":"Data Management","text":"<p>GMGI Fisheries data management workflow:</p> <p></p>"},{"location":"Data_To_Server/","title":"Downloading sequencing data to servers","text":"<p>Goal: Download .fastq files from sequencing center to HPC and/or move data between HPCs and personal computers.</p> <p>Table of Contents: - GMGI in-house sequencing to HPC  - External sequencing to HPC  - HPC to Personal Computer - HPC to AWS back-up </p> <p>To transfer data from HPC to HPC (e.g., GMGI to NU), use Globus instructions outlined in External sequencing to HPC. </p>"},{"location":"Data_To_Server/#illumina-basespace-to-nu-discovery-cluster-or-gmgi-in-house-hpc","title":"Illumina BaseSpace to NU Discovery Cluster or GMGI in-house HPC","text":"<p>Illumina BaseSpace CLI instructions</p> <p>Connecting your user to Illumina BaseSpace:  </p> <p>Each user only needs to complete this once to set-up. If completed for previous projects, skip to downloading data steps. </p> <ol> <li>Create folder called bin: <code>mkdir $HOME/bin</code> </li> <li>Download BaseSpace CLI: <code>wget \"https://launch.basespace.illumina.com/CLI/latest/amd64-linux/bs\" -O $HOME/bin/bs</code> </li> <li>Change the file permissions to make the downloaded binary executable: <code>chmod u+x $HOME/bin/bs</code> </li> <li>Authenticate your account: <code>bs auth</code> </li> <li>Navigate to the webpage provided and authenticate use of BaseSpace CLI.  </li> </ol> <p>Download data from each run to desired output path: </p> <ol> <li>Find the Run ID of desired download: Within Sequence Hub, navigate to Runs and select the desired run. The Run ID is in the webpage handle (e.g., https://basespace.illumina.com/run/123456789/details). </li> </ol> <p></p> <ol> <li>Navigate to <code>cd $HOME/bin</code> and download dataset: <code>bs download run -n run_name --extension=fastq.gz -o /local/output/path</code>. Replace <code>run_name</code> with the exact name of the run on BaseSpace.  </li> <li>Navigate to the output path <code>cd /local/output/path</code> and move all files out of subdirectories: <code>mv */* .</code> </li> </ol>"},{"location":"Data_To_Server/#globus-to-nu-discovery-cluster-or-gmgi-in-house-hpc","title":"Globus to NU Discovery Cluster or GMGI in-house HPC","text":"<p>External sequencing centers (e.g., UConn) will share data via Globus. Instructions from NU on transfering data and using Globus. Globus works by transferring data between 'endpoints'. NU's endpoint is called Discovery Cluster which is searchable but our in-house GMGI endpoint needs to be created for each user. </p> <p>Globus instructions: [https://docs.globus.org/globus-connect-server/v5.4/quickstart/]. Create a Globus account prior to instructions below. If transferring to NU, user needs to connect their NU account to their Globus account (see above NU instructions for this step). </p> <p>GMGI endpoint set-up (only need to do this once): 1. Navigate to the globusconnectpersonal-3.2.2 module that is already downloaded on GMGI's in-house server: <code>cd /data/resources/app_modules/globusconnectpersonal-3.2.2</code>. 2. Set-up an endpoint: <code>./globusconnectpersonal -setup --no-gui</code> 3. This will then ask you to click on a log-in link. Once logged in, you receive a authorization code. Paste that in your terminal window where it asked for this code. 4. Name your endpoint with your own user (change this to your first and last name): <code>user.name</code> 5. If successfully, globus will output: </p> <pre><code>Input a value for the Endpoint Name: user.name\n\nregistered new endpoint, id: [unique ID to you]\n\nsetup completed successfully\n</code></pre> <p>Start Globus transfer: 1. [GMGI only] Navigate to the globusconnectpersonal-3.2.2 module that is already downloaded on GMGI's in-house server: <code>cd /data/resources/app_modules/globusconnectpersonal-3.2.2</code>. 2. [GMGI only] Activate personal endpoint: <code>./globusconnectpersonal -start &amp;</code> 3. [GMGI only] Your <code>user.name</code> endpoint will now appear as an option on the Globus online interface.  4. Log into Globus and Navigate to 'Collections' on the left hand panel. Confirm that your GMGI endpoint is activated (green icon):</p> <p></p> <ol> <li>Select the 'File Manager' on the left hand panel. Choose the sequencing center endpoint in the left side and the server end point on the right side. NU's Discovery Cluster is searchable but GMGI endpoint will the user.name set up in previous steps. </li> </ol> <p></p> <ol> <li>Select all files that you want to transfer.  </li> <li>Select Start to begin the transfer.  </li> <li>Check the status of a transfer by selecting 'Activity' on the left hand panel.  </li> <li>[GMGI only] Once transfer is complete, deactivate the endpoint: <code>./globusconnectpersonal -stop</code>. </li> </ol>"},{"location":"Data_To_Server/#hpc-to-personal-computer-or-vice-versa","title":"HPC to personal computer or vice versa","text":"<p>Users can do this via Globus or 'scp' (secure copy paste) commands detailed below. NU instructions on transfer via terminal. Make sure you're using \"xfer.discovery.neu.edu\" for the discovery cluster and not login.discovery.neu.edu, or you'll get an email warning you that you're using too much CPU!</p> <p>For all the below code, change content in &lt;&gt; and then delete the &lt;&gt;. All commands need to be run in own terminal and not logged onto either server. </p> <p>Transfer a file: - To NU from personal computer: <code>scp &lt;filename and path&gt; &lt;username&gt;@xfer.discovery.neu.edu:/path/</code>  - To personal computer from NU: <code>scp &lt;username&gt;@xfer.discovery.neu.edu:/path/ /output/path/</code> </p> <p>Transfer a directory (a.k.a., repository) to personal computer from NU: <code>scp -r &lt;username&gt;@xfer.discovery.neu.edu:/path/ /output/path/</code> </p> <p>To transfer directly from GMGI to NU or vice versa, use Globus. </p>"},{"location":"Data_To_Server/#aws-back-up","title":"AWS Back-up","text":"<p>AWS is our Amazon Web Services S3 cloud-based storage to backup data long-term data storage and back-up. GMGI uploads data from our in-house server to AWS.</p> <p>What should be backed up: - Raw data files such as fastq files directly from the sequencer - Final result data files (i.e. count table, assemblies, etc.)  </p> <ol> <li>Make sure all files are compressed by:  </li> <li>Gzip all fastq files (e.g., raw data, trimmed data), .fasta/.fa files (e.g., reference genomes), and large .txt files (e.g., intermediate files created during analysis): <code>gzip *.fastq</code> or create a slurm array with a sbatch script.  </li> <li>Genozip all .bam, .sam, .vcf files (e.g., intermediate files created during analysis). Genozip program is downloaded NU in <code>/work/gmgi/packages/</code> for general use.  </li> </ol> <p>Prior to AWS back-up, check with Tim or Emma for approval of files and compression. </p> <ol> <li>Create a new screen session called AWS_tar (user can change this name to desired): <code>tmux new -s AWS_tar</code> </li> <li>Create a txt file with file sizes of all desired input: <code>ls -l *gz &gt; file_size.txt</code> </li> <li>Edit this file to be only file sizes and names: <code>awk '{print $5,$9}' file_size.txt &gt; file_size_edited.txt</code> </li> <li>View this edited file: <code>head file_size_edited.txt</code></li> </ol> <pre><code>11400971821 Mae-263_S1_R1_001.fastq.gz\n\n12253428145 Mae-263_S1_R2_001.fastq.gz\n\n11962611469 Mae-266_S2_R1_001.fastq.gz\n\n12839131166 Mae-266_S2_R2_001.fastq.gz\n\n9691926610 Mae-274_S3_R1_001.fastq.gz\n</code></pre> <ol> <li>Sum the first column: <code>awk '{sum += $1} END {print sum}' file_size_edited.txt</code>. This value is in Byte (B), but convert to MB or TB for a more helpful value to work with. It's important to know the size of the data you are working for storage and cost purposes. Backing up to AWS costs $$/monthly based on TBs stored and our HPC systems have max TB storage limitations.  </li> <li>Tar all desired data to result in on zipped file: <code>tar -czvf HaddockEpiAge1_rawData_20231113.tar.gz ./*fastq.gz</code>. Tar file name needs to be a unique identifier that includes project name, date, and description of the files included.  </li> <li>Detach from a session: Press Ctrl+B, release, and then press D. This tar function will take awhile especially for large datasets.  </li> <li>Reopen/attach a detached session: <code>tmux attach-session -t AWS_tar</code>.  </li> <li>Check the tar file size: <code>ls -lha</code>. Example output: <code>-rw-rw-r--. 1 estrand science 1736366676523 Nov 15 17:54 HaddockEpiAge1_rawData_20231113.tar.gz</code>. This 1736366676523 value is the size in B which should match exactly the sum of all input file sizes calculated previously.   </li> <li>Once this tar function is complete, end a tmux session (forever - not just detached): In the attached session, type exit and press enter. Or press Ctrl+D.  </li> <li>Move the packed tar archive file to AWS transfer folder: <code>mv HaddockEpiAge1_rawData_20231113.tar.gz /data/prj/AWS-transfers</code>.  </li> <li>Notify Jen that there is a transfer waiting so she can move this to AWS services and then remove from the /AWS-transfers folder. </li> </ol>"},{"location":"Introduction%20to%20Containers/","title":"Introduction to Containers","text":"<p>A container is a standard unit of software that packages up code and all its dependencies so the application runs quickly and reliably from one computing environment to another.</p> <p></p> <p>Two popular container engines: Docker, Singularity </p> <p></p> <p>Both work very similarly but Singularity is specifically designed for HPC systems.</p> <p>A container image is a lightweight, standalone, executable package of software that includes everything needed to run an application: code, runtime, system tools, system libraries and settings. Container images become containers at runtime and in the case of Docker containers \u2013 images become containers when they run on a container engine.</p> <p></p>"},{"location":"test/","title":"Test","text":"<p>TEST</p>"},{"location":"EpiAge/00-Notes/","title":"00 Notes","text":"<p>SNP calling from WGBS: https://github.com/Zymo-Research/wgbs-genetic-variants?_kx=fypEHc9edYeGAcai3QoMTMi5t0r0PIikczp5YwueFQk.JaPMuc</p>"},{"location":"PopGen/eDNA_01/","title":"coming soon!","text":"<p>TEST</p>"},{"location":"PopGen/eDNA_02/","title":"eDNA 02","text":"<p>TEST</p>"},{"location":"eDNA%2012S%20metab/","title":"Environmental DNA (eDNA) Bioinformatic Workflow","text":"<p>Graphic from Szekely et al. 2022</p>"},{"location":"eDNA%2012S%20metab/#bony-fish-and-elasmobranch-targets","title":"Bony Fish and Elasmobranch Targets","text":""},{"location":"eDNA%2012S%20metab/#invertebrate-targets","title":"Invertebrate Targets","text":""},{"location":"eDNA%2012S%20metab/01-Metabarcoding%20ampliseq%2012S%20Riaz/","title":"Metabarcoding workflow for 12S amplicon sequencing with Riaz primers","text":"<p>page details in progress. </p> <p>The 12S rRNA gene region of the mitogenome is ~950 bp. There are two popular primer sets to amplify two different regions of 12S: Riaz and MiFish. The following workflow includes script specific to the Riaz primer set, but includes some notes on the MiFish U/E primer set. </p> <p></p> <p>Riaz ecoPrimers citation: Riaz et al. 2011 MiFish citation: Miya et al. 2015</p> <p>Workflow done on HPC. Scripts to run: </p> <ol> <li>00-fastqc.sh   </li> <li>00-multiqc.sh  </li> <li>01a-metadata.R</li> <li>01b-ampliseq.sh</li> <li>02-taxonomicID.sh  </li> </ol>"},{"location":"eDNA%2012S%20metab/01-Metabarcoding%20ampliseq%2012S%20Riaz/#step-1-confirm-conda-environment-is-available-and-activate","title":"Step 1: Confirm conda environment is available and activate","text":"<p>The conda environment is started within each slurm script, but to activate conda environment outside of the slurm script to update packages or check what is installed:</p> <pre><code># Activate fisheries eDNA conda environment \nsource /work/gmgi/miniconda3/bin/activate fisheries_eDNA\n\n# List all available environments \nconda env list \n\n# List all packages installed in fisheries_eDNA\nconda list \n\n# Update a package\nconda update [package name]\n\n# Update nextflow ampliseq workflow \nnextflow pull nf-core/ampliseq\n</code></pre>"},{"location":"eDNA%2012S%20metab/01-Metabarcoding%20ampliseq%2012S%20Riaz/#step-2-assess-quality-of-raw-data","title":"Step 2: Assess quality of raw data","text":"<p>Background information on FASTQC. </p> <p><code>00-fastqc.sh</code>: </p> <pre><code>#!/bin/bash\n#SBATCH --error=output/fastqc_output/\"%x_error.%j\" #if your job fails, the error report will be put in this file\n#SBATCH --output=output/fastqc_output/\"%x_output.%j\" #once your job is completed, any final job report comments will be put in this file\n#SBATCH --partition=short\n#SBATCH --nodes=1\n#SBATCH --time=20:00:00\n#SBATCH --job-name=fastqc\n#SBATCH --mem=3GB\n#SBATCH --ntasks=24\n#SBATCH --cpus-per-task=2\n\n### USER TO-DO ### \n## 1. Set paths for your project\n\n# Activate conda environment\nsource /work/gmgi/miniconda3/bin/activate fisheries_eDNA\n\n## SET PATHS \nraw_path=\"\"\nout_dir=\"\"\n\n## CREATE SAMPLE LIST FOR SLURM ARRAY\n### 1. Create list of all .gz files in raw data path\nls -d ${raw_path}/*.gz &gt; ${raw_path}/rawdata\n\n### 2. Create a list of filenames based on that list created in step 1\nmapfile -t FILENAMES &lt; ${raw_path}/rawdata\n\n### 3. Create variable i that will assign each row of FILENAMES to a task ID\ni=${FILENAMES[$SLURM_ARRAY_TASK_ID]}\n\n## RUN FASTQC PROGRAM \nfastqc ${i} --outdir ${out_dir}\n</code></pre> <p>To run:   - Start slurm array with file 0 (e.g., with 138 files) = <code>sbatch --array=0-137 00-fastqc.sh</code>.</p> <p>Notes:  </p> <ul> <li>This is going to output many error and output files. After job completes, use <code>cat *output.* &gt; ../fastqc_output.txt</code> to create one file with all the output and <code>cat *error.* &gt; ../fastqc_error.txt</code> to create one file with all of the error message outputs. </li> <li>Within the <code>out_dir</code> output folder, use <code>ls *html | wc</code> to count the number of html output files (1st/2nd column values). This should be equal to the --array range used and the number of raw data files. If not, the script missed some input files so address this before moving on.  </li> </ul>"},{"location":"eDNA%2012S%20metab/01-Metabarcoding%20ampliseq%2012S%20Riaz/#step-3-visualize-quality-of-raw-data","title":"Step 3: Visualize quality of raw data","text":"<p>Background information on MULTIQC.</p> <p><code>00-multiqc.sh</code> </p> <pre><code>#!/bin/bash\n#SBATCH --error=output/\"%x_error.%j\" #if your job fails, the error report will be put in this file\n#SBATCH --output=output/\"%x_output.%j\" #once your job is completed, any final job report comments will be put in this file\n#SBATCH --partition=short\n#SBATCH --nodes=1\n#SBATCH --time=10:00:00\n#SBATCH --job-name=multiqc\n#SBATCH --mem=8GB\n#SBATCH --ntasks=24\n#SBATCH --cpus-per-task=2\n\n### USER TO-DO ### \n## 1. Set paths for your project\n## 2. Optional: change file name (multiqc_raw.html) as desired\n\n# Activate conda environment\nsource /work/gmgi/miniconda3/bin/activate fisheries_eDNA\n\n## SET PATHS \n## fastqc_output = output from 00-fastqc.sh; fastqc program\nfastqc_output=\"\" \nmultiqc_dir=\"\" \n\n## RUN MULTIQC \nmultiqc --interactive ${fastqc_output} -o ${multiqc_dir} --filename multiqc_raw.html\n</code></pre> <p>To run: - <code>sbatch 00-multiqc.sh</code> </p> <p>Notes:  </p> <ul> <li>Depending on the number of files per project, multiqc can be quick to run without a slurm script. To do this, activate conda environment within a working node:</li> </ul> <pre><code># Use srun to claim a node\nsrun --pty bash \n\n# Activate conda environment\nsource /work/gmgi/miniconda3/bin/activate fisheries_eDNA\n\n## SET PATHS \n## fastqc_output = output from 00-fastqc.sh; fastqc program\nfastqc_output=\"\" \nmultiqc_dir=\"\" \n\n## RUN MULTIQC \nmultiqc --interactive ${fastqc_output} -o ${multiqc_dir} --filename multiqc_raw.html\n</code></pre>"},{"location":"eDNA%2012S%20metab/01-Metabarcoding%20ampliseq%2012S%20Riaz/#step-4-nf-coreampliseq","title":"Step 4: nf-core/ampliseq","text":"<p>Nf-core: A community effort to collect a curated set of analysis pipelines built using Nextflow. Nextflow: scalable and reproducible scientific workflows using software containers, used to build wrapper programs like the one we use here.  </p> <p>[https://nf-co.re/ampliseq/2.11.0]: nfcore/ampliseq is a bioinformatics analysis pipeline used for amplicon sequencing, supporting denoising of any amplicon and supports a variety of taxonomic databases for taxonomic assignment including 16S, ITS, CO1 and 18S. </p> <p></p> <p>We use ampliseq for the following programs:  </p> <ul> <li>Cutadapt is trimming primer sequences from sequencing reads. Primer sequences are non-biological sequences that often introduce point mutations that do not reflect sample sequences. This is especially true for degenerated PCR primer. If primer trimming would be omitted, artifactual amplicon sequence variants might be computed by the denoising tool or sequences might be lost due to become labelled as PCR chimera.  </li> <li>DADA2 performs fast and accurate sample inference from amplicon data with single-nucleotide resolution. It infers exact amplicon sequence variants (ASVs) from amplicon data with fewer false positives than many other methods while maintaining high sensitivity.  </li> </ul> <p>We skip the taxonomic assignment because we use 3-db approach described in the next section. </p>"},{"location":"eDNA%2012S%20metab/01-Metabarcoding%20ampliseq%2012S%20Riaz/#12s-primer-sequences-required","title":"12S primer sequences (required)","text":"<p>Below is what we used for 12S amplicon sequencing. Ampliseq will automatically calculate the reverse compliment and include this for us.</p> <p>Riaz 12S amplicon F Original: ACTGGGATTAGATACCCC Riaz 12S amplicon F Degenerate: ACTGGGATTAGATACCCY    Riaz 12S amplicon R: TAGAACAGGCTCCTCTAG     </p> <p>MiFish primer set: </p> <p>MiFish-U 12S amplicon F: GTCGGTAAAACTCGTGCCAGC MiFish-U 12S amplicon R: CATAGTGGGGTATCTAATCCCAGTTTG       </p> <p>MiFish-E 12S amplicon F: GTTGGTAAATCTCGTGCCAGC   MiFish-E 12S amplicon R: CATAGTGGGGTATCTAATCCTAGTTTG    </p>"},{"location":"eDNA%2012S%20metab/01-Metabarcoding%20ampliseq%2012S%20Riaz/#metadata-sheet-optional","title":"Metadata sheet (optional)","text":"<p>The metadata file has to follow the QIIME2 specifications. Below is a preview of the sample sheet used for this test. Keep the column headers the same for future use. The first column needs to be \"ID\" and can only contain numbers, letters, or \"-\". This is different than the sample sheet. NAs should be empty cells rather than \"NA\". </p>"},{"location":"eDNA%2012S%20metab/01-Metabarcoding%20ampliseq%2012S%20Riaz/#create-samplesheet-sheet-for-ampliseq","title":"Create samplesheet sheet for ampliseq","text":"<p>This file indicates the sample ID and the path to R1 and R2 files. Below is a preview of the sample sheet used in this test. File created on RStudio Interactive on Discovery Cluster using (<code>create_metadatasheets.R</code>).  </p> <ul> <li>sampleID (required): Unique sample IDs, must start with a letter, and can only contain letters, numbers or underscores (no hyphons!).  </li> <li>forwardReads (required): Paths to (forward) reads zipped FastQ files  </li> <li>reverseReads (optional): Paths to reverse reads zipped FastQ files, required if the data is paired-end  </li> <li>run (optional): If the data was produced by multiple sequencing runs, any string  </li> </ul> sampleID forwardReads reverseReads run sample1 ./data/S1_R1_001.fastq.gz ./data/S1_R2_001.fastq.gz A sample2 ./data/S2_fw.fastq.gz ./data/S2_rv.fastq.gz A sample3 ./S4x.fastq.gz ./S4y.fastq.gz B sample4 ./a.fastq.gz ./b.fastq.gz B <p>This is an R script, not slurm script. Open RStudio interactive on Discovery Cluster to run this script.</p> <p>Prior to running R script, use the <code>rawdata</code> file created for the fastqc slurm array from within the raw data folder to create a list of files. Below is an example from our Offshore Wind project but the specifics of the sampleID will be project dependent. This project had four sequencing runs with different file names. </p> <p><code>01a-metadata.R</code></p> <pre><code>## Load libraries \n\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(strex) \n\n### Read in sample sheet \n\nsample_list &lt;- read.delim2(\"/work/gmgi/Fisheries/eDNA/offshore_wind2023/raw_data/rawdata\", header=F) %&gt;% \n  dplyr::rename(forwardReads = V1) %&gt;%\n  mutate(sampleID = str_after_nth(forwardReads, \"data/\", 1),\n         sampleID = str_before_nth(sampleID, \"_R\", 1))\n\n# creating sample ID \nsample_list$sampleID &lt;- gsub(\"-\", \"_\", sample_list$sampleID)\n\n# keeping only rows with R1\nsample_list &lt;- filter(sample_list, grepl(\"R1\", forwardReads, ignore.case = TRUE))\n\n# duplicating column \nsample_list$reverseReads &lt;- sample_list$forwardReads\n\n# replacing R1 with R2 in only one column \nsample_list$reverseReads &lt;- gsub(\"R1\", \"R2\", sample_list$reverseReads)\n\n# rearranging columns \nsample_list &lt;- sample_list[,c(2,1,3)]\n\nsample_list %&gt;% write.csv(\"/work/gmgi/Fisheries/eDNA/offshore_wind2023/metadata/samplesheet.csv\", \n                          row.names=FALSE, quote = FALSE)\n</code></pre>"},{"location":"eDNA%2012S%20metab/01-Metabarcoding%20ampliseq%2012S%20Riaz/#run-nf-coreampliseq-cutadapt-dada2","title":"Run nf-core/ampliseq (Cutadapt &amp; DADA2)","text":"<p>Update ampliseq workflow if needed: <code>nextflow pull nf-core/ampliseq</code>. </p> <p>Below script is set for Riaz primers:</p> <p><code>01b-ampliseq.sh</code>:</p> <pre><code>#!/bin/bash\n#SBATCH --error=output/\"%x_error.%j\" #if your job fails, the error report will be put in this file\n#SBATCH --output=output/\"%x_output.%j\" #once your job is completed, any final job report comments will be put in this file\n#SBATCH --partition=short\n#SBATCH --nodes=1\n#SBATCH --time=20:00:00\n#SBATCH --job-name=ampliseq\n#SBATCH --mem=70GB\n#SBATCH --ntasks=24\n#SBATCH --cpus-per-task=2\n\n### USER TO-DO ### \n## 1. Set paths for project \n## 2. Adjust SBATCH options above (time, mem, ntasks, etc.) as desired  \n## 3. Fill in F primer information based on primer type (no reverse compliment needed)\n## 4. Adjust parameters as needed (below is Fisheries team default for 12S)\n\n# LOAD MODULES\nmodule load singularity/3.10.3\nmodule load nextflow/23.10.1\n\n# SET PATHS \nmetadata=\"\" \noutput_dir=\"\"\n\nnextflow run nf-core/ampliseq -resume \\\n   -profile singularity \\\n   --input ${metadata}/samplesheet.csv \\\n   --FW_primer \"\" \\\n   --RV_primer \"TAGAACAGGCTCCTCTAG\" \\\n   --outdir ${output_dir} \\\n   --trunclenf 100 \\\n   --trunclenr 100 \\\n   --trunc_qmin 25 \\\n   --max_len 200 \\\n   --max_ee 2 \\\n   --min_len_asv 80 \\\n   --max_len_asv 115 \\\n   --sample_inference pseudo \\\n   --skip_taxonomy \\\n   --ignore_failed_trimming\n</code></pre> <p>To run:  - <code>sbatch 01b-ampliseq.sh</code> </p> <p>MiFish amplifies a longer target region which requires 2x250 bp sequencing (500 cycle kit). Thus following edits are required if using MiFish primers: - F/R primer correct sequences  - Edit <code>--min_len_asv</code>, <code>--max_len_asv</code> to reflect the correct target region length   - Edit <code>--trunclenf</code>, <code>--trunclenr</code>, <code>--max_len</code> to reflect correct trimming length for longer reads  </p>"},{"location":"eDNA%2012S%20metab/01-Metabarcoding%20ampliseq%2012S%20Riaz/#files-generated-by-ampliseq","title":"Files generated by ampliseq","text":"<p>Pipeline summary reports:  </p> <ul> <li><code>summary_report/</code></li> <li><code>summary_report.html</code>: pipeline summary report as standalone HTML file that can be viewed in your web browser.</li> <li><code>*.svg*</code>: plots that were produced for (and are included in) the report.</li> <li><code>versions.yml</code>: software versions used to produce this report.</li> </ul> <p>Preprocessing:  </p> <ul> <li>FastQC: <code>fastqc/</code> and <code>*_fastqc.html</code>: FastQC report containing quality metrics for your untrimmed raw fastq files.  </li> <li>Cutadapt: <code>cutadapt/</code> and <code>cutadapt_summary.tsv</code>: summary of read numbers that pass cutadapt  </li> <li>MultiQC: <code>multiqc</code>, <code>multiqc_data/</code>, <code>multiqc_plots/</code> with <code>multiqc_report.html</code>: a standalone HTML file that can be viewed in your web browser; </li> </ul> <p>ASV inferrence with DADA2:  </p> <ul> <li><code>dada2/</code>, <code>dada2/args/</code>, <code>data2/log/</code> </li> <li><code>ASV_seqs.fasta</code>: Fasta file with ASV sequences.</li> <li><code>ASV_table.tsv</code>: Counts for each ASV sequence.</li> <li><code>DADA2_stats.tsv</code>: Tracking read numbers through DADA2 processing steps, for each sample.</li> <li><code>DADA2_table.rds</code>: DADA2 ASV table as R object.</li> <li><code>DADA2_table.tsv</code>: DADA2 ASV table.  </li> <li><code>dada2/QC/</code></li> <li><code>*.err.convergence.txt</code>: Convergence values for DADA2's dada command, should reduce over several magnitudes and approaching 0.  </li> <li><code>*.err.pdf</code>: Estimated error rates for each possible transition. The black line shows the estimated error rates after convergence of the machine-learning algorithm. The red line shows the error rates expected under the nominal definition of the Q-score. The estimated error rates (black line) should be a good fit to the observed rates (points), and the error rates should drop with increased quality.  </li> <li><code>*_qual_stats.pdf</code>: Overall read quality profiles: heat map of the frequency of each quality score at each base position. The mean quality score at each position is shown by the green line, and the quartiles of the quality score distribution by the orange lines. The red line shows the scaled proportion of reads that extend to at least that position.  </li> <li><code>*_preprocessed_qual_stats.pdf</code>: Same as above, but after preprocessing.  </li> </ul> <p>We add an ASV length filter that will output <code>asv_length_filter/</code> with:  </p> <ul> <li><code>ASV_seqs.len.fasta</code>: Fasta file with filtered ASV sequences.  </li> <li><code>ASV_table.len.tsv</code>: Counts for each filtered ASV sequence.  </li> <li><code>ASV_len_orig.tsv</code>: ASV length distribution before filtering.  </li> <li><code>ASV_len_filt.tsv</code>: ASV length distribution after filtering.  </li> <li><code>stats.len.tsv</code>: Tracking read numbers through filtering, for each sample.  </li> </ul>"},{"location":"eDNA%2012S%20metab/01-Metabarcoding%20ampliseq%2012S%20Riaz/#step-5-blast-asv-sequences-output-from-dada2-against-our-3-databases","title":"Step 5: Blast ASV sequences (output from DADA2) against our 3 databases","text":"<p>Note that the GMGI-12S database will not apply to the MiFish primer sets so only Mitofish and NCBI are required. </p>"},{"location":"eDNA%2012S%20metab/01-Metabarcoding%20ampliseq%2012S%20Riaz/#populating-workgmgidatabases-folder","title":"Populating /work/gmgi/databases folder","text":"<p>We use NCBI, Mitofish, and GMGI-12S databases. </p>"},{"location":"eDNA%2012S%20metab/01-Metabarcoding%20ampliseq%2012S%20Riaz/#download-andor-update-nbci-blast-nt-database","title":"Download and/or update NBCI blast nt database","text":"<p>NCBI is updated daily and therefore needs to be updated each time a project is analyzed. This is the not the most ideal method but we were struggling to get the <code>-remote</code> flag to work within slurm because I don't think NU slurm is connected to the internet? NU help desk was helping for awhile but we didn't get anywhere.</p> <p>Within <code>/work/gmgi/databases/ncbi</code>, there is a <code>update_nt.sh</code> script with the following code. To run <code>sbatch update_nt.sh</code>. This won't take long as it will check for updates rather than re-downloading every time. </p> <pre><code>#!/bin/bash\n#SBATCH --partition=short\n#SBATCH --nodes=1\n#SBATCH --time=24:00:00\n#SBATCH --job-name=update_ncbi_nt\n#SBATCH --mem=50G\n#SBATCH --output=%x_%j.out\n#SBATCH --error=%x_%j.err\n\n# Activate conda environment\nsource /work/gmgi/miniconda3/bin/activate fisheries_eDNA\n\n# Create output directory if it doesn't exist\ncd /work/gmgi/databases/ncbi/nt\n\n# Update BLAST nt database\nupdate_blastdb.pl --decompress nt\n\n# Print completion message\necho \"BLAST nt database update completed\"\n</code></pre> <p>View the <code>update_ncbi_nt.out</code> file to confirm the echo printed at the end.</p>"},{"location":"eDNA%2012S%20metab/01-Metabarcoding%20ampliseq%2012S%20Riaz/#download-andor-update-mitofish-database","title":"Download and/or update Mitofish database","text":"<p>Check Mitofish webpage for the most recent database version number. Compare to the <code>work/gmgi/databases/12S</code> folder. If needed, update Mitofish database:</p> <pre><code>## download db \nwget https://mitofish.aori.u-tokyo.ac.jp/species/detail/download/?filename=download%2F/complete_partial_mitogenomes.zip  \n\n## unzip \nunzip 'index.html?filename=download%2F%2Fcomplete_partial_mitogenomes.zip'\n\n## clean headers \nawk '/^&gt;/ {print $1} !/^&gt;/ {print}' mito-all &gt; Mitofish_v4.05.fasta\n\n## remove excess files \nrm mito-all* \nrm index*\n\n## make NCBI db \n## make sure fisheries_eDNA conda environment is activated or module load ncbi-blast+/2.13.0\nmakeblastdb -in Mitofish_v4.02.fasta -dbtype nucl -out Mitofish_v4.02.fasta -parse_seqids\n</code></pre>"},{"location":"eDNA%2012S%20metab/01-Metabarcoding%20ampliseq%2012S%20Riaz/#download-gmgi-12s","title":"Download GMGI 12S","text":"<p>This is our in-house GMGI database that will include version numbers. Check <code>/work/gmgi/databases/12S/GMGI/</code> for current uploaded version number and check our Box folder for the most recent version number. </p> <p>On OOD portal, click the Interactive Apps dropdown. Select Home Directory under the HTML Viewer section. Navigate to the <code>/work/gmgi/databases/12S/GMGI/</code> folder. In the top right hand corner of the portal, select Upload and add the most recent .fasta file from our Box folder. </p> <p>To create a blast db from this reference fasta file (if updated): </p> <pre><code>cd /work/gmgi/databases/12S/GMGI/ \n\n## make NCBI db \n## make sure fisheries_eDNA conda environment is activated \n### CHANGE THE VERSION NUMBER BELOW TO LATEST\nmakeblastdb -in GMGI_Vert_Ref_2024v1.fasta -dbtype nucl -out GMGI_Vert_Ref_2024v1.fasta\n</code></pre>"},{"location":"eDNA%2012S%20metab/01-Metabarcoding%20ampliseq%2012S%20Riaz/#running-taxonomic-id-script","title":"Running taxonomic ID script","text":"<p><code>02-taxonomicID.sh</code>: </p> <pre><code>#!/bin/bash\n#SBATCH --error=output/\"%x_error.%j\" #if your job fails, the error report will be put in this file\n#SBATCH --output=output/\"%x_output.%j\" #once your job is completed, any final job report comments will be put in this file\n#SBATCH --partition=short\n#SBATCH --nodes=1\n#SBATCH --time=20:00:00\n#SBATCH --job-name=tax_ID\n#SBATCH --mem=30GB\n#SBATCH --ntasks=24\n#SBATCH --cpus-per-task=2\n\n### USER TO-DO ### \n## 1. Set paths for project; change db path if not 12S\n\n# Activate conda environment\nsource /work/gmgi/miniconda3/bin/activate fisheries_eDNA\n\n# SET PATHS \nASV_fasta=\"\"\nout=\"\"\n\ngmgi=\"/work/gmgi/databases/12S/GMGI\"\nmito=\"/work/gmgi/databases/12S/Mitofish\"\nncbi=\"/work/gmgi/databases/ncbi/nt\"\ntaxonkit=\"/work/gmgi/databases/taxonkit\"\n\n#### DATABASE QUERY ####\n### NCBI database \nblastn -db ${ncbi}/\"nt\" \\\n   -query ${ASV_fasta}/ASV_seqs.len.fasta \\\n   -out ${out}/BLASTResults_NCBI.txt \\\n   -max_target_seqs 10 -perc_identity 100 -qcov_hsp_perc 95 \\\n   -outfmt '6  qseqid   sseqid   sscinames   staxid pident   length   mismatch gapopen  qstart   qend  sstart   send  evalue   bitscore' \\\n   -verbose\n\n## Mitofish database \nblastn -db ${mito}/*.fasta \\\n   -query ${ASV_fasta}/ASV_seqs.len.fasta \\\n   -out ${out}/BLASTResults_Mito.txt \\\n   -max_target_seqs 10 -perc_identity 100 -qcov_hsp_perc 95 \\\n   -outfmt '6  qseqid   sseqid  pident   length   mismatch gapopen  qstart   qend  sstart   send  evalue   bitscore'\n\n## GMGI database \nblastn -db ${gmgi}/*.fasta \\\n   -query ${ASV_fasta}/ASV_seqs.len.fasta \\\n   -out ${out}/BLASTResults_GMGI.txt \\\n   -max_target_seqs 10 -perc_identity 98 -qcov_hsp_perc 95 \\\n   -outfmt '6  qseqid   sseqid   pident   length   mismatch gapopen  qstart   qend  sstart   send  evalue   bitscore'\n\n############################\n\n#### TAXONOMIC CLASSIFICATION #### \n## creating list of staxids from all three files \nawk -F $'\\t' '{ print $4}' ${out}/BLASTResults_NCBI.txt | sort -u &gt; ${out}/NCBI_sp.txt\n\n## annotating taxid with full taxonomic classification\ncat ${out}/NCBI_sp.txt | ${taxonkit}/taxonkit reformat -I 1 -r \"Unassigned\" &gt; ${out}/NCBI_taxassigned.txt\n</code></pre> <p>To run: - <code>sbatch 02-taxonomicID.sh</code> </p>"},{"location":"eDNA%2012S%20metab/02-datatable_prep_12S_Riaz/","title":"Datatable preparation base script for eDNA metabarcoding","text":"<p>.Rmd script</p> <p>This script takes your Blast output from the GMGI database, Mitofish database, and NCBI database to create one datatable with read counts and taxonomic assignment.</p> <p>Workflow summary: 1. Load libraries 2. Load metadata 3. Load BLAST output from GMGI, Mitofish, and NCBI 4. Load DADA2 ASV Table 5. Taxonomic Assignment - 5a. Identify ASVs with multiple hits from GMGI\u2019s database - 5b. Identify entries that mismatch between GMGI, Mitofish, and NCBI databases - 5c. Assign taxonomy based on hierarchical approach - 5d. Edit taxonomy annotations based on mismatch table choices - 5e. Adjusting common name and category for those entries that don\u2019t have one (from Mito or NCBI) 6. Filtering: Filter ASV by less than 0.1% reads and then collapse by group 7. Collapsing read counts by species name 8. Creating results output</p>"},{"location":"eDNA%2012S%20metab/02-datatable_prep_12S_Riaz/#load-libraries","title":"Load libraries","text":"<pre><code>library(ggplot2) ## for plotting\nlibrary(dplyr) ## for data table manipulation\n</code></pre> <pre><code>## \n## Attaching package: 'dplyr'\n\n## The following objects are masked from 'package:stats':\n## \n##     filter, lag\n\n## The following objects are masked from 'package:base':\n## \n##     intersect, setdiff, setequal, union\n</code></pre> <pre><code>library(tidyr) ## for data table manipulation\nlibrary(readr) ## for reading in tsv files\nlibrary(readxl) ## for reading in excel files\nlibrary(stringr) ## for data transformation\nlibrary(strex) ## for data transformation\nlibrary(writexl) ## for excel output\nlibrary(purrr) ## for data transformation\nlibrary(funrar) ## for make_relative()\nlibrary(tidyverse) ## for data table manipulation\n</code></pre> <pre><code>## \u2500\u2500 Attaching core tidyverse packages \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 tidyverse 2.0.0 \u2500\u2500\n## \u2714 forcats   1.0.0     \u2714 tibble    3.2.1\n## \u2714 lubridate 1.9.3\n\n## \u2500\u2500 Conflicts \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 tidyverse_conflicts() \u2500\u2500\n## \u2716 dplyr::filter() masks stats::filter()\n## \u2716 dplyr::lag()    masks stats::lag()\n## \u2139 Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n</code></pre>"},{"location":"eDNA%2012S%20metab/02-datatable_prep_12S_Riaz/#metadata-input","title":"Metadata input","text":""},{"location":"eDNA%2012S%20metab/02-datatable_prep_12S_Riaz/#identify-paths-for-metadata-and-project-data","title":"Identify paths for metadata and project data","text":"<p>Each user needs to write in their specific directory outputs prior to the file name. The default working directory is this document so the folder where this script is saved for the user. To change the workign directory to the Rproject directory, select \u2018Knit\u2019 and \u2018Knit Directory\u2019 &gt; \u2018Project Directory\u2019.</p> <pre><code>### User edits:\n### 1. change paths of input and output as desired \n\n## GMGI Fish database\npath_GMGIdb = \"C:/BoxDrive/Box/Science/Fisheries/Projects/eDNA/Metabarcoding Lab Resources/Reference Databases/GMGI_Vert_Ref.xlsx\"\npath_fishbase_tax = \"C:/BoxDrive/Box/Science/Fisheries/Projects/eDNA/Metabarcoding Lab Resources/Reference Databases/taxonomic_classification_fishbase.csv\"\npath_mitofish_tax = \"C:/BoxDrive/Box/Science/Fisheries/Projects/eDNA/Metabarcoding Lab Resources/Reference Databases/taxonomic_classification_mitofish.csv\"\n\n## BLAST results\npath_blast_gmgi = \"example_input/BLASTResults_GMGI.txt\"\npath_blast_mito = \"example_input/BLASTResults_Mito.txt\"\npath_blast_ncbi_taxassigned = \"example_input/NCBI_taxassigned.txt\"\npath_blast_ncbi = \"example_input/BLASTResults_NCBI.txt\"\n\n## ASV table results \n## confirm that the ASV_table.len.tsv name is correct for user's project\npath_asv_table = \"example_input/ASV_table.len.tsv\"\npath_output_summary = \"example_input/overall_summary.tsv\"\n\n# output paths \npath_choice_required = \"example_output/Taxonomic_assignments/Choice_required_GMGI_multiplehits.xlsx\"\npath_choice_required_edited=\"example_output/Taxonomic_assignments/Choice_required_GMGI_multiplehits_edited.xlsx\"\n\npath_disagree_list = \"example_output/Taxonomic_assignments/SampleReport_taxonomic_ID.xlsx\"\npath_disagree_list_edited=\"example_output/Taxonomic_assignments/SampleReport_taxonomic_ID_edited.xlsx\"\n\npath_commonnames_add=\"example_output/Taxonomic_assignments/CommonNames_required.xlsx\"\npath_commonnames_add_edited=\"example_output/Taxonomic_assignments/CommonNames_required_edited.xlsx\"\n\nresults_rawreads_matrix = \"example_output/Results_1_rawreads_matrix.xlsx\"\nresults_rawreads_long = \"example_output/Results_rawreads_long_format.xlsx\"\nresults_relab_matrix = \"example_output/Results_2_relative_abundance_matrix.xlsx\"\nresults_relab_long = \"example_output/Results_relative_abundance_long_format.xlsx\"\n\nreads_filtered_out=\"example_output/ASV_reads_filtered_out.xlsx\"\nASV_breakdown_sheet=\"example_output/ASV_breakdown.xlsx\"\n</code></pre>"},{"location":"eDNA%2012S%20metab/02-datatable_prep_12S_Riaz/#load-project-metadata","title":"Load project metadata","text":"<p>Metadata specific to each project. This contains information about each sample (e.g., month, site, time, sample type, etc.). Confirm that sample IDs match those used in the ASV_table.len.tsv file.</p> <pre><code>### User edits:\n### 1. change path of metadata file\n\n## EXCEL\nmeta &lt;- read_excel(\"example_input/metadata.xlsx\")\n## CSV \n# meta &lt;- read.csv(\"example_input/metadata.csv\", header = TRUE)\n</code></pre>"},{"location":"eDNA%2012S%20metab/02-datatable_prep_12S_Riaz/#load-database-metadata","title":"Load database metadata","text":"<p>No user edits in this section because paths have already been set above.</p> <pre><code># Load GMGI database information (common name, species name, etc.)\ngmgi_db &lt;- read_xlsx(path_GMGIdb, sheet = 1) %&gt;% dplyr::rename(sseqid = Ref) %&gt;%\n  ## removing &gt; from beginning of entires within Ref column\n  mutate(sseqid = gsub(\"&gt;\", \"\", sseqid))\n</code></pre>"},{"location":"eDNA%2012S%20metab/02-datatable_prep_12S_Riaz/#blast-data-input","title":"BLAST data input","text":"<p>No user edits unless user changed blastn parameters from fisheries team default.</p> <pre><code>## Setting column header names and classes\nblast_col_headers = c(\"ASV_ID\", \"sseqid\", \"pident\", \"length\", \"mismatch\", \"gapopen\",\n                                        \"qstart\", \"qend\", \"sstart\", \"send\", \"evalue\", \"bitscore\")\nblast_col_classes = c(rep(\"character\", 2), rep(\"numeric\", 10))\n</code></pre>"},{"location":"eDNA%2012S%20metab/02-datatable_prep_12S_Riaz/#gmgi-database","title":"GMGI database","text":"<p>No user edits.</p> <pre><code>Blast_GMGI &lt;- read.table(path_blast_gmgi, header=F, col.names = blast_col_headers, colClasses = blast_col_classes) %&gt;%\n  ## blast changes spaces to hyphons so we need to change that back to match our metadata\n  mutate(sseqid = gsub(\"-\", \" \", sseqid)) %&gt;%\n  ## join with GMGI database information\n  left_join(., gmgi_db, by = \"sseqid\")\n\n## Check how many ASVs were identified with the GMGI Database\nlength(unique(Blast_GMGI$ASV_ID)) \n</code></pre> <pre><code>## [1] 986\n</code></pre>"},{"location":"eDNA%2012S%20metab/02-datatable_prep_12S_Riaz/#mitofish-database","title":"Mitofish database","text":"<p>No user edits.</p> <pre><code>Blast_Mito &lt;- read.table(path_blast_mito, header=F, col.names = blast_col_headers, colClasses = blast_col_classes) %&gt;%\n  # renaming sseqid to species name\n  dplyr::rename(Species_name = sseqid) %&gt;%\n\n  # replacing _ with spaces\n  mutate(Species_name = gsub(\"_\", \" \", Species_name))\n</code></pre>"},{"location":"eDNA%2012S%20metab/02-datatable_prep_12S_Riaz/#ncbi-database","title":"NCBI database","text":"<p>No user edits.</p> <pre><code>NCBI_taxassigned &lt;- read.delim2(path_blast_ncbi_taxassigned, header=F, col.names = c(\"staxid\", \"Phylo\")) %&gt;%\n  ## creating taxonomic assignment columns\n  separate(Phylo, c(\"Kingdom\", \"Phylum\", \"Class\", \"Order\", \"Family\", \"Genus\", \"Species_name\"), sep = \";\") %&gt;%\n  ## creating species column based on Species_name\n  mutate(., species = str_after_nth(Species_name, \" \", 1))\n\nBlast_NCBI &lt;- read.table(path_blast_ncbi, header=F,\n                           col.names = c(\"ASV_ID\", \"sseqid\", \"sscinames\", \"staxid\", \"pident\", \"length\", \"mismatch\",\n                                         \"gapopen\", \"qstart\", \"qend\", \"sstart\", \"send\", \"evalue\", \"bitscore\"),\n                           colClasses = c(rep(\"character\", 3), \"integer\", rep(\"numeric\", 9))) %&gt;%\n  left_join(., NCBI_taxassigned, by = \"staxid\")\n</code></pre>"},{"location":"eDNA%2012S%20metab/02-datatable_prep_12S_Riaz/#load-dada2-asv-table","title":"Load DADA2 ASV Table","text":"<p>The column headers will be the Sample IDs and the first column is the ASV ID. ASVs are given a \u201crank\u201d based on sum of reads from that ASV (pre-filtering). \u2018Random\u2019 indicates that if ASVs are tied, then the code will randomly assign a rank for those tied. Because we don\u2019t need an exact rank here, \u2018random\u2019 will do for a tie-breaker.</p> <p>No user edits.</p> <pre><code>ASV_table &lt;- read_tsv(path_asv_table, show_col_types = FALSE) %&gt;%\n  ## calculate the sum of all reads for each ASV\n  mutate(., ASV_sum = rowSums(across(where(is.numeric)))) %&gt;% \n\n  ## calculate a ranking based on those sum calculated above\n  mutate(ASV_rank = rank(-ASV_sum, ties.method='random')) %&gt;%\n\n  ## move the sum and rank columns to after ASV_ID and arrange by rank\n  relocate(c(ASV_sum,ASV_rank), .after = ASV_ID) %&gt;% arrange((ASV_rank))\n\n## creating list of rankings\nASV_rank_list &lt;- ASV_table %&gt;% dplyr::select(ASV_ID, ASV_sum, ASV_rank)\n</code></pre>"},{"location":"eDNA%2012S%20metab/02-datatable_prep_12S_Riaz/#taxonomic-assignment","title":"Taxonomic Assignment","text":"<p>Identifying where NCBI, Mito, and GMGI disagree on tax assignment. With the hierarchial approach, ASVs that match to GMGI and several other databases will only result in GMGI assignment. By reviewing this df, we can be sure we aren\u2019t missing an assignment in our GMGI curated database.</p> <p>Sub-workflow: 1. Identify any ASVs that contain multiple hits within the GMGI database. 2. Identify entries that mismatch between GMGI, Mitofish, and NCBI databases. 3. Assign taxonomy based on hierarchical approach. 4. Edit taxonomy annotations based on mismatch table. 5. Adjusting common name for those entries that don\u2019t have one (from Mito or GMGI).</p>"},{"location":"eDNA%2012S%20metab/02-datatable_prep_12S_Riaz/#identify-any-asvs-that-contain-multiple-hits-within-the-gmgi-database","title":"Identify any ASVs that contain multiple hits within the GMGI database","text":"<p>At this point, a fisheries team member needs to make choices about which taxonomic assignment to accept.</p>"},{"location":"eDNA%2012S%20metab/02-datatable_prep_12S_Riaz/#create-list-of-those-asvs-with-multiple-hits","title":"Create list of those ASVs with multiple hits","text":"<p>No user edits.</p> <pre><code>multiple_hit_choice &lt;- Blast_GMGI %&gt;% group_by(ASV_ID) %&gt;%\n  ## take top percent identity hit, count the number of top hits, and filter to those with more than 1 top hit \n  slice_max(pident, n=1) %&gt;% count() %&gt;% filter(n&gt;1) %&gt;%\n\n  ## adding BLAST_GMGI information with these ASVs and ASV rank and sum\n  left_join(., Blast_GMGI, by = \"ASV_ID\") %&gt;%\n  left_join(., ASV_rank_list, by = \"ASV_ID\") %&gt;%\n\n  ## moving database percent ID to be next to Blast percent ID\n  relocate(c(db_percent_ID, ASV_sum, ASV_rank), .after = pident) %&gt;%\n\n  ## adding choice column for next steps \n  mutate(Choice = NA)\n\n## export this data frame as excel sheet \nmultiple_hit_choice %&gt;% write_xlsx(path_choice_required)\n</code></pre> <p>Based on the output above, user needs to make some choices. In the excel spreadsheet, user needs to mark \u2018x\u2019 on the choices desired while leaving the other entries blank.</p>"},{"location":"eDNA%2012S%20metab/02-datatable_prep_12S_Riaz/#choosing-one-of-several-hits","title":"Choosing one of several hits.","text":"<p>Load choice edited dataset. No user edits.</p> <pre><code>multiple_hit_choice_edited &lt;- read_xlsx(path_choice_required_edited) %&gt;%\n  ## selecting the choices made\n  filter(!is.na(Choice)) %&gt;%\n  ## selecting only columns needed \n  dplyr::select(ASV_ID, sseqid, Choice)\n</code></pre> <p>A for loop will filter Blast_GMGI df based on these choices. No user edits.</p> <pre><code># Create a new edited df\nBlast_GMGI_edited &lt;- Blast_GMGI \n\n# Loop through each row of the dataframe\nfor (i in multiple_hit_choice_edited$ASV_ID) {\n  # Extract the current row (will do this for each ASV_ID in the choice df)\n  current_row &lt;- multiple_hit_choice_edited %&gt;% subset(ASV_ID==i)\n\n  # Apply filter based on the current row's condition\n  Blast_GMGI_edited &lt;- Blast_GMGI_edited %&gt;%\n    filter(case_when(ASV_ID == current_row$ASV_ID ~ sseqid == current_row$sseqid,\n           TRUE ~ TRUE))\n}\n</code></pre>"},{"location":"eDNA%2012S%20metab/02-datatable_prep_12S_Riaz/#confirming-that-all-entries-have-been-dealth-with","title":"Confirming that all entries have been dealth with","text":"<p>No user edits.</p> <pre><code>### Check the below output to confirm the filtering steps above worked (if it worked, it won't be in output)\nBlast_GMGI_edited %&gt;% group_by(ASV_ID) %&gt;% slice_max(pident, n=1) %&gt;% count() %&gt;% filter(n&gt;1)\n</code></pre> <pre><code>## # A tibble: 0 \u00d7 2\n## # Groups:   ASV_ID [0]\n## # \u2139 2 variables: ASV_ID &lt;chr&gt;, n &lt;int&gt;\n</code></pre>"},{"location":"eDNA%2012S%20metab/02-datatable_prep_12S_Riaz/#identify-entries-that-mismatch-between-gmgi-mitofish-and-ncbi-databases","title":"Identify entries that mismatch between GMGI, Mitofish, and NCBI databases","text":"<p>Creating a df called \u201cDisagree\u201d. Review the output before moving onto the next section.</p> <p>No user edits.</p> <pre><code>Disagree &lt;- Blast_GMGI_edited %&gt;% group_by(ASV_ID) %&gt;% \n  dplyr::rename(., GMGI_db_ID = db_percent_ID, GMGI_pident = pident) %&gt;%\n  ## Creating new columns with species name based on pident information\n  mutate(\n    GMGI_100 = if_else(GMGI_pident == 100, Species_name, NA_character_),\n    GMGI_lessthan100 = if_else(GMGI_pident &lt; 100, Species_name, NA_character_)) %&gt;%\n\n  ## taking only the top hit per ASV ID\n  slice_max(GMGI_pident, n = 1, with_ties = FALSE) %&gt;% ungroup() %&gt;%\n\n  ## filtering to distinct rows with selected columns\n  distinct(ASV_ID, GMGI_db_ID, GMGI_pident, GMGI_100, GMGI_lessthan100) %&gt;%\n\n  ## adding Mitofish and editing the Blast_Mito df in the process\n  full_join(Blast_Mito %&gt;% dplyr::select(ASV_ID, Species_name) %&gt;%\n              dplyr::rename(Mitofish = Species_name) %&gt;%\n              distinct() %&gt;% group_by(ASV_ID) %&gt;%\n              mutate(Mitofish = paste0(Mitofish, collapse = \";\")),\n            by = \"ASV_ID\") %&gt;%\n\n  ## adding NCBI and editing the Blast_NCBI df in the process\n  full_join(Blast_NCBI %&gt;% dplyr::select(ASV_ID, Species_name) %&gt;%\n              dplyr::rename(NCBI = Species_name) %&gt;%\n              distinct() %&gt;% group_by(ASV_ID) %&gt;%\n              mutate(NCBI = paste0(NCBI, collapse = \";\")),\n            by = \"ASV_ID\") %&gt;%\n\n  ## adding ASV rank and sum information\n  left_join(., ASV_rank_list, by = \"ASV_ID\") %&gt;%\n\n  ## filtering out duplicate rows\n  distinct() %&gt;%\n\n  ## filtering to those entries that mismatch between GMGI, Mitofish, and NCBI\n  filter((GMGI_100 != GMGI_lessthan100 | GMGI_100 != Mitofish | GMGI_100 != NCBI | is.na(GMGI_100))) %&gt;%\n\n  ## adding choice column for next steps \n  mutate(Choice = NA)\n</code></pre> <pre><code>## Warning in full_join(., Blast_NCBI %&gt;% dplyr::select(ASV_ID, Species_name) %&gt;% : Detected an unexpected many-to-many relationship between `x` and `y`.\n## \u2139 Row 36 of `x` matches multiple rows in `y`.\n## \u2139 Row 201 of `y` matches multiple rows in `x`.\n## \u2139 If a many-to-many relationship is expected, set `relationship =\n##   \"many-to-many\"` to silence this warning.\n</code></pre> <pre><code>## export this data frame as excel sheet \nDisagree %&gt;% write_xlsx(path_disagree_list)\n</code></pre>"},{"location":"eDNA%2012S%20metab/02-datatable_prep_12S_Riaz/#assign-taxonomy-based-on-hierarchical-approach","title":"Assign taxonomy based on hierarchical approach","text":"<p>Taxonomic identification is taken from GMGI 100%, then GMGI \\&lt;100%, then Mitofish 100%, and finally NCBI 100%.</p> <p>No user edits.</p> <pre><code>ASV_table_taxID &lt;- ASV_table %&gt;% \n\n  ## 1. Top hit from GMGI's database\n  left_join(Blast_GMGI_edited %&gt;%  group_by(ASV_ID) %&gt;%\n              slice_max(pident, n = 1) %&gt;%\n                            dplyr::select(ASV_ID, Species_name),\n            by = join_by(ASV_ID)) %&gt;%\n\n  ## 2. Mitofish database\n  ### join df, select ASV_ID and Species_name columns, rename Species_name to Mito, call only distinct rows\n  left_join(., Blast_Mito %&gt;% dplyr::select(ASV_ID, Species_name) %&gt;% dplyr::rename(Mito = Species_name) %&gt;% distinct() %&gt;%\n\n              ### group by ASV_ID, and collapse all species names separated by ;, then take only distinct rows\n              group_by(ASV_ID) %&gt;% mutate(Mito = paste0(Mito, collapse = \";\")) %&gt;% distinct(), by = \"ASV_ID\") %&gt;%\n\n  ### if GMGI annotation is NA, then replace with Mitofish \n  mutate(., Species_name = ifelse(is.na(Species_name), Mito, Species_name)) %&gt;%\n\n  ## 3. NCBI database; same functions as above\n  left_join(., Blast_NCBI %&gt;% dplyr::select(ASV_ID, Species_name) %&gt;% dplyr::rename(NCBI = Species_name) %&gt;% distinct() %&gt;%\n              group_by(ASV_ID) %&gt;% mutate(NCBI = paste0(NCBI, collapse = \";\")) %&gt;% distinct(), by = \"ASV_ID\") %&gt;%\n  mutate(., Species_name = ifelse(is.na(Species_name), NCBI, Species_name)) %&gt;%\n\n  ## 4. if Species name is STILL not filled, call it \"Unassigned\"\n  mutate(., Species_name = ifelse(is.na(Species_name), \"Unassigned\", Species_name)) %&gt;%  \n\n  ## removing Mito spp and NCBI spp\n  dplyr::select(-Mito, -NCBI) %&gt;%\n\n  ## move species name to be after ASV_ID\n  relocate(., c(Species_name), .after = ASV_ID)\n</code></pre>"},{"location":"eDNA%2012S%20metab/02-datatable_prep_12S_Riaz/#edit-taxonomy-annotations-based-on-mismatch-table","title":"Edit taxonomy annotations based on mismatch table","text":"<p>Override any annotations with edited taxonomic identification table. No user edits.</p> <pre><code>## read in edited df \ntaxonomic_choice &lt;- read_xlsx(path_disagree_list_edited) %&gt;%\n    ## selecting only columns needed \n  dplyr::select(ASV_ID, Choice)  \n\n# Create a new edited df\nASV_table_taxID_edited &lt;- ASV_table_taxID \n\n# Loop through each row of the dataframe\nfor (i in taxonomic_choice$ASV_ID) {\n  # Extract the current row (will do this for each ASV_ID in the choice df)\n  current_row &lt;- taxonomic_choice %&gt;% subset(ASV_ID==i)\n\n  # Apply filter based on the current row's condition\n  ASV_table_taxID_edited &lt;- ASV_table_taxID_edited %&gt;%\n    mutate(Species_name = case_when(\n          ASV_ID == current_row$ASV_ID ~ current_row$Choice,\n           TRUE ~ Species_name))\n}\n</code></pre>"},{"location":"eDNA%2012S%20metab/02-datatable_prep_12S_Riaz/#confirm-all-entries-are-dealt-with","title":"Confirm all entries are dealt with","text":"<p>No user edits.</p> <pre><code>## Output will be blank\nASV_table_taxID_edited %&gt;% dplyr::select(Species_name) %&gt;% distinct() %&gt;% \n  filter(., grepl(\";\", Species_name)) %&gt;% arrange(Species_name) \n</code></pre> <pre><code>## # A tibble: 0 \u00d7 1\n## # \u2139 1 variable: Species_name &lt;chr&gt;\n</code></pre>"},{"location":"eDNA%2012S%20metab/02-datatable_prep_12S_Riaz/#adjusting-common-name-for-those-entries-that-dont-have-one-from-mito-or-ncbi","title":"Adjusting common name for those entries that don\u2019t have one (from Mito or NCBI)","text":"<p>No user edits.</p> <pre><code>### add common name column to df\nASV_table_taxID_edited &lt;- ASV_table_taxID_edited %&gt;%\n  left_join(., gmgi_db %&gt;% dplyr::select(Species_name, Common_name, Category) %&gt;% distinct(), by = \"Species_name\") %&gt;%\n  relocate(., c(Common_name, Category), .after = Species_name)\n\n### print entries with no common name\nASV_table_taxID_edited %&gt;% dplyr::select(Species_name, Common_name) %&gt;% \n  filter(is.na(Common_name)) %&gt;% distinct() %&gt;%\n  mutate(Category = NA, Kingdom = NA, Phylum = NA, Class = NA, Order = NA, Family = NA, Genus = NA, species = NA) %&gt;%\n  mutate(\n    across(everything(), ~case_when(\n      Species_name == \"Unassigned\" ~ \"Unassigned\",\n      TRUE ~ .x\n    ))) %&gt;% \n  write_xlsx(path_commonnames_add)\n</code></pre> <p>Editing common names and category when needed.</p> <pre><code>## read in edited df \ncommonNames_annotated &lt;- read_xlsx(path_commonnames_add_edited)\n\n# Create a new edited df\nASV_table_taxID_annotated &lt;- ASV_table_taxID_edited \n\n# Loop through each row of the dataframe\nfor (i in commonNames_annotated$Species_name) {\n  # Extract the current row (will do this for each ASV_ID in the choice df)\n  current_row &lt;- commonNames_annotated %&gt;% subset(Species_name==i)\n\n  # Apply filter based on the current row's condition\n  ASV_table_taxID_annotated &lt;- ASV_table_taxID_annotated %&gt;%\n    mutate(Common_name = case_when(\n          Species_name == current_row$Species_name ~ current_row$Common_name,\n           TRUE ~ Common_name)) %&gt;%\n    mutate(Category = case_when(\n          Species_name == current_row$Species_name ~ current_row$Category,\n           TRUE ~ Category))  \n}\n\n## printing list of species name without common names \n## after additions to mutate function above, this output should be zero \nASV_table_taxID_annotated %&gt;% dplyr::select(Species_name, Common_name) %&gt;% filter(is.na(Common_name)) %&gt;% distinct()\n</code></pre> <pre><code>## # A tibble: 0 \u00d7 2\n## # \u2139 2 variables: Species_name &lt;chr&gt;, Common_name &lt;chr&gt;\n</code></pre>"},{"location":"eDNA%2012S%20metab/02-datatable_prep_12S_Riaz/#filtering-filter-asv-by-less-than-01-reads-and-then-collapse-by-group","title":"Filtering: Filter ASV by less than 0.1% reads and then collapse by group","text":""},{"location":"eDNA%2012S%20metab/02-datatable_prep_12S_Riaz/#filter-out-reads-that-are-less-than-01-of-asv-row-total-per-sample","title":"Filter out reads that are less than 0.1% of ASV (row) total per sample.","text":"<p>Create an output of what you\u2019re losing with filtering.</p> <p>No user edits.</p> <pre><code>ASV_table_taxID_filtered &lt;- ASV_table_taxID_annotated %&gt;%\n  ## telling the df we are doing the following function by rows (ASVs)\n  rowwise() %&gt;%\n\n  ## filtering out any values that are less than 0.001 of the total ASV read # in each sample\n  mutate(across(.cols = (7:ncol(.)),            \n                .fns = ~ ifelse((.x/ASV_sum)&lt;0.001, NA, .x))) %&gt;% ungroup()\n\n## output of what we're losing\nASV_table_taxID_edited %&gt;% rowwise() %&gt;%\n  mutate(across(.cols = (7:ncol(.)),            \n                .fns = ~ ifelse((.x/ASV_sum)&gt;0.001, NA, .x))) %&gt;% ungroup() %&gt;% write_xlsx(reads_filtered_out)\n\n## Export ASV break-down for 03-data_quality.Rmd\nASV_table_taxID_filtered %&gt;% dplyr::select(ASV_ID, Species_name, Common_name, Category, ASV_sum, ASV_rank) %&gt;%\n  write_xlsx(ASV_breakdown_sheet)\n</code></pre>"},{"location":"eDNA%2012S%20metab/02-datatable_prep_12S_Riaz/#collapsing-read-counts-by-species-name","title":"Collapsing read counts by species name","text":"<p>No user edits.</p> <pre><code>ASV_table_taxID_collapsed &lt;- ASV_table_taxID_filtered %&gt;% \n  # removing original ASV_ID to collapse\n  dplyr::select(-ASV_ID) %&gt;%  \n\n  ## group by Species_name and sample\n  dplyr::group_by(Species_name, Common_name, Category) %&gt;%\n\n  ## sum down column by species name and sample to collapse\n  summarise(across(6:last_col(), ~ sum(., na.rm = TRUE))) %&gt;% ungroup()\n</code></pre> <pre><code>## `summarise()` has grouped output by 'Species_name', 'Common_name'. You can\n## override using the `.groups` argument.\n</code></pre>"},{"location":"eDNA%2012S%20metab/02-datatable_prep_12S_Riaz/#creating-results-output","title":"Creating results output","text":"<p>Raw reads results output. No user edits.</p> <pre><code>## Raw reads matrix (wide format)\nASV_table_taxID_collapsed %&gt;% write_xlsx(results_rawreads_matrix)\n\n## Raw reads long format and filtering out entries with zero reads\nASV_table_taxID_collapsed %&gt;% \n  gather(\"sampleID\", \"reads\", c(4:last_col())) %&gt;%\n  filter(reads &gt; 0) %&gt;% \n  left_join(., meta, by = \"sampleID\") %&gt;%\n  write_xlsx(results_rawreads_long)\n</code></pre> <p>Relative Abundance No user edits.</p> <pre><code>### Calculating relative abundance\ndf_relab &lt;- ASV_table_taxID_collapsed %&gt;% \n  gather(\"sampleID\", \"reads\", 4:last_col()) %&gt;%\n  group_by(sampleID) %&gt;%\n\n  ### total\n  mutate(sample_total = sum(reads)) %&gt;%\n  group_by(sampleID, Species_name) %&gt;%\n\n  ## relab\n  mutate(relab = reads/sample_total) %&gt;% ungroup() %&gt;%\n  select(-reads, -sample_total) \n\ndf_relab %&gt;%\n  left_join(., meta, by = \"sampleID\") %&gt;%\n  write_xlsx(results_relab_long)\n\ndf_relab %&gt;% spread(sampleID, relab) %&gt;% write_xlsx(results_relab_matrix)\n</code></pre>"},{"location":"eDNA%2012S%20metab/03-data_quality/","title":"Metabarcoding data quality: eDNA metabarcoding base script","text":"<p>.Rmd script</p> <p>This script evaluates your sequence quality and taxonomic assignment quality. Figures produced in this script can go into supplemental data for a manuscript.</p>"},{"location":"eDNA%2012S%20metab/03-data_quality/#load-libraries","title":"Load libraries","text":"<pre><code>library(dplyr) # for data transformation\n</code></pre> <pre><code>## \n## Attaching package: 'dplyr'\n\n## The following objects are masked from 'package:stats':\n## \n##     filter, lag\n\n## The following objects are masked from 'package:base':\n## \n##     intersect, setdiff, setequal, union\n</code></pre> <pre><code>library(tidyverse) # for data transformation\n</code></pre> <pre><code>## \u2500\u2500 Attaching core tidyverse packages \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 tidyverse 2.0.0 \u2500\u2500\n## \u2714 forcats   1.0.0     \u2714 readr     2.1.5\n## \u2714 ggplot2   3.5.1     \u2714 stringr   1.5.1\n## \u2714 lubridate 1.9.3     \u2714 tibble    3.2.1\n## \u2714 purrr     1.0.2     \u2714 tidyr     1.3.1\n\n## \u2500\u2500 Conflicts \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 tidyverse_conflicts() \u2500\u2500\n## \u2716 dplyr::filter() masks stats::filter()\n## \u2716 dplyr::lag()    masks stats::lag()\n## \u2139 Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n</code></pre> <pre><code>library(ggplot2) # for plotting\nlibrary(readxl) ## for reading in excel files\nlibrary(viridis)\n</code></pre> <pre><code>## Loading required package: viridisLite\n</code></pre> <pre><code>library(hrbrthemes)\nlibrary(ggrepel)\nlibrary(cowplot)\n</code></pre> <pre><code>## \n## Attaching package: 'cowplot'\n## \n## The following object is masked from 'package:lubridate':\n## \n##     stamp\n</code></pre> <pre><code># removing scientific notation\n## remove line or comment out if not desired \noptions(scipen=999)\n</code></pre>"},{"location":"eDNA%2012S%20metab/03-data_quality/#load-data","title":"Load data","text":"<pre><code>### User edits:\n### 1. Replace the 3 paths below: edit example_input to your project specific path \n### 2. Confirm your sampleIDs match between metadata, results df, and filtering stats output\n\nfiltering_stats &lt;- read_tsv(\"example_input/overall_summary.tsv\", show_col_types = FALSE) %&gt;% dplyr::rename(sampleID = sample)\n\nmeta &lt;- read_excel(\"example_input/metadata.xlsx\")\n\nresults &lt;- read_xlsx(\"example_output/Results_rawreads_long_format.xlsx\") %&gt;%\n  ## calculate sum of reads \n  group_by(sampleID) %&gt;%\n  mutate(`Number of reads` = sum(reads)) %&gt;%\n\n  ## calculate relative abundance\n  group_by(sampleID, Species_name) %&gt;%\n  mutate(`Relative Abundance` = reads/`Number of reads`) %&gt;%\n\n  ## factor the Category list \n  mutate(Category = factor(Category, levels = c(\"Human\", \"Livestock\", \"Other\", \"Unassigned\", \"Bird\",\n                                                \"Sea Turtle\", \"Elasmobranch\", \"Marine Mammal\", \"Teleost Fish\")))\n\nASV_breakdown &lt;- read_xlsx(\"example_output/ASV_breakdown.xlsx\") %&gt;%\n  ## factor the Category list \n  mutate(Category = factor(Category, levels = c(\"Human\", \"Livestock\", \"Other\", \"Unassigned\", \"Bird\",\n                                                \"Sea Turtle\", \"Elasmobranch\", \"Marine Mammal\", \"Teleost Fish\")))\n</code></pre>"},{"location":"eDNA%2012S%20metab/03-data_quality/#sequence-data","title":"Sequence data","text":""},{"location":"eDNA%2012S%20metab/03-data_quality/#data-transformation","title":"Data Transformation","text":"<p>No user edits.</p> <pre><code>df &lt;- full_join(filtering_stats, meta, by = \"sampleID\") %&gt;%\n  # filtering out columns we don't need \n  dplyr::select(-cutadapt_reverse_complemented) %&gt;%\n\n  # removing percentage icon from cutadapt_passing_filters_percent\n  mutate(cutadapt_passing_filters_percent = gsub(\"%\", \"\", cutadapt_passing_filters_percent)) %&gt;%\n\n  # confirming that all columns of interest are numerical \n  mutate_at(vars(2:10), as.numeric) %&gt;%\n\n  # data transformation so all columns of interest are together \n  gather(\"measure\", \"value\", 2:10)  \n</code></pre>"},{"location":"eDNA%2012S%20metab/03-data_quality/#plotting","title":"Plotting","text":"<p>Suggested webpage to choose colors: https://coolors.co/</p> <pre><code>### User edits:\n### 1. Change paths of output to desired folder (data/figures is suggested data structure)\n### 2. Change x axis and color, size based on metadata desired \n### 3. Change custom colors and sizes if desired and number of colors and sizes based on metadata variable chosen \n\ndf %&gt;% \n\n  filter(!is.na(Month)) %&gt;% \n  ## USER EDITS IN LINE BELOW \n  ggplot(., aes(x=Month, y=value)) + \n\n  ## adding points in jitter format \n  geom_jitter(width=0.15, alpha=0.5, fill=\"#0077b6\", color='black', size=1, shape=21) + \n\n  ## option for additional boxplots if desired (uncomment to add)\n  #geom_boxplot() +\n\n  ## using facet_wrap to create grid based on variables and factor() to order them in custom format\n  facet_wrap(~factor(measure, levels=c('cutadapt_total_processed', 'cutadapt_passing_filters', \n                                       'cutadapt_passing_filters_percent', 'DADA2_input',\n                                 'filtered', 'denoisedF', 'denoisedR', 'merged', 'nonchim')), scales = \"free\") +\n\n  ## graph asthetics \n  theme_bw() +\n  ylab(\"Number of reads\") + \n\n  theme(panel.background=element_rect(fill='white', colour='black'),\n        strip.background=element_rect(fill='white', colour='black'),\n        strip.text = element_text(size = 10, face=\"bold\"),\n        legend.position = \"right\",\n        axis.text.y = element_text(size=7, color=\"grey30\"),\n        axis.text.x = element_text(angle=45, hjust=1),\n        axis.title.y = element_text(margin = margin(t = 0, r = 10, b = 0, l = 0), size=11, face=\"bold\"),\n        axis.title.x = element_text(margin = margin(t = 10, r = 0, b = 0, l = 0), size=11, face=\"bold\"))\n</code></pre> <pre><code>## Warning: Removed 45 rows containing missing values or values outside the scale range\n## (`geom_point()`).\n</code></pre> <p></p> <pre><code>ggsave(\"example_output/Figures/SampleReport_FilteringStats.png\", width = 11, height=9)\n</code></pre> <pre><code>## Warning: Removed 45 rows containing missing values or values outside the scale range\n## (`geom_point()`).\n</code></pre> <p>Condensed plot used in contract reporting.</p> <pre><code>### User edits:\n### 1. Change paths of output to desired folder (data/figures is suggested data structure)\n### 2. Change x axis and color, size based on metadata desired \n### 3. Change custom colors and sizes if desired and number of colors and sizes based on metadata variable chosen \n\ndf %&gt;% \n  subset(measure == \"cutadapt_total_processed\" | measure == \"nonchim\") %&gt;%\n\n  ## USER EDITS IN LINE BELOW \n  ggplot(., aes(x=measure, y=value)) + \n\n  ## adding points in jitter format \n  geom_boxplot(outlier.shape = NA, fill=NA, aes(color = measure)) +\n  geom_jitter(width=0.15, shape=21, alpha=0.25, size=1.5, color = 'black', aes(fill = measure)) + \n\n  ## graph asthetics \n  theme_bw() +\n  labs(\n    y=\"Number of reads\",\n    x=\"Bioinformatic Step\"\n    ) + \n\n  ## USER EDITS IN MANUAL CODE BELOW \n  # scale_color_manual(values = c(\"red3\", \"lightblue\", \"purple2\", \"gold\", \"green4\", \"black\")) +\n  # scale_size_manual(values = c(21,17)) +\n  scale_fill_manual(values = c(\"#264653\", \"#2a9d8f\")) +\n  scale_color_manual(values = c(\"#264653\", \"#2a9d8f\")) +\n  scale_x_discrete(labels = c(\"cutadapt_total_processed\" = \"Start\", \n                              \"nonchim\" = \"Final\")) +\n\n  theme(panel.background=element_rect(fill='white', colour='black'),\n        strip.background=element_rect(fill='white', colour='black'),\n        strip.text = element_text(size = 10, face=\"bold\"),\n        legend.position = \"none\",\n        axis.text.y = element_text(size=10, color=\"black\"),\n        axis.text.x = element_text(size=10, color=\"black\"),\n        axis.title.y = element_text(margin = margin(t = 0, r = 10, b = 0, l = 0), size=11, face=\"bold\"),\n        axis.title.x = element_text(margin = margin(t = 10, r = 0, b = 0, l = 0), size=11, face=\"bold\"))\n</code></pre> <pre><code>## Warning: Removed 10 rows containing non-finite outside the scale range\n## (`stat_boxplot()`).\n\n## Warning: Removed 10 rows containing missing values or values outside the scale range\n## (`geom_point()`).\n</code></pre> <p></p> <pre><code>ggsave(\"example_output/Figures/SampleReport_FilteringStats_Condensed.png\", width = 4, height=4)\n</code></pre> <pre><code>## Warning: Removed 10 rows containing non-finite outside the scale range\n## (`stat_boxplot()`).\n## Removed 10 rows containing missing values or values outside the scale range\n## (`geom_point()`).\n</code></pre>"},{"location":"eDNA%2012S%20metab/03-data_quality/#plot-taxonomy","title":"Plot taxonomy","text":""},{"location":"eDNA%2012S%20metab/03-data_quality/#data-transformation_1","title":"Data transformation","text":"<p>No user edits.</p> <pre><code>results_summary &lt;- results %&gt;% \n  group_by(Category) %&gt;%\n  reframe(sum_reads = sum(reads))\n\ngeneral_stats &lt;- results %&gt;% \n  group_by(Category) %&gt;%\n  reframe(sum_reads = sum(reads)) %&gt;% \n  mutate(total = sum(sum_reads),\n         percent = sum_reads/total*100) %&gt;% dplyr::select(Category, percent) %&gt;% distinct() %&gt;%\n  ## round to 2 decimal places \n  mutate(across(c('percent'), round, 3))\n</code></pre> <pre><code>## Warning: There was 1 warning in `mutate()`.\n## \u2139 In argument: `across(c(\"percent\"), round, 3)`.\n## Caused by warning:\n## ! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\n## Supply arguments directly to `.fns` through an anonymous function instead.\n## \n##   # Previously\n##   across(a:b, mean, na.rm = TRUE)\n## \n##   # Now\n##   across(a:b, \\(x) mean(x, na.rm = TRUE))\n</code></pre> <pre><code>ASV_summary &lt;- ASV_breakdown %&gt;%\n  group_by(Category) %&gt;%\n  reframe(count = n_distinct(ASV_ID))\n\nspecies_summary &lt;- results %&gt;%\n  group_by(Category) %&gt;%\n  reframe(count = n_distinct(Species_name))\n</code></pre>"},{"location":"eDNA%2012S%20metab/03-data_quality/#relative-abundance-by-category","title":"Relative abundance by category","text":"<pre><code>fill_colors &lt;- c(\"Human\" = \"#e76f51\", \n                 \"Livestock\" = \"#FF740A\", \n                 \"Other\" = \"#FE9E20\", \n                 \"Unassigned\" = \"#FFC571\",\n                 \"Bird\" = \"#FFECC2\",\n                 \"Sea Turtle\" = \"#C8D2B1\", \n                 \"Elasmobranch\" = \"#DCF1F9\",\n                 \"Marine Mammal\" =\"#8CD0EC\",\n                 \"Teleost Fish\" = \"#3FB1DE\"\n                 )\n\nresults %&gt;% group_by(sampleID, Category) %&gt;%\n  reframe(group_sum = sum(reads),\n         group_relab = group_sum/`Number of reads`\n           ) %&gt;% distinct() %&gt;%\n\nggplot(., aes(y=group_relab, x=Category)) + \n  geom_boxplot(outlier.shape = NA, aes(color=Category), fill=NA) +\n  geom_jitter(aes(fill=Category), width=0.2, shape=21, color='black', size = 0.75, alpha=0.35) +\n    scale_color_manual(values = fill_colors) +\n    scale_fill_manual(values = fill_colors) +\n    labs(color = \"Category\") +\n    scale_x_discrete(labels = scales::label_wrap(10)) +  \n    theme_bw() + \n    xlab(\"Category\") + ylab(\"Relative abundance\") +\n    theme(panel.background=element_rect(fill='white', colour='black'),\n          legend.position = \"none\",\n        axis.text.y = element_text(size=7, color=\"grey30\"),\n        axis.text.x = element_text(size=7), # angle=45, hjust=1\n        axis.title.y = element_text(margin = margin(t = 0, r = 10, b = 0, l = 0), size=11, face=\"bold\"),\n        axis.title.x = element_text(margin = margin(t = 10, r = 0, b = 0, l = 0), size=11, face=\"bold\"))\n</code></pre> <pre><code>ggsave(\"example_output/Figures/Categories_relative_abundance.png\", width = 6.5, height = 4)\n</code></pre>"},{"location":"eDNA%2012S%20metab/03-data_quality/#human-specific-reads-by-sample-type","title":"Human specific reads by Sample Type","text":"<pre><code>results %&gt;% group_by(sampleID, Category) %&gt;%\n  reframe(group_sum = sum(reads),\n         group_relab = group_sum/`Number of reads`\n           ) %&gt;% distinct() %&gt;%\n\n  ## subset to include Human and metadata \n  subset(Category == \"Human\") %&gt;%\n  left_join(., meta, by = \"sampleID\") %&gt;%\n\n  ggplot(., aes(x=SampleType, y=group_relab)) +\n  geom_boxplot(outlier.shape=NA, fill=NA, color = \"#e76f51\") +\n  geom_jitter(fill=\"#e76f51\", shape=21, alpha=0.5, width=0.2) +\n  labs(\n    x = \"Sample Type\",\n    y = \"Relative Abundance\"\n  ) +\n  theme_bw() +\n  theme(panel.background=element_rect(fill='white', colour='black'),\n          legend.position = \"none\",\n        axis.text.y = element_text(size=7, color=\"grey30\"),\n        axis.text.x = element_text(size=7),\n        axis.title.y = element_text(margin = margin(t = 0, r = 10, b = 0, l = 0), size=11, face=\"bold\"),\n        axis.title.x = element_text(margin = margin(t = 10, r = 0, b = 0, l = 0), size=11, face=\"bold\"))\n</code></pre> <pre><code>ggsave(\"example_output/Figures/Human.png\", width=4, height=3)\n</code></pre>"},{"location":"eDNA%2012S%20metab/03-data_quality/#piechart","title":"Piechart","text":"<p>Reads Piechart. Used in contract report.</p> <pre><code>### User edits:\n### 1. Change paths of output to desired folder (data/figures is suggested data structure)\n### 2. Change scale brewer color if desired \n\npiechart &lt;- general_stats %&gt;%  \n  mutate(csum = rev(cumsum(rev(percent))), \n         pos = percent/2 + lead(csum, 1),\n         pos = if_else(is.na(pos), percent/2, pos))\n\nunassigned_percent &lt;- piechart %&gt;% \n  filter(Category == \"Unassigned\") %&gt;% \n  pull(percent)\n\npiechart_reads &lt;- general_stats %&gt;% \n  ggplot(., aes(x=\"\", y = percent, fill = Category)) +\n  geom_col(color = \"black\", width=1.25) +\n  geom_label_repel(data = piechart,\n                   aes(y = pos, label = paste0(percent, \"%\")),\n                   size = 3, nudge_x = 1, show.legend = FALSE) +\n  coord_polar(theta = \"y\") +\n  scale_fill_manual(values = fill_colors) +\n  theme_bw() +\n  # labs(\n  #   x = NULL, y = NULL, fill = \"Category\", \n  #      caption = paste0(\"Unassigned reads = \", unassigned_percent, \"%\")\n  # ) +\n  theme(\n    plot.background = element_rect(fill = \"white\", colour = NA),\n    plot.margin = margin(t = 5, r = 5, b = 5, l = 5, unit = \"pt\"),\n    legend.position = \"none\",\n    panel.border = element_blank(),  # Remove panel border\n    panel.grid = element_blank(),    # Remove grid lines\n    axis.ticks = element_blank(),    # Remove axis ticks\n    axis.text = element_blank(),     # Remove axis text\n    axis.title = element_blank()     # Remove axis titles\n      ) +\n  ggtitle(\"Raw reads (%)\") +\n  xlab(\"\") + ylab(\"\") + labs(fill = \"Category\"); piechart_reads\n</code></pre> <p></p> <pre><code>ggsave(\"example_output/Figures/Category_breakdown_percent_rawreads.png\", width=4, height=3)\n</code></pre> <p>ASV Piechart (NOT used in contract report)</p> <pre><code>### User edits:\n### 1. Change paths of output to desired folder (data/figures is suggested data structure)\n### 2. Change scale brewer color if desired \n\npiechart_ASV &lt;- ASV_summary %&gt;%  \n  mutate(csum = rev(cumsum(rev(count))), \n         pos = count/2 + lead(csum, 1),\n         pos = if_else(is.na(pos), count/2, pos))\n\npiechart_ASV &lt;- ASV_summary %&gt;% \n  ggplot(., aes(x=\"\", y = count, fill = Category)) +\n  geom_col(color = \"black\", width=1.25) +\n  geom_label_repel(data = piechart_ASV,\n                   aes(y = pos, label = paste0(count)),\n                   size = 3, nudge_x = 1, show.legend = FALSE) +\n  coord_polar(theta = \"y\") +\n  scale_fill_manual(values = fill_colors) +\n  theme_bw() +\n  theme(\n    plot.background = element_rect(fill = \"white\", colour = NA),\n    plot.margin = margin(t = 5, r = 5, b = 5, l = 5, unit = \"pt\"),\n    panel.border = element_blank(),  # Remove panel border\n    panel.grid = element_blank(),    # Remove grid lines\n    axis.ticks = element_blank(),    # Remove axis ticks\n    axis.text = element_blank(),     # Remove axis text\n    #legend.position = \"none\"\n    axis.title = element_blank()     # Remove axis titles\n      ) +\n  ggtitle(\"Number of ASVs\") +\n  xlab(\"\") + ylab(\"\") + labs(fill = \"Category\"); piechart_ASV\n</code></pre> <p></p> <pre><code>ggsave(\"example_output/Figures/Category_breakdown_ASVs.png\", width=4, height=3)\n</code></pre> <p>Number of species pie chart. Used in contract report.</p> <pre><code>piechart_spp &lt;- species_summary %&gt;%  \n  mutate(csum = rev(cumsum(rev(count))), \n         pos = count/2 + lead(csum, 1),\n         pos = if_else(is.na(pos), count/2, pos))\n\npiechart_species_plot &lt;- species_summary %&gt;% \n  ggplot(., aes(x=\"\", y = count, fill = Category)) +\n  geom_col(color = \"black\", width=1.25) +\n  geom_label_repel(data = piechart_spp,\n                   aes(y = pos, label = paste0(count)),\n                   size = 3, nudge_x = 1, show.legend = FALSE) +\n  coord_polar(theta = \"y\") +\n  scale_fill_manual(values = fill_colors) +\n  theme_bw() +\n  theme(\n    plot.background = element_rect(fill = \"white\", colour = NA),\n    plot.margin = margin(t = 5, r = 5, b = 5, l = 5, unit = \"pt\"),\n    panel.border = element_blank(),  # Remove panel border\n    panel.grid = element_blank(),    # Remove grid lines\n    axis.ticks = element_blank(),    # Remove axis ticks\n    axis.text = element_blank(),     # Remove axis text\n    axis.title = element_blank()     # Remove axis titles\n  ) +\n  ggtitle(\"Number of species\") +\n  xlab(\"\") + ylab(\"\") + labs(fill = \"Category\"); piechart_species_plot\n</code></pre> <p></p> <pre><code>ggsave(\"example_output/Figures/Category_breakdown_species.png\", width=4, height=3)\n</code></pre> <p>Plot together and export</p> <pre><code>plot_grid(piechart_reads, piechart_species_plot,\n          ncol=2,\n          align = \"vh\"\n          )\n</code></pre> <p></p> <pre><code>ggsave(\"example_output/Figures/Category_breakdown_contract.png\", width=14, height=4)\n</code></pre>"},{"location":"eDNA%2012S%20metab/04-relative_abundance_heatmap/","title":"Relative Abundance Heatmaps: eDNA metabarcoding base script","text":"<p>.Rmd script</p> <p>This script plots your relative abundance matrix as a heatmap. Figures produced are potentially part of the main figures of your manuscript/report.</p>"},{"location":"eDNA%2012S%20metab/04-relative_abundance_heatmap/#load-libraries","title":"Load libraries","text":"<pre><code>library(ggplot2) ## for plotting\nlibrary(dplyr) ## for data table manipulation\n</code></pre> <pre><code>## \n## Attaching package: 'dplyr'\n\n## The following objects are masked from 'package:stats':\n## \n##     filter, lag\n\n## The following objects are masked from 'package:base':\n## \n##     intersect, setdiff, setequal, union\n</code></pre> <pre><code>library(tidyr) ## for data table manipulation\nlibrary(readxl) ## for reading in excel files\nlibrary(stringr) ## for data transformation\nlibrary(strex) ## for data transformation\nlibrary(purrr) ## for data transformation\nlibrary(funrar) ## for make_relative()\nlibrary(tidyverse) ## for data transformation\n</code></pre> <pre><code>## \u2500\u2500 Attaching core tidyverse packages \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 tidyverse 2.0.0 \u2500\u2500\n## \u2714 forcats   1.0.0     \u2714 readr     2.1.5\n## \u2714 lubridate 1.9.3     \u2714 tibble    3.2.1\n\n## \u2500\u2500 Conflicts \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 tidyverse_conflicts() \u2500\u2500\n## \u2716 dplyr::filter() masks stats::filter()\n## \u2716 dplyr::lag()    masks stats::lag()\n## \u2139 Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n</code></pre> <pre><code>library(naniar) ## replace_with_na_all function\nlibrary(ggh4x) ## for facet wrap options\n</code></pre> <pre><code>## \n## Attaching package: 'ggh4x'\n## \n## The following object is masked from 'package:ggplot2':\n## \n##     guide_axis_logticks\n</code></pre> <pre><code>library(tidytext)\nlibrary(forcats)\nlibrary(scales)\n</code></pre> <pre><code>## \n## Attaching package: 'scales'\n## \n## The following object is masked from 'package:readr':\n## \n##     col_factor\n## \n## The following object is masked from 'package:purrr':\n## \n##     discard\n</code></pre>"},{"location":"eDNA%2012S%20metab/04-relative_abundance_heatmap/#load-data","title":"Load data","text":"<pre><code>df &lt;- read_xlsx(\"example_output/Results_relab_long.xlsx\") %&gt;%\n  mutate(across(c(relab), ~ round(.x, 5)))\n\ntaxlevels &lt;- read_excel(\n  \"C:/BoxDrive/Box/Science/Fisheries/Projects/eDNA/Metabarcoding Lab Resources/Reference Databases/GMGI_Vert_Ref.xlsx\") %&gt;% \n  dplyr::select(\"Species_name\", \"Kingdom\", \"Phylum\", \"Class\", \"Order\", \"Family\", \"Genus\", \"species\") %&gt;%\n  distinct()\n\ndf_annotated &lt;- df %&gt;% left_join(., taxlevels, by = \"Species_name\")\n\n## bringing in common names information for those not in our db\ncommonNames_annotated &lt;- read_xlsx(\"example_output/Taxonomic_assignments/CommonNames_required_edited.xlsx\")\n\n# Loop through each row of the dataframe to add taxonomic level information from required edited worksheet \nfor (i in commonNames_annotated$Species_name) {\n  # Extract the current row (will do this for each ASV_ID in the choice df)\n  current_row &lt;- commonNames_annotated %&gt;% subset(Species_name==i)\n\n  # Apply filter based on the current row's condition\ndf_annotated &lt;- df_annotated %&gt;%\n  mutate(across(c(Common_name, Category, Kingdom, Phylum, Class, Order, Family, Genus, species),\n                ~case_when(Species_name == current_row$Species_name ~ current_row[[cur_column()]],\n                           TRUE ~ .x)))\n}\n\n## tax list \ndf_tax &lt;- df_annotated %&gt;% dplyr::select(Species_name, Kingdom:Genus) %&gt;% distinct()\n</code></pre>"},{"location":"eDNA%2012S%20metab/04-relative_abundance_heatmap/#remove-categories","title":"Remove Categories","text":"<p>If you want to plot relative abundance without human, other, or livestock categories. As FYI/warning, relative abundance is calculated with these categories included. Relative abundance can also be thought of as proportion of total reads, which is calculated from the total reads for that sample.</p> <pre><code>df_filtered &lt;- df_annotated %&gt;% \n  filter(!Category == \"Other\" &amp; !Category == \"Livestock\" &amp; !Category == \"Unassigned\" &amp; !Category == \"Human\") \n\ndf_average &lt;- df_annotated %&gt;%\n  group_by(SampleType, Species_name) %&gt;%\n  ### average relative abundance by sample Type\n  mutate(avg_relab = mean(relab, na.rm=TRUE))\n</code></pre> <p>If targeted species heatmap is desired, replace df_annotated with df_filtered in the heatmap code below. If so, remember to change the file name exported.</p>"},{"location":"eDNA%2012S%20metab/04-relative_abundance_heatmap/#plot","title":"Plot","text":"<p>reverse label order: scale y discrete limits reverse limits=rev</p> <p>https://coolors.co/ (hit tools on the top right hand side)</p> <pre><code>## if subset of categories is desired, replace df below with df_filtered\ndf_annotated %&gt;%\n\n  ## replace zeros with NAs for plotting\n  replace_with_na_all(condition = ~.x == 0.00000) %&gt;%\n\n  # Create a factor for Common_name ordered by Order within each Category\n  group_by(Category) %&gt;%\n  mutate(Common_name = factor(Common_name, levels = unique(Common_name[order(Order, desc(Common_name))]))) %&gt;%\n  ungroup() %&gt;%\n\n  ## ggplot basic options (USER EDIT: X AND Y AXIS)\n  ggplot(., aes(x = sampleID, y = Common_name)) +\n  geom_tile(aes(fill = relab), color = \"black\") +\n\n  ## x, y, and legend labels (USER EDITS IF DESIRED)\n  ylab(\"Common name\") +\n  xlab(\"Site\") +\n  labs(fill = \"Relative Abundance (%)\") +\n\n  ## color of the tile options; direction=1 will flip the low/high (USER EDITS IF DESIRED)\n  scale_fill_gradient(na.value = \"white\", low = \"lightskyblue2\", high = \"#0C4D66\") + \n\n  ## facet grid with Category and project variables\n  facet_grid2(Category ~ SampleType, \n              scales = \"free\", space = \"free\", \n              labeller = labeller(Category = label_wrap_gen(width = 10))) +\n\n  ## graph theme options\n  theme_classic() +\n  theme(\n    ## axis text \n    axis.text.x = element_text(angle = 90, size=6, color=\"grey25\", hjust = 1),\n    axis.text.y = element_text(colour = 'black', size = 8),\n\n    ## legend text and title \n    legend.text = element_text(size = 8, color=\"black\"),\n    legend.title = element_text(margin = margin(t = 0, r = 0, b = 5, l = 0), size=10, color=\"black\", face=\"bold\"),\n    legend.position = c(-0.4, -0.05), \n    legend.key.height = unit(5, 'mm'),\n    legend.direction = \"horizontal\",\n    legend.key.width = unit(5, 'mm'),\n    legend.title.align = 0.5,\n    legend.title.position = \"top\",\n\n    ## axis titles \n    axis.title.y = element_text(margin = margin(t = 0, r = 10, b = 0, l = 0), size=14, face=\"bold\"),\n    axis.title.x = element_text(margin = margin(t = 10, r = 0, b = 0, l = 0), size=14, face=\"bold\"),\n\n    ## facet wrap labels\n    strip.text.x = element_text(color = \"black\", face = \"bold\", size = 12),\n    strip.text.y = element_text(color = \"black\", face = \"bold\", size = 12, angle=0),\n    strip.background.y = element_blank(),\n    strip.clip = \"off\"\n    )\n</code></pre> <pre><code>## Warning: The `legend.title.align` argument of `theme()` is deprecated as of ggplot2\n## 3.5.0.\n## \u2139 Please use theme(legend.title = element_text(hjust)) instead.\n## This warning is displayed once every 8 hours.\n## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was\n## generated.\n\n## Warning: A numeric `legend.position` argument in `theme()` was deprecated in ggplot2\n## 3.5.0.\n## \u2139 Please use the `legend.position.inside` argument of `theme()` instead.\n## This warning is displayed once every 8 hours.\n## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was\n## generated.\n</code></pre> <p></p> <pre><code>## USER EDITS WIDTH AND HEIGHT TO DESIRED   \nggsave(\"example_output/Figures/Relative_abundance.png\", width = 22, height = 14)  \n</code></pre> <p>Heatmap plot by sample type</p> <pre><code>df_average %&gt;%\n  dplyr::select(Species_name, Common_name, Category, SampleType, Kingdom:avg_relab) %&gt;%\n\n  ## replace zeros with NAs for plotting\n  replace_with_na_all(condition = ~.x == 0.00000) %&gt;%\n\n  # Create a factor for Common_name ordered by Order within each Category\n  group_by(Category) %&gt;%\n  mutate(Common_name = factor(Common_name, levels = unique(Common_name[order(Order, desc(Common_name))]))) %&gt;%\n  ungroup() %&gt;%\n\n    ## ggplot basic options (USER EDIT: X AND Y AXIS)\n  ggplot(., aes(x = SampleType, y = Common_name)) +\n  geom_tile(aes(fill = avg_relab), color = \"black\") +\n\n  ## x, y, and legend labels (USER EDITS IF DESIRED)\n  ylab(\"Common name\") +\n  xlab(\"\") +\n  labs(fill = \"Relative Abundance (%)\") +\n\n  ## color of the tile options; direction=1 will flip the low/high (USER EDITS IF DESIRED)\n  scale_fill_gradient(na.value = \"white\", low = \"lightskyblue2\", high = \"#0C4D66\") + \n\n  ## facet grid with Category and project variables\n  facet_grid2(Category ~ SampleType, \n              scales = \"free\", space = \"free\", \n              labeller = labeller(Category = label_wrap_gen(width = 10))) +\n\n  ## graph theme options\n  theme_classic() +\n  theme(\n    ## axis text \n    axis.text.x = element_text(angle = 90, size=6, color=\"grey25\", hjust = 1),\n    axis.text.y = element_text(colour = 'black', size = 8),\n\n    ## legend text and title \n    legend.text = element_text(size = 8, color=\"black\"),\n    legend.title = element_text(margin = margin(t = 0, r = 0, b = 5, l = 0), size=10, color=\"black\", face=\"bold\"),\n    legend.position = c(-0.4, -0.05), \n    legend.key.height = unit(5, 'mm'),\n    legend.direction = \"horizontal\",\n    legend.key.width = unit(5, 'mm'),\n    legend.title.align = 0.5,\n    legend.title.position = \"top\",\n\n    ## axis titles \n    axis.title.y = element_text(margin = margin(t = 0, r = 10, b = 0, l = 0), size=14, face=\"bold\"),\n    axis.title.x = element_text(margin = margin(t = 10, r = 0, b = 0, l = 0), size=14, face=\"bold\"),\n\n    ## facet wrap labels\n    strip.text.x = element_text(color = \"black\", face = \"bold\", size = 12),\n    strip.text.y = element_text(color = \"black\", face = \"bold\", size = 12, angle=0),\n    strip.background.y = element_blank(),\n    strip.clip = \"off\"\n    )\n</code></pre> <p></p> <pre><code>ggsave(\"example_output/Figures/Relative_abundance_sampletype.png\", width = 7, height = 14) \n</code></pre>"},{"location":"eDNA%2012S%20metab/04-relative_abundance_heatmap/#top-species-list-visual","title":"Top Species List visual","text":"<pre><code>top_list &lt;- read_xlsx(\"example_output/Results_rawreads_long.xlsx\") %&gt;%\n  filter(!Category == \"Other\" &amp; !Category == \"Livestock\" &amp; !Category == \"unassigned\" &amp; !Category == \"Human\") %&gt;%\n  group_by(Species_name, Common_name) %&gt;%\n  summarise(total = sum(reads),\n            log = log10(total),\n            total_M = total/1000000) %&gt;% \n  arrange(desc(total)) %&gt;%\n  head(30) \n</code></pre> <pre><code>## `summarise()` has grouped output by 'Species_name'. You can override using the\n## `.groups` argument.\n</code></pre> <pre><code>ggplot(top_list, aes(x = fct_reorder(Common_name, log), y = log)) +\n  geom_segment(aes(xend = Common_name, yend = 0), color = \"#97C1DE\") +  # Lollipop stick\n  geom_point(size = 3, shape=21, color='grey30', fill = \"#97C1DE\") +  # Lollipop head\n  coord_flip() +  # Flip coordinates for horizontal lollipop chart\n  labs(\n       x = \"\",\n       y = \"Normalized Reads\") +\n  theme_bw() +\n  theme(axis.text.y = element_text(size = 8, color='black'), #, face=\"italic\"\n        axis.text.x = element_text(size = 6),\n        axis.title.y = element_text(margin = margin(t = 0, r = 10, b = 0, l = 0), size=10, face=\"bold\"),\n        axis.title.x = element_text(margin = margin(t = 10, r = 0, b = 0, l = 0), size=10, face=\"bold\"),\n        axis.text.x.top = element_text(size = 8, color='black', face=\"italic\", angle = 45, hjust = 0)) +\n  scale_y_continuous(\n    labels = comma,\n    limits = c(0, max(top_list$log) + (max(top_list$log)*0.1))  # Set the upper limit to max value + 10%\n  )\n</code></pre> <pre><code>ggsave(\"example_output/Figures/Top_species_log.png\", width=3.5, height=6)\n</code></pre>"},{"location":"eDNA%2012S%20metab/04-relative_abundance_heatmap/#bubble-plot","title":"Bubble plot","text":"<pre><code>raw_df &lt;- read_xlsx(\"example_output/Results_rawreads_long.xlsx\") %&gt;%\n  group_by(Species_name, Common_name, Category) %&gt;%\n  reframe(sum = sum(reads)/1000000,\n          xaxis = \"x\") %&gt;%\n  left_join(., df_tax, by = \"Species_name\") %&gt;%\n  mutate(Target_group = case_when(\n    Category == \"Human\" ~ \"Nontarget\",\n    Category == \"Other\" ~ \"Nontarget\",\n    Category == \"Unassigned\" ~ \"Nontarget\",\n    Category == \"Livestock\" ~ \"Nontarget\",\n    Category == \"Bird\" ~ \"Target1\",\n    Category == \"Elasmobranch\" ~ \"Target1\",\n    Category == \"Marine Mammal\" ~ \"Target1\",\n    Category == \"Sea Turtle\" ~ \"Target1\",\n    Category == \"Teleost Fish\" ~ \"Target2\"\n  )) %&gt;% filter(!Target_group == \"Nontarget\") \n\n\nraw_df %&gt;% \n  filter(!Target_group == \"Nontarget\") %&gt;%\n  # Create a factor for Common_name ordered by Order within each Category\n  group_by(Category) %&gt;%\n  mutate(Common_name = factor(Common_name, levels = unique(Common_name[order(Order, desc(Common_name))]))) %&gt;%\n  ungroup() %&gt;%\n\n  ggplot(., aes(x=xaxis, y=Common_name)) + \n\n  geom_point(aes(size=sum, fill=sum), color = 'black', shape=21) +\n  scale_fill_gradient(na.value = \"white\", low = \"lightskyblue2\", high = \"#0C4D66\") +\n\n  facet_grid2(Category ~ ., scales = \"free\", space = \"free\") +\n\n  theme_bw() +\n  labs(\n    x=\"\",\n    y=\"\",\n    fill = \"Reads (M)\",\n    size = \"Reads (M)\"\n  ) +\n\n  theme(\n    axis.text.y = element_text(size = 8, color = 'black'),\n    axis.text.x = element_blank(),\n    axis.title.y = element_text(margin = margin(t = 0, r = 10, b = 0, l = 0), size=12, face=\"bold\"),\n    axis.title.x = element_text(margin = margin(t = 10, r = 0, b = 0, l = 0), size=12, face=\"bold\"),\n    ## facet wrap labels\n    strip.text.x = element_text(color = \"black\", face = \"bold\", size = 12),\n    #strip.text.y = element_text(color = \"black\", face = \"bold\", size = 12, angle=0),\n    strip.text.y = element_blank(),\n    strip.background.y = element_blank(),\n    strip.clip = \"off\",\n    # Combine legends\n    legend.position = \"right\",\n    legend.box = \"vertical\"\n  ) +\n  guides(\n    fill = \"none\",\n    size = guide_legend(order = 2, reverse = TRUE,\n                        override.aes = list(fill = scales::seq_gradient_pal(\"#0C4D66\", \"lightskyblue2\")\n                                            (seq(0, 1, length.out = 5))))\n  )\n</code></pre> <pre><code>ggsave(\"example_output/Figures/Species_bubbleplot.png\", width=4.5, height=16)\n</code></pre> <p>Setting up plot for loop</p> <pre><code># # Get unique categories\n# categories &lt;- unique(raw_df$Category)\n#   \n# # Set the fixed size range\n# size_range &lt;- c(0, max(raw_df$sum))\n# \n# # Create color palette\n# color_palette &lt;- scales::seq_gradient_pal(\"#0C4D66\", \"lightskyblue2\")(seq(0, 1, length.out = 6))\n# \n# # Create a list to store the plots\n# plot_list &lt;- list()\n# \n# # Define custom widths and heights for each category\n# dimensions &lt;- data.frame(\n#   Category = categories,\n#   Width = c(4, 4, 4, 4, 4),  # Example widths for each category\n#   Height = c(7, 7, 7, 7, 7)   # Example heights for each category\n# )\n# \n# aspect_ratio &lt;- 2\n</code></pre> <p>For loop to create each plot</p> <pre><code># # Loop through each category\n# for (cat in categories) {\n#   \n#   raw_df_filtered &lt;- raw_df %&gt;% \n#     filter(Category == cat) %&gt;% #!Target_group == \"Nontarget\", \n#     # Create a factor for Common_name ordered by Order within each Category\n#     mutate(Common_name = factor(Common_name, levels = unique(Common_name[order(Order, desc(Common_name))])))\n# \n#   plot &lt;- raw_df_filtered %&gt;%\n#     ggplot(aes(x=xaxis, y=Common_name)) + \n#     geom_point(aes(size=sum, fill=sum), color = 'black', shape=21) +\n#     scale_size_continuous(range = c(1, 10), limits = size_range) +\n#     scale_fill_gradient(na.value = \"white\", low = \"lightskyblue2\", high = \"#0C4D66\", limits = size_range) +\n#     theme_bw() +\n#     labs(\n#       title = cat,\n#       x=\"\",\n#       y=\"\",\n#       fill = \"Reads (M)\",\n#       size = \"Reads (M)\"\n#     ) +\n#     theme(\n#       axis.text.y = element_text(size = 8, color = 'black'),\n#       axis.text.x = element_blank(),\n#       axis.title.y = element_text(margin = margin(t = 0, r = 10, b = 0, l = 0), size=12, face=\"bold\"),\n#       axis.title.x = element_text(margin = margin(t = 10, r = 0, b = 0, l = 0), size=12, face=\"bold\"),\n#       plot.title = element_text(hjust = 0.5, face = \"bold\", size = 14),\n#       # Combine legends\n#       legend.position = \"right\",\n#       legend.box = \"vertical\"\n#     ) +\n#     guides(\n#       fill = \"none\",\n#       size = guide_legend(order = 2, reverse = TRUE,\n#                           override.aes = list(fill = color_palette))\n#     ) #+ coord_fixed(ratio=aspect_ratio)\n#   \n#   # Store the plot in the list\n#   plot_list[[cat]] &lt;- plot\n#   \n# # Get the corresponding width and height\n#     width &lt;- dimensions$Width[dimensions$Category == cat]\n#     height &lt;- dimensions$Height[dimensions$Category == cat]\n# \n#   # Save the plot with specified dimensions\n#   ggsave(filename = paste0(\"example_output/Figures/Bubbleplot/Bubbleplot_\", cat, \".png\"), \n#          plot = plot, \n#          width = width, \n#          height = height,\n#          units = \"in\", \n#          dpi = 300)\n# }\n</code></pre>"},{"location":"eDNA%2012S%20metab/05-biodiversity/","title":"Phyloseq Diversity Metrics: eDNA metabarcoding base script","text":"<p>This script analyzes your relative abundance matrix to assess alpha and beta diversity. Figures produced are potentially part of the main figures of your manuscript/report.</p>"},{"location":"eDNA%2012S%20metab/05-biodiversity/#load-libraries","title":"Load libraries","text":"<pre><code>library(ggplot2) ## for plotting\nlibrary(tidyverse) ## for data manipulation \n</code></pre> <pre><code>## \u2500\u2500 Attaching core tidyverse packages \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 tidyverse 2.0.0 \u2500\u2500\n## \u2714 dplyr     1.1.4     \u2714 readr     2.1.5\n## \u2714 forcats   1.0.0     \u2714 stringr   1.5.1\n## \u2714 lubridate 1.9.3     \u2714 tibble    3.2.1\n## \u2714 purrr     1.0.2     \u2714 tidyr     1.3.1\n## \u2500\u2500 Conflicts \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 tidyverse_conflicts() \u2500\u2500\n## \u2716 dplyr::filter() masks stats::filter()\n## \u2716 dplyr::lag()    masks stats::lag()\n## \u2139 Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n</code></pre> <pre><code>library(phyloseq)\nlibrary(knitr)\nlibrary(readxl)\nlibrary(writexl)\nlibrary(cowplot)\n</code></pre> <pre><code>## \n## Attaching package: 'cowplot'\n## \n## The following object is masked from 'package:lubridate':\n## \n##     stamp\n</code></pre> <pre><code>## for stats\nlibrary(pairwiseAdonis)\n</code></pre> <pre><code>## Loading required package: vegan\n## Loading required package: permute\n## Loading required package: lattice\n## This is vegan 2.6-8\n## Loading required package: cluster\n</code></pre> <pre><code>library(lme4) ## for stats\n</code></pre> <pre><code>## Loading required package: Matrix\n## \n## Attaching package: 'Matrix'\n## \n## The following objects are masked from 'package:tidyr':\n## \n##     expand, pack, unpack\n</code></pre> <pre><code>library(car) ## for stats\n</code></pre> <pre><code>## Loading required package: carData\n## \n## Attaching package: 'car'\n## \n## The following object is masked from 'package:dplyr':\n## \n##     recode\n## \n## The following object is masked from 'package:purrr':\n## \n##     some\n</code></pre> <pre><code>library(stats) ## for stats\nlibrary(vegan)\nlibrary(\"microbiome\") ## for alpha diversity functions\n</code></pre> <pre><code>## \n## microbiome R package (microbiome.github.com)\n##     \n## \n## \n##  Copyright (C) 2011-2022 Leo Lahti, \n##     Sudarshan Shetty et al. &lt;microbiome.github.io&gt;\n## \n## \n## Attaching package: 'microbiome'\n## \n## The following object is masked from 'package:vegan':\n## \n##     diversity\n## \n## The following object is masked from 'package:ggplot2':\n## \n##     alpha\n## \n## The following object is masked from 'package:base':\n## \n##     transform\n</code></pre> <pre><code>## set seed\nset.seed(1234)\n</code></pre>"},{"location":"eDNA%2012S%20metab/05-biodiversity/#load-data","title":"Load data","text":"<p>Before continuing with analyses, decide on what data you\u2019re going to use an input. Relative abundance, raw reads, rarefied counts (can be done in phyloseq), variance stabilizing transformation (vst) from DESeq2? The input will impact your interpretation and thus is important to decide before conducting any stats to avoid bias towards a particular result.</p> <p>On the Fisheries team, we have traditionally used relative abundance and the following code uses that dataset.</p>"},{"location":"eDNA%2012S%20metab/05-biodiversity/#relative-abundance-data","title":"Relative Abundance data","text":"<pre><code># Option 1: Change global options (affects all subsequent operations)\n## Convert scientific notation to regular numbers\noptions(scipen = 999)\n\n## Relative abundance matrix \ndf &lt;- read_xlsx(\"example_output/Results_relab_matrix.xlsx\") %&gt;%\n  ## removing common_name and category for now \n  dplyr::select(-Common_name, -Category) %&gt;%\n\n  ## Remove columns with NA values\n  dplyr::select(where(~!any(is.na(.)))) %&gt;%\n\n  ## making species_name rownames instead of column \n  column_to_rownames(var = \"Species_name\") %&gt;%\n\n  # remove columns that sum to 0\n  select(where(~sum(., na.rm = TRUE) != 0))\n</code></pre>"},{"location":"eDNA%2012S%20metab/05-biodiversity/#metadata","title":"Metadata","text":"<pre><code>meta &lt;- read_xlsx(\"example_input/metadata.xlsx\") %&gt;%\n\n  ## rownames are also needed in phyloseq meta table\n  mutate(sampleID2=sampleID) %&gt;% column_to_rownames(var = \"sampleID2\")\n</code></pre>"},{"location":"eDNA%2012S%20metab/05-biodiversity/#create-phyloseq-object","title":"Create phyloseq object","text":"<pre><code>## Create ASV (OTU) table and meta table \notu &lt;- otu_table(df, taxa_are_rows = T)\nmeta_phyloseq &lt;- sample_data(meta)\n\n## Merge metadata and OTU table into one phyloseq \"object\"\nphylo_obj &lt;- merge_phyloseq(otu, meta_phyloseq)\n\n## view phyloseq obj \n## expected output = otu_table() with taxa and sample numbers and sample_data() with the sample and column numbers\nprint(phylo_obj)\n</code></pre> <pre><code>## phyloseq-class experiment-level object\n## otu_table()   OTU Table:         [ 107 taxa and 325 samples ]\n## sample_data() Sample Data:       [ 325 samples by 10 sample variables ]\n</code></pre> <pre><code># Ensure that your OTU table doesn't contain any NA or negative values (output should be FALSE)\nany(is.na(otu_table(phylo_obj)))\n</code></pre> <pre><code>## [1] FALSE\n</code></pre> <pre><code>any(otu_table(phylo_obj) &lt; 0)\n</code></pre> <pre><code>## [1] FALSE\n</code></pre>"},{"location":"eDNA%2012S%20metab/05-biodiversity/#subsetting-phyloseq-object","title":"Subsetting phyloseq object","text":"<p>Optional if you\u2019d like to break your data up by a certain variable (surface water, bottom water). If desired, uncomment the code chunk below.</p> <pre><code># surface &lt;- subset_samples(phylo_obj, SampleType == \"Surface Water\")\n# bottom &lt;- subset_samples(phylo_obj, SampleType == \"Bottom Water\") \n</code></pre>"},{"location":"eDNA%2012S%20metab/05-biodiversity/#alpha-diversity","title":"Alpha Diversity","text":"<p>Comparing the species diversity (shannon index or species richness) at each site.</p>"},{"location":"eDNA%2012S%20metab/05-biodiversity/#calculating-shannon-index-and-species-richness","title":"Calculating Shannon Index and Species Richness","text":"<p>The alpha function in the microbiome package calculates several alpha diversity indices. The most relevant are likely observed (species richness) and diversity_shannon (shannon index).</p> <p>https://microbiome.github.io/tutorials/Alphadiversity.html</p> <p>Or use plot_richness function from: https://rstudio-pubs-static.s3.amazonaws.com/1071936_6115f873acbc4dc4a30b1380cc3885fb.html</p> <pre><code>fill_col = c(\"cyan4\", \"deeppink4\", \"#ed6a5a\")\n\n## Calculate\nalpha_div &lt;- estimate_richness(phylo_obj, measures = c(\"Shannon\", \"Simpson\")) %&gt;%\n  rownames_to_column(var = \"sampleID\") %&gt;% left_join(., meta, by = \"sampleID\") \n\nalpha_div %&gt;%\n  gather(\"measurement\", \"value\", Shannon:Simpson) %&gt;%\n  ggplot(., aes(x=SampleType, y=value)) +\n  geom_boxplot(aes(color = SampleType), fill=NA, outlier.shape=NA, show.legend = FALSE) +\n  geom_jitter(width=0.2, shape=21, aes(fill = SampleType), color = 'black', alpha=0.5) +\n  facet_wrap(~measurement, scales = \"free_y\", strip.position = \"left\") +\n  ## labels\n  labs(x=\"Sample Type\", y=\"\", fill = \"Sample Type\") +\n\n  scale_fill_manual(values = fill_col) +\n  scale_color_manual(values = fill_col) +\n\n  ## theme options\n  theme_bw() + \n  theme(panel.background=element_blank(),\n        strip.background=element_blank(),\n        strip.text = element_text(size = 10, face=\"bold\"),\n        legend.position = \"none\",\n        strip.clip = 'off',\n        strip.placement = \"outside\",\n        axis.text.y = element_text(size=8, color=\"grey30\"),\n        axis.text.x = element_text(size=8, color=\"grey30\"),\n        axis.title.y = element_text(margin = margin(t = 0, r = 10, b = 0, l = 0), size=10, face=\"bold\"),\n        axis.title.x = element_text(margin = margin(t = 10, r = 0, b = 0, l = 0), size=10, face=\"bold\"))\n</code></pre> <p></p> <pre><code>ggsave(\"example_output/Figures/alpha_diversity.png\", width = 6, height = 4)\n</code></pre> <p>Richness vs.\u00a0Shannon per sample</p> <pre><code>## Species Richness\nbiodiv_df &lt;- df %&gt;% \n  rownames_to_column(var = \"Species_name\") %&gt;% \n  gather(\"sampleID\", \"relab\", 2:last_col()) %&gt;% \n  group_by(sampleID) %&gt;%\n  summarize(richness = sum(relab &gt; 0)) %&gt;%\n\n## Species Evenness ~ Shannon\nleft_join(., alpha_div %&gt;% dplyr::select(sampleID, Shannon), by = \"sampleID\") \n\n## Exporting data \nbiodiv_df %&gt;% dplyr::rename(Richness = richness) %&gt;% left_join(., meta, by = \"sampleID\") %&gt;%\n  write_xlsx(\"example_output/Biodiversity.xlsx\")\n\n## plotting\nbiodiv_df %&gt;%\n  ggplot(., aes(x=richness, y=Shannon)) + \n\n  geom_rect(aes(xmin = quantile(biodiv_df$richness, 0.75), xmax = Inf, \n                ymin = quantile(biodiv_df$Shannon, 0.75, na.rm = TRUE), ymax = Inf),\n            fill = \"#F2F7F2\", alpha = 0.15) +\n\n  geom_point(fill = \"#97C1DE\", color='black', shape=21, alpha=0.5, size=2) + \n\n  labs(\n    x = \"Species Richness\",\n    y = \"Species Evenness (Shannon Index)\"\n  ) +\n  theme_bw() +\n  theme(\n    axis.text.y = element_text(size=8, color=\"grey30\"),\n        axis.text.x = element_text(size=8, color=\"grey30\"),\n        axis.title.y = element_text(margin = margin(t = 0, r = 10, b = 0, l = 0), size=10, face=\"bold\"),\n        axis.title.x = element_text(margin = margin(t = 10, r = 0, b = 0, l = 0), size=10, face=\"bold\")\n  )\n</code></pre> <pre><code>## Warning: Use of `biodiv_df$richness` is discouraged.\n## \u2139 Use `richness` instead.\n\n## Warning: Use of `biodiv_df$Shannon` is discouraged.\n## \u2139 Use `Shannon` instead.\n\n## Warning: Removed 5 rows containing missing values or values outside the scale range\n## (`geom_point()`).\n</code></pre> <p></p> <pre><code>ggsave(\"example_output/Figures/biodiversity.png\", width = 5.5, height = 5)\n</code></pre> <pre><code>## Warning: Use of `biodiv_df$richness` is discouraged.\n## \u2139 Use `richness` instead.\n\n## Warning: Use of `biodiv_df$Shannon` is discouraged.\n## \u2139 Use `Shannon` instead.\n\n## Warning: Removed 5 rows containing missing values or values outside the scale range\n## (`geom_point()`).\n</code></pre>"},{"location":"eDNA%2012S%20metab/05-biodiversity/#statistics","title":"Statistics","text":"<p>Test: T-test or ANOVA - Type I, II, and III: - * indicates an interaction (SampleType*Month). Usually we are interested in the interaction of our factors. - + indicates an additive effect (SampleType + Month)</p> <p>T-test to be used when only two groups to compare and ANOVA to be used with 3+ groups. Al</p> <p>non-parametric Kolmogorov-Smirnov test for two-group comparisons when there are no relevant covariates</p> <pre><code>## Create model \naov &lt;- aov(Shannon ~ SampleType, data = alpha_div)\n\n## ANOVA test on above model\nAnova(aov, type = \"III\")\n</code></pre> <pre><code>## Anova Table (Type III tests)\n## \n## Response: Shannon\n##              Sum Sq  Df F value       Pr(&gt;F)    \n## (Intercept)  10.880   1 28.7312 0.0000001589 ***\n## SampleType    2.308   2  3.0478      0.04884 *  \n## Residuals   121.935 322                         \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n</code></pre> <p>Test: If 3+ groups, Tukey Post Hoc Comparisons</p> <pre><code>TukeyHSD(aov)\n</code></pre> <pre><code>##   Tukey multiple comparisons of means\n##     95% family-wise confidence level\n## \n## Fit: aov(formula = Shannon ~ SampleType, data = alpha_div)\n## \n## $SampleType\n##                          diff         lwr       upr     p adj\n## Control-Blank       0.4623737  0.01228584 0.9124616 0.0425023\n## Inside WEA-Blank    0.3613299 -0.06740447 0.7900642 0.1176828\n## Inside WEA-Control -0.1010438 -0.29204877 0.0899611 0.4272387\n</code></pre>"},{"location":"eDNA%2012S%20metab/05-biodiversity/#beta-diversity","title":"Beta Diversity","text":"<p>Comparing the community assemblages between sites/groups.</p> <p>Resources (Read before continuing): - https://ourcodingclub.github.io/tutorials/ordination/ - https://uw.pressbooks.pub/appliedmultivariatestatistics/chapter/comparison-of-ordination-techniques/</p> <p>Common options: - Principal Components Analysis (PCA): Euclidean distance measure - Principal Coordinates Analysis (PCoA): Dissimilarity distance based - Non-metric MultiDimensional Scaling (NMDS): Dissimilarity distance based</p> <p>PCoA and NMDS handle zero\u2019s in community matrices much better than PCA. For metabarcoding data, usually PCoA and NMDS are more appropriate. The differences between PCoA and NMDS are minor compared to difference between PCA. NMDS is iterative and used a different ordering method (see resource links above for more info).</p> <p>NMDS requires evaluation of the output \u2018stress value\u2019: This value tells you how well the model fit your data. This is helpful to include on your NMDS plot in reports/manuscripts/presentations.</p> <p>Stress (0-1 scale) Interpretation <code>&lt; 0.05</code> = Excellent representation with no prospect of misinterpretation <code>&lt; 0.10</code> = Good ordination with no real disk of drawing false inferences <code>&lt; 0.20</code> = Can be useful but has potential to mislead. In particular, shouldn\u2019t place too much reliance on the details <code>&gt; 0.20</code> = Could be dangerous to interpret <code>&gt; 0.35</code> = Samples placed essentially at random; little relation to original ranked distances</p> <p>If your stress value is &gt;0.2, do not include in analyses and try PCoA instead.</p>"},{"location":"eDNA%2012S%20metab/05-biodiversity/#calculating-dissimilarity-matrix","title":"Calculating dissimilarity matrix","text":"<pre><code>## Bray Curtis Dissimilarity Matrix (used in statistics)\nbray_df &lt;- phyloseq::distance(phylo_obj, method = \"bray\")\n\n## Sample information\nsample_df &lt;- data.frame(sample_data(phylo_obj))\n</code></pre>"},{"location":"eDNA%2012S%20metab/05-biodiversity/#plotting","title":"Plotting","text":""},{"location":"eDNA%2012S%20metab/05-biodiversity/#pcoa","title":"PCoA","text":"<p>https://www.rdocumentation.org/packages/phyloseq/versions/1.16.2/topics/ordinate</p> <pre><code>## Conduct PCoA \npcoa &lt;- ordinate(physeq = phylo_obj, method = \"PCoA\", distance = \"bray\")\n\n## Plotting\nplot_ordination(phylo_obj, pcoa, \n\n                ## USER EDITS shape, color, alpha, fill, etc. as desired based on project metadata\n                color = \"Depth\") +\n\n  ## Point and point aesthetics\n  geom_point(aes(color = Depth), alpha = .5, size = 5) +\n  scale_color_manual(values = c(\"green4\", \"gold3\")) +\n\n  ## Labels: USER EDITS as desired\n  labs(color = \"Sample Type\") +\n  ggtitle(\"PCoA example\") +\n\n  ## Theme: USER EDITS as desired\n  theme_bw() +\n  theme(\n    legend.title = element_text(face = \"bold\", size=12),\n    legend.position = \"right\",\n    axis.title.y = element_text(margin = margin(t = 0, r = 10, b = 0, l = 0), size=10, face=\"bold\"),\n    axis.title.x = element_text(margin = margin(t = 10, r = 0, b = 0, l = 0), size=10, face=\"bold\"),\n  ) \n</code></pre> <p></p> <pre><code>## USER EDITS WIDTH AND HEIGHT TO DESIRED   \nggsave(\"example_output/Figures/PCoA_phyloseq.png\", width = 8, height = 6)\n</code></pre>"},{"location":"eDNA%2012S%20metab/05-biodiversity/#nmds","title":"NMDS","text":"<pre><code>## Calculating NMDS \n## filtering for the sake of example visual\nphylo_obj_filtered &lt;- prune_samples(sample_data(phylo_obj)$SampleType != \"Blank\", phylo_obj)\n\nNMDS &lt;- ordinate(physeq = phylo_obj_filtered, method = \"NMDS\", distance = \"bray\")\n</code></pre> <pre><code>## Run 0 stress 0.2313732 \n## Run 1 stress 0.2495084 \n## Run 2 stress 0.2346765 \n## Run 3 stress 0.2457704 \n## Run 4 stress 0.2481374 \n## Run 5 stress 0.2360292 \n## Run 6 stress 0.2377895 \n## Run 7 stress 0.237543 \n## Run 8 stress 0.2442771 \n## Run 9 stress 0.2434889 \n## Run 10 stress 0.2538366 \n## Run 11 stress 0.2454598 \n## Run 12 stress 0.236795 \n## Run 13 stress 0.2361073 \n## Run 14 stress 0.2456342 \n## Run 15 stress 0.2375375 \n## Run 16 stress 0.2483309 \n## Run 17 stress 0.2331353 \n## Run 18 stress 0.2391534 \n## Run 19 stress 0.2405843 \n## Run 20 stress 0.2308905 \n## ... New best solution\n## ... Procrustes: rmse 0.02822854  max resid 0.1826603 \n## *** Best solution was not repeated -- monoMDS stopping criteria:\n##      1: no. of iterations &gt;= maxit\n##     18: stress ratio &gt; sratmax\n##      1: scale factor of the gradient &lt; sfgrmin\n</code></pre> <pre><code>## Plotting\nplot_ordination(phylo_obj_filtered, NMDS, \n\n                ## USER EDITS shape, color, alpha, fill, etc. as desired based on project metadata\n                color = \"SampleType\") +\n\n  ## Point and point aesthetics\n  geom_point(aes(color = SampleType), alpha = .5, size = 5) +\n  scale_color_manual(values = fill_col) +\n\n  ## Labels: USER EDITS as desired\n  labs(color = \"Sample Type\") +\n  ggtitle(\"NMDS example\") +\n\n  ## adding stress value to plot (user edits x and y to desired location)\n  annotate(geom = \"label\", x = 1.2, y = 1.8, \n           label = sprintf(\"Stress: %.6f\", NMDS$stress), hjust = 0, vjust = 1, \n           label.size = NA, fontface = \"italic\", color = \"grey30\", size = 2.75, fill=\"white\") +\n\n  ## Theme: USER EDITS as desired\n  theme_bw() +\n  theme(\n    legend.title = element_text(face = \"bold\", size=12),\n    legend.position = \"right\",\n    axis.title.y = element_text(margin = margin(t = 0, r = 10, b = 0, l = 0), size=10, face=\"bold\"),\n    axis.title.x = element_text(margin = margin(t = 10, r = 0, b = 0, l = 0), size=10, face=\"bold\"),\n  ) \n</code></pre> <pre><code>## USER EDITS WIDTH AND HEIGHT TO DESIRED   \nggsave(\"example_output/Figures/NMDS_phyloseq.png\", width = 8, height = 6)\n</code></pre>"},{"location":"eDNA%2012S%20metab/05-biodiversity/#statistics_1","title":"Statistics","text":"<p>Test: PERMANOVA - * indicates an interaction (SampleType*Month). Usually we are interested in the interaction of our factors. - + indicates an additive effect (SampleType + Month)</p> <p>The output will tell you which factors significantly impact the community assemblage (matrix).</p> <pre><code>adonis2(bray_df ~ SampleType, data = sample_df, permutations = 99)\n</code></pre> <pre><code>## Permutation test for adonis under reduced model\n## Permutation: free\n## Number of permutations: 99\n## \n## adonis2(formula = bray_df ~ SampleType, data = sample_df, permutations = 99)\n##           Df SumOfSqs      R2      F Pr(&gt;F)   \n## Model      2    2.024 0.01842 3.0217   0.01 **\n## Residual 322  107.839 0.98158                 \n## Total    324  109.863 1.00000                 \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n</code></pre> <p>Additional test: Pairwise PERMANOVA</p> <p>The output will tell you which specific variable within each factor is driving the significant effects.</p> <pre><code>pairwise.adonis2(bray_df ~ SampleType, data = sample_df)\n</code></pre> <pre><code>## $parent_call\n## [1] \"bray_df ~ SampleType , strata = Null , permutations 999\"\n## \n## $`Inside WEA_vs_Blank`\n##           Df SumOfSqs      R2      F Pr(&gt;F)    \n## Model      1    1.434 0.01691 4.2478  0.001 ***\n## Residual 247   83.362 0.98309                  \n## Total    248   84.796 1.00000                  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## $`Inside WEA_vs_Control`\n##           Df SumOfSqs      R2      F Pr(&gt;F)  \n## Model      1    0.639 0.00606 1.8952  0.049 *\n## Residual 311  104.859 0.99394                \n## Total    312  105.498 1.00000                \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## $Blank_vs_Control\n##          Df SumOfSqs      R2      F Pr(&gt;F)   \n## Model     1   1.1454 0.04005 3.5878  0.002 **\n## Residual 86  27.4558 0.95995                 \n## Total    87  28.6012 1.00000                 \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## attr(,\"class\")\n## [1] \"pwadstrata\" \"list\"\n</code></pre>"},{"location":"eDNA%20COI%20metab/01-Metabarcoding%20Mothur%20COI/","title":"eDNA Metabarcoding for COI target region","text":""},{"location":"eDNA%20COI%20metab/01-Metabarcoding%20Mothur%20COI/#primer-set","title":"Primer set","text":"<p>The COI region is commonly used for metabarcoding practices and consequently there are many primer options to choose from. The Fisheries team at GMGI has optimized the Leray Geller set (outlined in red box below). Citation: Leray et al 2013.</p> <p>We primarily use this set for invertebrate targets and 12S for vertebrate communities. </p> <p></p>"},{"location":"eDNA%20COI%20metab/01-Metabarcoding%20Mothur%20COI/#workflow","title":"Workflow","text":"<p>Mothur program page</p> <p></p> <p>Workflow done on HPC. Scripts to run: </p> <ol> <li>Confirm conda environment is available      </li> <li>Assess quality of raw data (00-fastqc.sh)        </li> <li>Visualize quality of raw data (00-multiqc.sh)     </li> <li>Set-up Mothur program (01-Mothur-setup.sh)  </li> <li>QC'ing seqs via Mothur (02-Mothur-QC.sh)      </li> <li>Determining and counting unique sequences with Mothur (03-Mothur-unique.sh)   </li> <li>Taxonomic Assignment with Mothur (04-Mothur-tax.sh)  </li> </ol> <p>Descriptions of Mothur steps are from A. Huffmyer and E. Strand Mothur for 16S notebook posts. </p>"},{"location":"eDNA%20COI%20metab/01-Metabarcoding%20Mothur%20COI/#step-1-confirm-conda-environment-is-available","title":"Step 1: Confirm conda environment is available","text":"<p>The conda environment is started within each slurm script, but to activate conda environment outside of the slurm script to update packages or check what is installed:</p> <pre><code># Activate conda\nsource /work/gmgi/miniconda3/bin/activate\n\n# Activate fisheries eDNA conda environment \nconda activate fisheries_eDNA\nconda activate eDNA_COI\n\n# List all available environments \nconda env list \n\n# List all packages installed in fisheries_eDNA\nconda list \n\n# Update a package\nconda update [package name]\n</code></pre>"},{"location":"eDNA%20COI%20metab/01-Metabarcoding%20Mothur%20COI/#step-2-assess-quality-of-raw-data","title":"Step 2: Assess quality of raw data","text":"<p>Background information on FASTQC. </p> <p><code>00-fastqc.sh</code>: </p> <pre><code>#!/bin/bash\n#SBATCH --error=output/fastqc_output/\"%x_error.%j\" #if your job fails, the error report will be put in this file\n#SBATCH --output=output/fastqc_output/\"%x_output.%j\" #once your job is completed, any final job report comments will be put in this file\n#SBATCH --partition=short\n#SBATCH --nodes=1\n#SBATCH --time=20:00:00\n#SBATCH --job-name=fastqc\n#SBATCH --mem=3GB\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=2\n\n### USER TO-DO ### \n## 1. Set paths for your project\n\n# Activate conda environment\nsource /work/gmgi/miniconda3/bin/activate fisheries_eDNA\n\n## SET PATHS \nraw_path=$1\nout_dir=$2\n\n## CREATE SAMPLE LIST FOR SLURM ARRAY\n### 1. Create list of all .gz files in raw data path\nls -d ${raw_path}/*.gz &gt; ${raw_path}/rawdata\n\n### 2. Create a list of filenames based on that list created in step 1\nmapfile -t FILENAMES &lt; ${raw_path}/rawdata\n\n### 3. Create variable i that will assign each row of FILENAMES to a task ID\ni=${FILENAMES[$SLURM_ARRAY_TASK_ID - 1]}\n\n## RUN FASTQC PROGRAM \nfastqc ${i} --outdir ${out_dir}\n</code></pre> <p>To run:   - Start slurm array (e.g., with 138 files) = <code>sbatch --array=1-138 /work/gmgi/scripts/00-fastqc.sh /path/to/raw/data /path/to/output/directory</code>.</p> <p>Notes:  </p> <ul> <li>This is going to output many error and output files. After job completes, use <code>cat *output.* &gt; ../fastqc_output.txt</code> to create one file with all the output and <code>cat *error.* &gt; ../fastqc_error.txt</code> to create one file with all of the error message outputs. </li> <li>Within the <code>out_dir</code> output folder, use <code>ls *html | wc</code> to count the number of html output files (1st/2nd column values). This should be equal to the --array range used and the number of raw data files. If not, the script missed some input files so address this before moving on.  </li> </ul>"},{"location":"eDNA%20COI%20metab/01-Metabarcoding%20Mothur%20COI/#step-3-visualize-quality-of-raw-data","title":"Step 3: Visualize quality of raw data","text":"<p>Background information on MULTIQC.</p> <p><code>00-multiqc.sh</code> </p> <pre><code>#!/bin/bash\n#SBATCH --error=output/\"%x_error.%j\" #if your job fails, the error report will be put in this file\n#SBATCH --output=output/\"%x_output.%j\" #once your job is completed, any final job report comments will be put in this file\n#SBATCH --partition=short\n#SBATCH --nodes=1\n#SBATCH --time=10:00:00\n#SBATCH --job-name=multiqc\n#SBATCH --mem=8GB\n#SBATCH --ntasks=24\n#SBATCH --cpus-per-task=2\n\n### USER TO-DO ### \n## 1. Set paths for your project\n## 2. Optional: change file name (multiqc_raw.html) as desired\n\n# Activate conda environment\nsource /work/gmgi/miniconda3/bin/activate fisheries_eDNA\n\n## SET PATHS \n## fastqc_output = output from 00-fastqc.sh; fastqc program\nfastqc_output=$1\nmultiqc_dir=$2\nfilename=$3\n\n## RUN MULTIQC \nmultiqc --interactive ${fastqc_output} -o ${multiqc_dir} --filename multiqc_${filename}.html\n</code></pre> <p>To run: - Navigate to fastqc output and run <code>sbatch /work/gmgi/scripts/00-multiqc.sh /path/to/fastqc/data /path/to/multiqc/output/directory filename</code> </p> <p>Notes:  </p> <ul> <li>Depending on the number of files per project, multiqc can be quick to run without a slurm script. To do this, run each line separately in the command line after activating the conda environment.  </li> </ul>"},{"location":"eDNA%20COI%20metab/01-Metabarcoding%20Mothur%20COI/#step-4-set-up-mothur-program","title":"Step 4: Set-up Mothur program","text":""},{"location":"eDNA%20COI%20metab/01-Metabarcoding%20Mothur%20COI/#create-primer-file","title":"Create primer file","text":"<p>Project set-up create primer seq file, only needed once, if file is already there do not create a new one: 1. Navigate to COI databases folder: <code>cd /work/gmgi/databases/COI</code> 2. Create new file for Leray Gellar primer set: <code>nano mothur_oligos_LG</code> 3. Copy and paste the below content:</p> <pre><code>## Leray Geller 2013 primers \n## format = primer F R\n\nprimer GGWACWGGWTGAACWGTWTAYCCYCC TAIACYTCIGGRTGICCRAARAAYCA\n</code></pre>"},{"location":"eDNA%20COI%20metab/01-Metabarcoding%20Mothur%20COI/#mothur-program-set-up-project","title":"Mothur program set-up project","text":"<p>Create directory: <code>mkdir Mothur_data</code></p> <p><code>make.file()</code>: tells mothur to look for fastq files in the data directory (<code>rawdata_dir</code>) and identify forward and reverse reads. Put type=gz to look for gz files. If there are .fasta or .fastq files you can use type=fasta or type=fastq. The prefix gives the output file a name - this can be a project name. This creates a file called proj_name.files with sample name, R1, R2 listed in columns. - Output: <code>proj.paired.files</code> within the <code>rawdata_dir</code> </p> <p><code>make.contigs()</code>: makes contigs of forward and reverse reads for each sample. This will assemble contigs for each pair of files and use R1 and R2 quality information to correct any sequencing errors in locations where there is overlap and the quality of the complementary sequence is sufficient. Because we added an oligos file, make.contigs will remove the primers on these sequences.  - <code>pdiffs=5</code>: primer differences=5. Try various parameters from 2-10 (based on other pipeline I've used or seen in the past). - <code>checkorient=t</code>: Degenerate primers can be used in our oligos file. We are also using check orient to allow Mothur to \"flip\" the reverse primer if it is not found. - <code>trimoverlap=F</code>: TRUE is needed if sequencing length was longer than amplicon length. The COI region is 313 bp which is longer than 2x250 bp sequencing.  </p> <p><code>summary.seqs()</code>: generates summary information about the sequences from the files we made above.</p> <p>Run time: ~30 min for one MiSeq run worth (96 samples). </p> <p><code>01-Mothur-setup.sh</code></p> <pre><code>#!/bin/bash\n#SBATCH --error=output/\"%x_error.%j\" #if your job fails, the error report will be put in this file\n#SBATCH --output=output/\"%x_output.%j\" #once your job is completed, any final job report comments will be put in this file\n#SBATCH --partition=short\n#SBATCH --nodes=1\n#SBATCH --time=10:00:00\n#SBATCH --job-name=mothur_setup\n#SBATCH --mem=10GB\n#SBATCH --ntasks=3\n#SBATCH --cpus-per-task=3\n\n# Activate conda environment\nsource /work/gmgi/miniconda3/bin/activate eDNA_COI\n\n# Set Paths \noligo_file=\"/work/gmgi/databases/COI/mothur_oligos_LG\"\n\n## User specific paths\n#rawdata_dir=\"/work/gmgi/Fisheries/eDNA/offshore_wind/invertebrate/raw_data\"\n#proj_name=\"OSW_2023_invert\" \n#output_dir=\"/work/gmgi/Fisheries/eDNA/offshore_wind/invertebrate/Mothur_data\"\n\nrawdata_dir=$1\noutput_dir=$2\nproj_name=$3\n\n## Make symlinks from raw data with new name (_ instead of -)\nfor file in ${rawdata_dir}/*.gz; do\n    filename=$(basename \"$file\")\n    newname=${filename//-/_}\n    ln -s \"$file\" \"${output_dir}/${newname}\"\ndone\n\ncd ${output_dir} \n\n## Make file with raw fastq files\nmothur \"#make.file(inputdir=., type=gz, prefix=${proj_name})\"   \n\n## Make contig file \nmothur \"#make.contigs(inputdir=., outputdir=., file=${proj_name}.paired.files, trimoverlap=F, oligos=${oligo_file}, pdiffs=5, checkorient=T)\"\n\n## Create a summary file with trimmed contigs \nmothur \"#summary.seqs(fasta=${proj_name}.paired.trim.contigs.fasta)\"\n</code></pre> <p>To run: <code>sbatch /work/gmgi/scripts/eDNA/COI/Mothur/01-Mothur-setup.sh /path/to/raw/data /path/to/output/directory project_prefix</code> </p> <p>Example: </p> <pre><code>sbatch 01-Mothur-setup.sh \\\n    /work/gmgi/Fisheries/eDNA/offshore_wind/invertebrate/raw_data \\\n    /work/gmgi/Fisheries/eDNA/offshore_wind/invertebrate/Mothur_data \\\n    OSW_2023_invert\n</code></pre>"},{"location":"eDNA%20COI%20metab/01-Metabarcoding%20Mothur%20COI/#assess-output-data-from-osw-example","title":"Assess output (data from OSW example):","text":"<p>Count the number of sequences that were removed and the number that were kept by counting sequences in each fasta file</p> <pre><code>proj_name=\"OSW_2023_invert\"\ngrep -c \"^&gt;\" ${proj_name}.paired.trim.contigs.fasta\n## output = 10,272,910\n\ngrep -c \"^&gt;\" ${proj_name}.paired.scrap.contigs.fasta \n## output = 3,033,941\n</code></pre> <p>The logfiles will be named with the run ID so I used <code>mv</code> to change these to names that reflect the step: - <code>mothur.01setup.makecontigs.logfile</code> - <code>mothur.01setup.summary.contigs.logfile</code> </p> <p>Summary of OSW data from <code>mothur.01setup.summary.contigs.logfile</code>: </p> <pre><code>                Start   End     NBases  Ambigs  Polymer NumSeqs\nMinimum:        1       124     124     0       3       1\n2.5%-tile:      1       251     251     0       4       265093\n25%-tile:       1       365     365     0       5       2650929\nMedian:         1       365     365     0       6       5301857\n75%-tile:       1       365     365     1       6       7952785\n97.5%-tile:     1       404     404     8       8       10338621\nMaximum:        1       502     502     196     233     10603713\nMean:   1       363     363     1       5\n# of Seqs:      10603713\n</code></pre> <p>This table shows quantile values about the distribution of sequences for a few things:  - Start position: All at 1 now, will start at different point after some QC. - End position: We see that there are some sequences that are very short and we may need to remove those later. - Number of bases: length. we see most are in expected range here, but one is super long! This might tell us there is no overlap so they are butted up against each other. We will remove things like this. - Ambigs: Number of ambiguous calls in sequences. Here there are a few that have ambiguous base calls. We will remove any sequence with an ambiguous call or any longer than we would expect for COI region. - Polymer: Length of polymer repeats. - NumSeqs: Number of sequences.  </p> <p>Output:   - proj_name.contigs.groups                - proj_name.contigs.report                                       - proj_name.scrap.contigs.fasta           - proj_name.trim.contigs.fasta  - proj_name.trim.contigs.summary  </p> <p>Descriptions of contig files:  - Trim file = sequences that were \"good\". - Scrap file = sequences that were \"bad\". - Groups file = what group each sequence belongs to map sequence to each sample from the trimmed sequence file. - Contigs report file = information on sequences that were aligned and paired together.  </p>"},{"location":"eDNA%20COI%20metab/01-Metabarcoding%20Mothur%20COI/#step-5-qcing-seqs-via-mothur","title":"Step 5: QC'ing seqs via Mothur","text":"<p>Additional QC steps besides removing primers. Check the 25%-tile, median, and 75%-tile values from the table above. That will determine the max cut-offs below along with the expected length of the COI region (~313 bp but in OSW data, the majority are &gt;350 bp). </p> <p><code>screen.seqs()</code>: specify the fasta file of the contigs generated in the previous step and remove any sequence with an ambiguous call (\"N\"). We will also remove sequences &gt;450 nt. We will also set a minimum size amount (200). These parameters could be adjusted based on specific experiment and variable region.</p> <p><code>summary.seqs()</code>: summarizes the quantity and characteristics of sequences in a FASTA file. Often used directly after a screen.seqs(), align.seqs(), or unique.seqs() to summarize the results of the previous step. </p> <p><code>unique.seqs()</code>: extracts unique sequences from a FASTA-formatted sequence file. </p> <p><code>count.seqs()</code>: counts the total number of sequences represented by each representative sequence in a name or count file. This can also count based on groups given (e.g., samples). </p> <p><code>02-Mothur-QC.sh</code></p> <pre><code>#!/bin/bash\n#SBATCH --error=output/\"%x_error.%j\" #if your job fails, the error report will be put in this file\n#SBATCH --output=output/\"%x_output.%j\" #once your job is completed, any final job report comments will be put in this file\n#SBATCH --partition=short\n#SBATCH --nodes=1\n#SBATCH --time=10:00:00\n#SBATCH --job-name=mothur_QC\n#SBATCH --mem=10GB\n#SBATCH --ntasks=6\n#SBATCH --cpus-per-task=2\n\n# Activate conda environment\nsource /work/gmgi/miniconda3/bin/activate eDNA_COI\n\ndir=$1\nproj_name=$2\n\ncd ${dir}\n\n## Screen seqs and summarize\nmothur \"#screen.seqs(inputdir=., outputdir=., fasta=${proj_name}.paired.trim.contigs.fasta, group=${proj_name}.paired.contigs.groups, maxambig=0, maxlength=450, minlength=200)\"\nmothur \"#summary.seqs(fasta=${proj_name}.paired.trim.contigs.good.fasta)\"\n\n## Count the number of unique sequences\nmothur \"#unique.seqs(fasta=${proj_name}.paired.trim.contigs.good.fasta)\"\n\n## Counts seqs per sample\nmothur \"#count.seqs(name=${proj_name}.paired.trim.contigs.good.names, group=${proj_name}.paired.contigs.good.groups)\"\nmothur \"#summary.seqs(fasta=${proj_name}.paired.trim.contigs.good.unique.fasta, count=${proj_name}.paired.trim.contigs.good.count_table)\"\n</code></pre> <p>To run: <code>sbatch /work/gmgi/scripts/eDNA/COI/Mothur/02-Mothur-QC.sh /path/to/output/directory project_prefix</code>  Example: Running OSW from invertebrate/scripts folder </p> <pre><code>sbatch 02-Mothur-QC.sh \\\n    /work/gmgi/Fisheries/eDNA/offshore_wind/invertebrate/Mothur_data \\\n    OSW_2023_invert\n</code></pre> <p>The logfiles will be named with the run ID so I used <code>mv</code> to change these to names that reflect the step: - <code>mothur.02QC.screenseqs.logfile</code> - <code>mothur.02QC.screenseqs.summary.logfile</code>  - <code>mothur.02QC.uniqueseqs.logfile</code> - <code>mothur.02QC.countseqs.logfile</code> - <code>mothur.02QC.countseqs.summary.logfile</code> </p> <p>OSW example from the output files: </p> <pre><code>                Start   End     NBases  Ambigs  Polymer NumSeqs\nMinimum:        1       213     213     0       3       1\n2.5%-tile:      1       251     251     0       4       183163\n25%-tile:       1       365     365     0       5       1831628\nMedian:         1       365     365     0       6       3663256\n75%-tile:       1       365     365     0       6       5494883\n97.5%-tile:     1       404     404     0       7       7143348\nMaximum:        1       450     450     0       171     7326510\nMean:   1       363     363     0       5\n# of Seqs:      7326510\n# of unique: 3521350\n</code></pre>"},{"location":"eDNA%20COI%20metab/01-Metabarcoding%20Mothur%20COI/#step-6-taxonomic-assignment-with-mothur","title":"Step 6: Taxonomic Assignment with Mothur","text":"<p>Using MetaZooGene Global database as reference. This database has sequences from both BOLD and NCBI GenBank. </p> <p>https://metazoogene.org/mzgdb/. Navigate to the Worlds oceans page and click MZGdb Data Access page. We have downloaded both the Global and North Atlantic versions.   </p> <p>Download and update database:</p> <pre><code>cd /work/gmgi/databases/COI/MetaZooGene\n\n## Global \nwget https://www.st.nmfs.noaa.gov/copepod/collaboration/metazoogene/atlas/data-src/MZGfasta-coi__T4000000__o00__A.fasta.gz\nwget https://www.st.nmfs.noaa.gov/copepod/collaboration/metazoogene/atlas/data-src/MZGmothur-coi__T4000000__o00__A.txt.gz\n\n## North Atlantic \nwget https://www.st.nmfs.noaa.gov/copepod/collaboration/metazoogene/atlas/data-src/MZGfasta-coi__T4000000__o02__A.fasta.gz\nwget https://www.st.nmfs.noaa.gov/copepod/collaboration/metazoogene/atlas/data-src/MZGmothur-coi__T4000000__o02__A.txt.gz\n\n## unzip into new name file \n## replace with version number \ngunzip -c MZGfasta-coi__T4000000__o00__A.fasta.gz &gt; MZG_v2023-m07-15_Global_modeA.fasta\ngunzip -c MZGfasta-coi__T4000000__o02__A.fasta.gz &gt; MZG_v2023-m07-15_NorthAtlantic_modeA.fasta\ngunzip -c MZGmothur-coi__T4000000__o02__A.txt.gz &gt; MZG_v2023-m07-15-NorthAtlantic_modeA.txt\ngunzip -c MZGmothur-coi__T4000000__o00__A.txt.gz &gt; MZG_v2023-m07-15-Global_modeA.txt\n</code></pre> <p>Align the database prior to using as input in Mothur:</p> <p><code>MZG_align.sh</code></p> <pre><code>#!/bin/bash\n#SBATCH --error=\"%x_error.%j\" #if your job fails, the error report will be put in this file\n#SBATCH --output=\"%x_output.%j\" #once your job is completed, any final job report comments will be put in this file\n#SBATCH --partition=short\n#SBATCH --nodes=1\n#SBATCH --time=10:00:00\n#SBATCH --job-name=MZG_MAFFT\n#SBATCH --mem=50GB\n#SBATCH --ntasks=24\n#SBATCH --cpus-per-task=2\n\n# Activate conda environment\nsource /work/gmgi/miniconda3/bin/activate eDNA_COI\n\n### changing name format to MAFFT_xxx_aligned.fasta \n\n# Assign input and output file names from command-line arguments\nINPUT_FILE=$1\nOUTPUT_FILE=$2\n\n# Run MAFFT with the provided input and output files\nmafft --auto \"$INPUT_FILE\" &gt; \"$OUTPUT_FILE\"\n</code></pre> <p>Use format: <code>sbatch MZG_align.sh input output</code>  Example: <code>sbatch MZG_align.sh MZG_v2023-m07-15_NorthAtlantic_modeA.fasta MAFFT_MZG_v2023-m07-15_NorthAtlantic_modeA_aligned.fasta</code> <code>sbatch MZG_align.sh MZG_v2023-m07-15_Global_modeA.fasta MAFFT_MZG_v2023-m07-15_Global_modeA_aligned.fasta</code> </p> <p>Running taxonomic assignment via Mothur: </p> <p><code>03-Mothur-tax.sh</code></p> <pre><code>#!/bin/bash\n#SBATCH --error=output/\"%x_error.%j\" #if your job fails, the error report will be put in this file\n#SBATCH --output=output/\"%x_output.%j\" #once your job is completed, any final job report comments will be put in this file\n#SBATCH --partition=short\n#SBATCH --nodes=1\n#SBATCH --time=24:00:00\n#SBATCH --job-name=mothur_tax\n#SBATCH --mem=20GB\n#SBATCH --ntasks=12\n#SBATCH --cpus-per-task=2\n\n# Activate conda environment\nsource /work/gmgi/miniconda3/bin/activate eDNA_COI\n\ndir=\"/work/gmgi/Fisheries/eDNA/offshore_wind/invertebrate/Mothur_data\"\nproj_name=\"OSW_2023_invert\" \nref=$1\n\ncd ${dir}\n\n## Aligning to taxonomy file\nmothur \"#align.seqs(fasta=${proj_name}.paired.trim.contigs.good.unique.fasta, reference=${ref}, flip=T, processors=48)\"\nmothur \"#summary.seqs(fasta=${proj_name}.paired.trim.contigs.good.unique.align)\"\n\n## Identifying those that don't meet the criteria \nmothur \"#screen.seqs(fasta=${proj_name}.paired.trim.contigs.good.unique.align, count=${proj_name}.paired.trim.contigs.good.count_table, optimize=start, criteria=99, maxhomop=8)\"\nmothur \"#summary.seqs(fasta=current, count=current)\"\nmothur \"#count.groups(count=${proj_name}.paired.trim.contigs.good.count_table)\"\n\n## Filtering those out\nmothur \"#filter.seqs(fasta=${proj_name}.paired.trim.contigs.good.unique.good.align, vertical=T, trump=.)\"\nmothur \"#summary.seqs(fasta=current, count=current)\"\n\n## ID unique sequences again\nmothur \"#unique.seqs(fasta=${proj_name}.paired.trim.contigs.good.unique.good.filter.fasta, count=${proj_name}.paired.trim.contigs.good.good.count_table)\"\n</code></pre> <p>How to run: </p> <pre><code>sbatch 03-Mothur-tax.sh /work/gmgi/databases/COI/MetaZooGene/MAFFT_MZG_v2023-m07-15_NorthAtlantic_modeA_aligned.fasta \n</code></pre> <p>Atlantic comparison:</p> <pre><code>                Start   End     NBases  Ambigs  Polymer NumSeqs\nMinimum:        0       0       0       0       1       1\n2.5%-tile:      1833    7145    3       0       1       86005\n25%-tile:       6781    7145    364     0       5       860046\nMedian:         22391   24785   365     0       6       1720092\n75%-tile:       22508   25827   365     0       6       2580137\n97.5%-tile:     119396  119405  368     0       7       3354178\nMaximum:        121316  121316  450     0       171     3440182\nMean:   26581   33626   308     0       5\n# of Seqs:      3440182\n</code></pre> <p>World comparison:</p> <pre><code>\n</code></pre> <p>If we are using MetaZooGene and it's all COI sequences, why would we want to filter any out? Skip screen seqs for now.</p>"},{"location":"eDNA%20COI%20metab/01-Metabarcoding%20Mothur%20COI/#step-8-qc-taxonomic-alignment","title":"Step 8: QC Taxonomic Alignment","text":"<p><code>05-Mothur-taxQC.sh</code></p> <pre><code>#!/bin/bash\n#SBATCH --error=output/\"%x_error.%j\" #if your job fails, the error report will be put in this file\n#SBATCH --output=output/\"%x_output.%j\" #once your job is completed, any final job report comments will be put in this file\n#SBATCH --partition=short\n#SBATCH --nodes=1\n#SBATCH --time=10:00:00\n#SBATCH --job-name=mothur_taxQC\n#SBATCH --mem=50GB\n#SBATCH --ntasks=6\n#SBATCH --cpus-per-task=2\n\n# Activate conda environment\nsource /work/gmgi/miniconda3/bin/activate eDNA_COI\n\ndir=\"/work/gmgi/Fisheries/eDNA/offshore_wind/invertebrate/Mothur_data\"\nproj_name=\"OSW_2023_invert\" \n\ncd ${dir}\n\n## Identifying those that don't meet the criteria \nmothur \"#screen.seqs(fasta=${proj_name}.paired.trim.contigs.good.unique.align, count=${proj_name}.paired.trim.contigs.good.count_table, optimize=start, criteria=99, maxhomop=8)\"\nmothur \"#summary.seqs(fasta=current, count=current)\"\nmothur \"#count.groups(count=${proj_name}.paired.trim.contigs.good.count_table)\"\n\n## Filtering those out\nmothur \"#filter.seqs(fasta=${proj_name}.paired.trim.contigs.good.unique.good.align, vertical=T, trump=.)\"\nmothur \"#summary.seqs(fasta=current, count=current)\"\n\n## ID unique sequences again\nmothur \"#unique.seqs(fasta=${proj_name}.paired.trim.contigs.good.unique.good.filter.fasta, count=${proj_name}.paired.trim.contigs.good.good.count_table)\"\n</code></pre>"},{"location":"eDNA%20COI%20metab/01-Metabarcoding%20Mothur%20COI/#step-9-denoise-and-remove-chimeras","title":"Step 9: Denoise and remove chimeras","text":""},{"location":"eDNA%20COI%20metab/01-Metabarcoding%20Mothur%20COI/#denoising","title":"Denoising","text":"<p>Now we need to further polish and cluster the data with pre.cluster. The purpose of this step is to remove noise due to sequencing error. The rational behind this step assumes that the most abundant sequences are the most trustworthy and likely do not have sequencing errors. Pre-clustering then looks at the relationship between abundant and rare sequences - rare sequences that are \"close\" (e.g., 1 nt difference) to highly abundant sequences are likely due to sequencing error. This step will pool sequences and look at the maximum differences between sequences within this group to form ASV groupings.</p> <p>In this step, the number of sequences is not reduced, but they are grouped into amplicon sequence variants ASV's which reduces the error rate. </p> <p>Other programs that conduct this \"denoising\" are DADA2, UNOISE, and DEBLUR. However, these programs remove the rare sequences, which can distort the relative abundance of remaining sequences. DADA2 also removes all sigletons (sequences with single representation) which disproportionately affects the sequence relative abundance. Mothur avoids the removal of rare sequences for this reason.</p>"},{"location":"eDNA%20COI%20metab/01-Metabarcoding%20Mothur%20COI/#removing-chimeras","title":"Removing chimeras","text":"<p>At this point we have removed as much sequencing error as we can and it is time to turn our attention to removing chimeras. We'll do this using the VSEARCH algorithm that is called within mothur using the chimera.vsearch command. </p> <p><code>04-Mothur-denoise.sh</code></p> <pre><code>#!/bin/bash\n#SBATCH --error=output/\"%x_error.%j\" #if your job fails, the error report will be put in this file\n#SBATCH --output=output/\"%x_output.%j\" #once your job is completed, any final job report comments will be put in this file\n#SBATCH --partition=short\n#SBATCH --nodes=1\n#SBATCH --time=10:00:00\n#SBATCH --job-name=mothur_denoise\n#SBATCH --mem=20GB\n#SBATCH --ntasks=6\n#SBATCH --cpus-per-task=2\n\n# Activate conda environment\nsource /work/gmgi/miniconda3/bin/activate eDNA_COI\n\ndir=\"/work/gmgi/Fisheries/eDNA/offshore_wind/invertebrate/Mothur_data\"\nproj_name=\"OSW_2023_invert\" \n\ncd ${dir}\n\n## Denoise \nmothur \"#pre.cluster(fasta=${proj_name}.paired.trim.contigs.good.unique.good.filter.unique.fasta, count=${proj_name}.paired.trim.contigs.good.unique.good.filter.count_table, diffs=2, method=unoise)\"\n\n## Identify chimeras \nmothur \"#chimera.vsearch(fasta=${proj_name}.paired.trim.contigs.good.unique.good.filter.unique.precluster.fasta, count=${proj_name}.paired.trim.contigs.good.unique.good.filter.unique.precluster.count_table, dereplicate=T)\"\n\n## Remove chimeras\nmothur \"#remove.seqs(fasta=${proj_name}.paired.trim.contigs.good.unique.good.filter.unique.precluster.fasta, accnos=${proj_name}.paired.trim.contigs.good.unique.good.filter.unique.precluster.denovo.vsearch.accnos)\"\n\n## Produce summary file \nsummary.seqs(fasta=${proj_name}.paired.trim.contigs.good.unique.good.filter.unique.precluster.pick.fasta, count=${proj_name}.paired.trim.contigs.good.unique.good.filter.unique.precluster.denovo.vsearch.pick.count_table)\n</code></pre>"},{"location":"eDNA%20COI%20metab/01-Metabarcoding%20Mothur%20COI/#step-10-classify-sequences","title":"Step 10: Classify sequences","text":"<p>Fasta reference file: Taxonomy file:  </p> <p><code>05-Mothur-ASV.sh</code></p> <pre><code>\n</code></pre>"},{"location":"eDNA%20COI%20metab/01-Metabarcoding%20ampliseq%20COI/","title":"Metabarcoding workflow for COI amplicon sequencing","text":"<p>The COI region is commonly used for metabarcoding practices and consequently there are many primer options to choose from. The Fisheries team at GMGI has optimized the Leray Geller set (outlined in red box below). Citation: Leray et al 2013.</p> <p>We primarily use this set for invertebrate targets and 12S for vertebrate communities. </p> <p></p> <p>Workflow done on HPC. Scripts to run: </p> <ol> <li>00-fastqc.sh   </li> <li>00-multiqc.sh  </li> <li>01a-metadata.R</li> <li>01b-ampliseq.sh</li> <li>02-taxonomicID.sh  </li> </ol>"},{"location":"eDNA%20COI%20metab/01-Metabarcoding%20ampliseq%20COI/#step-1-confirm-conda-environment-is-available","title":"Step 1: Confirm conda environment is available","text":"<p>The conda environment is started within each slurm script, but to activate conda environment outside of the slurm script to update packages or check what is installed:</p> <pre><code># Activate conda\nsource ~/../../work/gmgi/miniconda3/bin/activate\n\n# Activate fisheries eDNA conda environment \nconda activate fisheries_eDNA\n\n# List all available environments \nconda env list \n\n# List all packages installed in fisheries_eDNA\nconda list \n\n# Update a package\nconda update [package name]\n\n# Update nextflow ampliseq workflow \nnextflow pull nf-core/ampliseq\n</code></pre>"},{"location":"eDNA%20COI%20metab/01-Metabarcoding%20ampliseq%20COI/#step-2-assess-quality-of-raw-data","title":"Step 2: Assess quality of raw data","text":"<p>Background information on FASTQC. </p> <p><code>00-fastqc.sh</code>: </p> <pre><code>#!/bin/bash\n#SBATCH --error=output/fastqc_output/\"%x_error.%j\" #if your job fails, the error report will be put in this file\n#SBATCH --output=output/fastqc_output/\"%x_output.%j\" #once your job is completed, any final job report comments will be put in this file\n#SBATCH --partition=short\n#SBATCH --nodes=1\n#SBATCH --time=20:00:00\n#SBATCH --job-name=fastqc\n#SBATCH --mem=3GB\n#SBATCH --ntasks=24\n#SBATCH --cpus-per-task=2\n\n### USER TO-DO ### \n## 1. Set paths for your project\n\n# Activate conda environment\nsource ~/../../work/gmgi/miniconda3/bin/activate\nconda activate fisheries_eDNA\n\n## SET PATHS \nraw_path=\"\"\nout_dir=\"\"\n\n## CREATE SAMPLE LIST FOR SLURM ARRAY\n### 1. Create list of all .gz files in raw data path\nls -d ${raw_path}/*.gz &gt; ${raw_path}/rawdata\n\n### 2. Create a list of filenames based on that list created in step 1\nmapfile -t FILENAMES &lt; ${raw_path}/rawdata\n\n### 3. Create variable i that will assign each row of FILENAMES to a task ID\ni=${FILENAMES[$SLURM_ARRAY_TASK_ID]}\n\n## RUN FASTQC PROGRAM \nfastqc ${i} --outdir ${out_dir}\n</code></pre> <p>To run:   - Start slurm array (e.g., with 138 files) = <code>sbatch --array=0-137 00-fastqc.sh</code>.</p> <p>Notes:  </p> <ul> <li>This is going to output many error and output files. After job completes, use <code>cat *output.* &gt; ../fastqc_output.txt</code> to create one file with all the output and <code>cat *error.* &gt; ../fastqc_error.txt</code> to create one file with all of the error message outputs. </li> <li>Within the <code>out_dir</code> output folder, use <code>ls *html | wc</code> to count the number of html output files (1st/2nd column values). This should be equal to the --array range used and the number of raw data files. If not, the script missed some input files so address this before moving on.  </li> </ul>"},{"location":"eDNA%20COI%20metab/01-Metabarcoding%20ampliseq%20COI/#step-3-visualize-quality-of-raw-data","title":"Step 3: Visualize quality of raw data","text":"<p>Background information on MULTIQC.</p> <p><code>00-multiqc.sh</code> </p> <pre><code>#!/bin/bash\n#SBATCH --error=output/\"%x_error.%j\" #if your job fails, the error report will be put in this file\n#SBATCH --output=output/\"%x_output.%j\" #once your job is completed, any final job report comments will be put in this file\n#SBATCH --partition=short\n#SBATCH --nodes=1\n#SBATCH --time=10:00:00\n#SBATCH --job-name=multiqc\n#SBATCH --mem=8GB\n#SBATCH --ntasks=24\n#SBATCH --cpus-per-task=2\n\n### USER TO-DO ### \n## 1. Set paths for your project\n## 2. Optional: change file name (multiqc_raw.html) as desired\n\n# Activate conda environment\nsource ~/../../work/gmgi/miniconda3/bin/activate\nconda activate fisheries_eDNA\n\n## SET PATHS \n## fastqc_output = output from 00-fastqc.sh; fastqc program\nfastqc_output=\"\" \nmultiqc_dir=\"\" \n\n## RUN MULTIQC \nmultiqc --interactive ${fastqc_output} -o ${multiqc_dir} --filename multiqc_raw.html\n</code></pre> <p>To run: - <code>sbatch 00-multiqc.sh</code> </p> <p>Notes:  </p> <ul> <li>Depending on the number of files per project, multiqc can be quick to run without a slurm script. To do this, run each line separately in the command line after activating the conda environment.  </li> </ul>"},{"location":"eDNA%20COI%20metab/01-Metabarcoding%20ampliseq%20COI/#step-4-downloading-and-updating-reference-databases","title":"Step 4: Downloading and updating reference databases","text":""},{"location":"eDNA%20COI%20metab/01-Metabarcoding%20ampliseq%20COI/#download-andor-update-bold-database","title":"Download and/or update BOLD database","text":"<p>Visit the Figshare cite for v4 and check for any latest versions. If a new version is available, download the COI references sequences from this webpage: bold_clustered.assignTaxonomy.fasta.gz and bold_clustered.addSpecies.fasta.gz. Via NU Cluster OOD, upload these files to <code>/work/gmgi/databases/COI/BOLD</code>. </p> <p>I downloaded <code>taxref_reformat_coidb.sh</code> from the ampliseq github page. I then changed the first line to be only .gz files (<code>for f in *.gz; do</code> instead of <code>for f in $(ls); do</code>)</p> <pre><code># Start on a computing node \nsrun --pty bash \n\n# Activate conda environment\nsource /work/gmgi/miniconda3/bin/activate fisheries_eDNA\n\ncd /work/gmgi/databases/COI/BOLD \n## upload new .gz files via OOD \n\n# Edit names to reflect the version #\nmv bold_clustered.addspecies.fasta.gz bold_v4_clustered.addspecies.fasta.gz                                                                                           \nmv bold_clustered.assigntaxonomy.fasta.gz bold_v4_clustered.assigntaxonomy.fasta.gz \n\nbash taxref_reformat_coidb.sh\n</code></pre> <p>These resulting files <code>addSpecies.fna</code> and <code>assignTaxonomy.fna</code> will be fed into the ampliseq script below.</p>"},{"location":"eDNA%20COI%20metab/01-Metabarcoding%20ampliseq%20COI/#download-andor-update-nbci-blast-nt-database","title":"Download and/or update NBCI blast nt database","text":"<p>NCBI is updated daily and therefore needs to be updated each time a project is analyzed. This is the not the most ideal method but we were struggling to get the <code>-remote</code> flag to work within slurm because I don't think NU slurm is connected to the internet? NU help desk was helping for awhile but we didn't get anywhere.</p> <p>Within <code>/work/gmgi/databases/ncbi</code>, there is a <code>update_nt.sh</code> script with the following code. To run <code>sbatch update_nt.sh</code>. This won't take long as it will check for updates rather than re-downloading every time. </p> <pre><code>#!/bin/bash\n#SBATCH --partition=short\n#SBATCH --nodes=1\n#SBATCH --time=24:00:00\n#SBATCH --job-name=update_ncbi_nt\n#SBATCH --mem=50G\n#SBATCH --output=%x_%j.out\n#SBATCH --error=%x_%j.err\n\n# Activate conda environment\nsource /work/gmgi/miniconda3/bin/activate fisheries_eDNA\n\n# Create output directory if it doesn't exist\ncd /work/gmgi/databases/ncbi/nt\n\n# Update BLAST nt database\nupdate_blastdb.pl --decompress nt\n\n# Print completion message\necho \"BLAST nt database update completed\"\n</code></pre> <p>View the <code>update_ncbi_nt.out</code> file to confirm the echo printed at the end.</p> <p>Emma is still troubleshooting the -remote flag to also avoid storing the nt db within our /work/gmgi folder. </p>"},{"location":"eDNA%20COI%20metab/01-Metabarcoding%20ampliseq%20COI/#other-options","title":"Other options","text":"<ul> <li>COInr: downloads NCBI and BOLD (Barcode of Life Database) databases to create one database for comparison. COInr is already downloaded in the conda environment and pulls NCBI And BOLD directly.  </li> <li>MARES (MARine Eukaryote Species) and github: This database combines sequences from both GenBank and BOLD to increase taxonomic coverage and confidence for marine eukaryotes. MARES Github repo. Paper link.     </li> <li>MIDORI: A database specifically for COI sequences. MIDORI Reference pulls from GenBank.    </li> </ul> <p>COInr</p> <p></p> <p>COInr database instructions. There are options to include custom sequences if needed.</p> <p>The latest version of BOLD is 2015 so this 2022 set is the most updated. Use our own NCBI as well to catch recent entries. </p> <pre><code>cd /work/gmgi/packages \ngit clone https://github.com/meglecz/mkCOInr.git\n\ncd /work/gmgi/databases/COI\nwget https://zenodo.org/record/6555985/files/COInr_2022_05_06.tar.gz\ntar -zxvf COInr_2022_05_06.tar.gz\nrm COInr_2022_05_06.tar.gz\nmv COInr_2022_05_06 COInr\n\n## converting database information for blast \nperl /work/gmgi/packages/mkCOInr/scripts/format_db.pl -tsv COInr/COInr.tsv -outfmt blast -outdir /work/gmgi/databases/COI/COInr -out COInr_blast\n\n## creating list of sseqID and taxIDs for R df step \nawk '{print $1 \"\\t\" $2}' COInr.tsv &gt; COInr_taxIDlist.tsv\n</code></pre> <p>MIDORI</p> <p>Visit the MIDORI website to check for the most updated db. This folder is already formatted for blast searching so we don't need to create a blast formatted db. </p> <pre><code>cd /work/gmgi/databases/COI/MIDORI\n\n## download zip file from MIDORI website for CO1 sequences in BLAST format from nucleotide reference\nwget https://www.reference-midori.info/download/Databases/GenBank261_2024-06-15/BLAST/uniq/fasta/MIDORI2_UNIQ_NUC_GB261_CO1_BLAST.fasta.zip\nunzip MIDORI2_UNIQ_NUC_GB261_CO1_BLAST.fasta.zip \n\n## change notation if version is different \nmakeblastdb -in MIDORI2_UNIQ_NUC_GB261_CO1_BLAST.fasta -dbtype nucl -out MIDORI2_UNIQ_NUC_GB261_CO1_BLAST.fasta\n</code></pre>"},{"location":"eDNA%20COI%20metab/01-Metabarcoding%20ampliseq%20COI/#step-4-nf-coreampliseq","title":"Step 4: nf-core/ampliseq","text":"<p>Nf-core: A community effort to collect a curated set of analysis pipelines built using Nextflow. Nextflow: scalable and reproducible scientific workflows using software containers, used to build wrapper programs like the one we use here.  </p> <p>[https://nf-co.re/ampliseq/2.11.0]: nfcore/ampliseq is a bioinformatics analysis pipeline used for amplicon sequencing, supporting denoising of any amplicon and supports a variety of taxonomic databases for taxonomic assignment including 16S, ITS, CO1 and 18S. </p> <p></p> <p>We use ampliseq for the following programs:  </p> <ul> <li>Cutadapt is trimming primer sequences from sequencing reads. Primer sequences are non-biological sequences that often introduce point mutations that do not reflect sample sequences. This is especially true for degenerated PCR primer. If primer trimming would be omitted, artifactual amplicon sequence variants might be computed by the denoising tool or sequences might be lost due to become labelled as PCR chimera.  </li> <li>DADA2 performs fast and accurate sample inference from amplicon data with single-nucleotide resolution. It infers exact amplicon sequence variants (ASVs) from amplicon data with fewer false positives than many other methods while maintaining high sensitivity.   </li> </ul> <p>We skip the taxonomic assignment because we use 3-db approach described in the next section. </p> <p>Should we try BOLD through ampliseq?</p>"},{"location":"eDNA%20COI%20metab/01-Metabarcoding%20ampliseq%20COI/#coi-primer-sequences-required","title":"COI primer sequences (required)","text":"<p>Below is what we used for COI amplicon sequencing. This results in ~313 bp expected ASV. </p> <p>LG COI amplicon F: GGWACWGGWTGAACWGTWTAYCCYCC     LG COI amplicon R: TAIACYTCIGGRTGICCRAARAAYCA       </p> <p>Ampliseq will automatically calculate the reverse compliment and include this for us.</p>"},{"location":"eDNA%20COI%20metab/01-Metabarcoding%20ampliseq%20COI/#metadata-sheet-optional","title":"Metadata sheet (optional)","text":"<p>The metadata file has to follow the QIIME2 specifications. Below is a preview of the sample sheet used for this test. Keep the column headers the same for future use. The first column needs to be \"ID\" and can only contain numbers, letters, or \"-\". This is different than the sample sheet. NAs should be empty cells rather than \"NA\". </p>"},{"location":"eDNA%20COI%20metab/01-Metabarcoding%20ampliseq%20COI/#create-samplesheet-sheet-for-ampliseq","title":"Create samplesheet sheet for ampliseq","text":"<p>This file indicates the sample ID and the path to R1 and R2 files. Below is a preview of the sample sheet used in this test. File created on RStudio Interactive on Discovery Cluster using (<code>create_metadatasheets.R</code>).  </p> <ul> <li>sampleID (required): Unique sample IDs, must start with a letter, and can only contain letters, numbers or underscores (no hyphons!).  </li> <li>forwardReads (required): Paths to (forward) reads zipped FastQ files  </li> <li>reverseReads (optional): Paths to reverse reads zipped FastQ files, required if the data is paired-end  </li> <li>run (optional): If the data was produced by multiple sequencing runs, any string  </li> </ul> sampleID forwardReads reverseReads run sample1 ./data/S1_R1_001.fastq.gz ./data/S1_R2_001.fastq.gz A sample2 ./data/S2_fw.fastq.gz ./data/S2_rv.fastq.gz A sample3 ./S4x.fastq.gz ./S4y.fastq.gz B sample4 ./a.fastq.gz ./b.fastq.gz B <p>This is an R script, not slurm script. Open RStudio interactive on Discovery Cluster to run this script.</p> <p>Prior to running R script, use the <code>rawdata</code> file created for the fastqc slurm array from within the raw data folder to create a list of files. Below is an example from our Offshore Wind project but the specifics of the sampleID will be project dependent. This project had four sequencing runs with different file names. </p> <p><code>01a-metadata.R</code></p> <pre><code>## Load libraries \n\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(strex) \n\n### Read in sample sheet \n\nsample_list &lt;- read.delim2(\"/work/gmgi/Fisheries/eDNA/offshore_wind2023/raw_data/rawdata\", header=F) %&gt;% \n  dplyr::rename(forwardReads = V1) %&gt;%\n  mutate(sampleID = str_after_nth(forwardReads, \"data/\", 1),\n         sampleID = str_before_nth(sampleID, \"_R\", 1))\n\n# creating sample ID \nsample_list$sampleID &lt;- gsub(\"-\", \"_\", sample_list$sampleID)\n\n# keeping only rows with R1\nsample_list &lt;- filter(sample_list, grepl(\"R1\", forwardReads, ignore.case = TRUE))\n\n# duplicating column \nsample_list$reverseReads &lt;- sample_list$forwardReads\n\n# replacing R1 with R2 in only one column \nsample_list$reverseReads &lt;- gsub(\"R1\", \"R2\", sample_list$reverseReads)\n\n# rearranging columns \nsample_list &lt;- sample_list[,c(2,1,3)]\n\nsample_list %&gt;% write.csv(\"/work/gmgi/Fisheries/eDNA/offshore_wind2023/metadata/samplesheet.csv\", \n                          row.names=FALSE, quote = FALSE)\n</code></pre>"},{"location":"eDNA%20COI%20metab/01-Metabarcoding%20ampliseq%20COI/#run-nf-coreampliseq-cutadapt-dada2","title":"Run nf-core/ampliseq (Cutadapt &amp; DADA2)","text":"<p>Update ampliseq workflow if needed: <code>nextflow pull nf-core/ampliseq</code>. </p> <p>Testing this on OSW work first. </p> <p><code>01b-ampliseq.sh</code>:</p> <pre><code>#!/bin/bash\n#SBATCH --error=output/\"%x_error.%j\" #if your job fails, the error report will be put in this file\n#SBATCH --output=output/\"%x_output.%j\" #once your job is completed, any final job report comments will be put in this file\n#SBATCH --partition=short\n#SBATCH --nodes=1\n#SBATCH --time=20:00:00\n#SBATCH --job-name=ampliseq\n#SBATCH --mem=70GB\n#SBATCH --ntasks=24\n#SBATCH --cpus-per-task=2\n\n### USER TO-DO ### \n## 1. Set paths for project \n## 2. Adjust SBATCH options above (time, mem, ntasks, etc.) as desired  \n## 3. Fill in F primer information based on primer type (no reverse compliment needed)\n## 4. Adjust parameters as needed (below is Fisheries team default for COI)\n\n# LOAD MODULES\nmodule load singularity/3.10.3\nmodule load nextflow/23.10.1\n\n# SET PATHS \nmetadata=\"/work/gmgi/Fisheries/eDNA/offshore_wind/invertebrate/metadata\" \noutput_dir=\"/work/gmgi/Fisheries/eDNA/offshore_wind/invertebrate/ampliseq_COIdb_results\"\naddSpecies=\"/work/gmgi/databases/COI/BOLD/addSpecies.fna\"\nassignTaxonomy=\"/work/gmgi/databases/COI/BOLD/assignTaxonomy.fna\"\n\nnextflow run nf-core/ampliseq -resume \\\n   -profile singularity \\\n   --input ${metadata}/samplesheet.csv \\\n   --FW_primer \"GGWACWGGWTGAACWGTWTAYCCYCC\" \\\n   --RV_primer \"TAIACYTCIGGRTGICCRAARAAYCA\" \\\n   --outdir ${output_dir} \\\n   --trunclenf 220 \\\n   --trunclenr 220 \\\n   --trunc_qmin 25 \\\n   --max_ee 2 \\\n   --min_len_asv 300 \\\n   --max_len_asv 330 \\\n   --dada_ref_tax_custom ${assignTaxonomy} \\\n   --dada_ref_tax_custom_sp ${addSpecies} \\\n   --dada2_addspecies_allowmultiple TRUE \\\n   --sample_inference pseudo \\\n   --ignore_failed_trimming\n</code></pre> <p>Could add back in?     --max_len 200 \\    --skip_taxonomy </p> <p>From zach paper: removing the co-amplified putative nuclear mitochondrial  pseudogenes (NUMTs) is highly recommended (Creedy et al., 2022;  Porter &amp; Hajibabaei, 2021; Song et al., 2008). - MetaWorks  and VTAM implement a step of removing putative NUMTs OR multisample features matrix may be processed with metaMATE  (And\u00fajar et al., 2021) to remove putative NUMTs and other erroneous sequences (based on, e.g., length and relative read abundance)</p> <p>DnoisE program - https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-021-04115-6</p> <p>python package called BOLDigger has been developed to help automate batch query submissions to the BOLD identification engine and can be used to identify COI, ITS, rbcL, and matK sequences (Buchner and Leese, 2020)</p> <p>In addition to the potential of amino acid translation, the protein coding nature of COI leads to relatively stricter expectations of amplicon length</p> <p>Use QIIME2 and trained classifier?</p> <p>To run:  - <code>sbatch 01b-ampliseq.sh</code> </p>"},{"location":"eDNA%20COI%20metab/01-Metabarcoding%20ampliseq%20COI/#files-generated-by-ampliseq","title":"Files generated by ampliseq","text":"<p>Pipeline summary reports:  </p> <ul> <li><code>summary_report/</code></li> <li><code>summary_report.html</code>: pipeline summary report as standalone HTML file that can be viewed in your web browser.</li> <li><code>*.svg*</code>: plots that were produced for (and are included in) the report.</li> <li><code>versions.yml</code>: software versions used to produce this report.</li> </ul> <p>Preprocessing:  </p> <ul> <li>FastQC: <code>fastqc/</code> and <code>*_fastqc.html</code>: FastQC report containing quality metrics for your untrimmed raw fastq files.  </li> <li>Cutadapt: <code>cutadapt/</code> and <code>cutadapt_summary.tsv</code>: summary of read numbers that pass cutadapt  </li> <li>MultiQC: <code>multiqc</code>, <code>multiqc_data/</code>, <code>multiqc_plots/</code> with <code>multiqc_report.html</code>: a standalone HTML file that can be viewed in your web browser; </li> </ul> <p>ASV inferrence with DADA2:  </p> <ul> <li><code>dada2/</code>, <code>dada2/args/</code>, <code>data2/log/</code> </li> <li><code>ASV_seqs.fasta</code>: Fasta file with ASV sequences.</li> <li><code>ASV_table.tsv</code>: Counts for each ASV sequence.</li> <li><code>DADA2_stats.tsv</code>: Tracking read numbers through DADA2 processing steps, for each sample.</li> <li><code>DADA2_table.rds</code>: DADA2 ASV table as R object.</li> <li><code>DADA2_table.tsv</code>: DADA2 ASV table.  </li> <li><code>dada2/QC/</code></li> <li><code>*.err.convergence.txt</code>: Convergence values for DADA2's dada command, should reduce over several magnitudes and approaching 0.  </li> <li><code>*.err.pdf</code>: Estimated error rates for each possible transition. The black line shows the estimated error rates after convergence of the machine-learning algorithm. The red line shows the error rates expected under the nominal definition of the Q-score. The estimated error rates (black line) should be a good fit to the observed rates (points), and the error rates should drop with increased quality.  </li> <li><code>*_qual_stats.pdf</code>: Overall read quality profiles: heat map of the frequency of each quality score at each base position. The mean quality score at each position is shown by the green line, and the quartiles of the quality score distribution by the orange lines. The red line shows the scaled proportion of reads that extend to at least that position.  </li> <li><code>*_preprocessed_qual_stats.pdf</code>: Same as above, but after preprocessing.  </li> </ul> <p>We add an ASV length filter that will output <code>asv_length_filter/</code> with:  </p> <ul> <li><code>ASV_seqs.len.fasta</code>: Fasta file with filtered ASV sequences.  </li> <li><code>ASV_table.len.tsv</code>: Counts for each filtered ASV sequence.  </li> <li><code>ASV_len_orig.tsv</code>: ASV length distribution before filtering.  </li> <li><code>ASV_len_filt.tsv</code>: ASV length distribution after filtering.  </li> <li><code>stats.len.tsv</code>: Tracking read numbers through filtering, for each sample.  </li> </ul>"},{"location":"eDNA%20COI%20metab/01-Metabarcoding%20ampliseq%20COI/#step-5-running-additional-taxonomic-assignment-id-script","title":"Step 5: Running additional taxonomic assignment ID script","text":"<p><code>02-taxonomicID.sh</code>: </p> <pre><code>#!/bin/bash\n#SBATCH --error=output/\"%x_error.%j\" #if your job fails, the error report will be put in this file\n#SBATCH --output=output/\"%x_output.%j\" #once your job is completed, any final job report comments will be put in this file\n#SBATCH --partition=short\n#SBATCH --nodes=1\n#SBATCH --time=20:00:00\n#SBATCH --job-name=tax_ID\n#SBATCH --mem=30GB\n#SBATCH --ntasks=24\n#SBATCH --cpus-per-task=2\n\n### USER TO-DO ### \n## 1. Set paths for project; change db path if not 12S\n\n# Activate conda environment\nsource /work/gmgi/miniconda3/bin/activate fisheries_eDNA\n\n# SET PATHS \nASV_fasta=\"\"\nout=\"\"\n\nCOInr=\"/work/gmgi/databases/COI/COInr\"\nmidori=\"/work/gmgi/databases/COI/MIDORI\"\nncbi=\"/work/gmgi/databases/ncbi/nt\"\ntaxonkit=\"/work/gmgi/databases/taxonkit\"\n\n#### DATABASE QUERY ####\n### NCBI database \nblastn -db ${ncbi}/\"nt\" \\\n   -query ${ASV_fasta}/ASV_seqs.len.fasta \\\n   -out ${out}/BLASTResults_NCBI.txt \\\n   -max_target_seqs 20 -perc_identity 99 -qcov_hsp_perc 95 \\\n   -outfmt '6  qseqid   sseqid   sscinames   staxid pident   length   mismatch gapopen  qstart   qend  sstart   send  evalue   bitscore'\n\n## MIDORI database \nblastn -db ${midori}/*.fasta\" \\\n   -query ${ASV_fasta}/ASV_seqs.len.fasta \\\n   -out ${out}/BLASTResults_midori.txt \\\n   -max_target_seqs 10 -perc_identity 98 -qcov_hsp_perc 95 \\\n   -outfmt '6  qseqid   sseqid  pident   length   mismatch gapopen  qstart   qend  sstart   send  evalue   bitscore'\n\n############################\n\n#### TAXONOMIC CLASSIFICATION #### \n## creating list of staxids from all three files \nawk -F $'\\t' '{ print $4}' ${out}/BLASTResults_NCBI.txt | sort -u &gt; ${out}/NCBI_sp.txt\n\n## annotating taxid with full taxonomic classification\ncat ${out}/NCBI_sp.txt | ${taxonkit}/taxonkit reformat -I 1 -r \"Unassigned\" &gt; ${out}/NCBI_taxassigned.txt\n</code></pre> <p>To run: - <code>sbatch 02-taxonomicID.sh</code> </p>"},{"location":"eDNA%20qPCR/qPCR-analysis/","title":"qPCR data sheets","text":"<p>.Rmd script</p>"},{"location":"eDNA%20qPCR/qPCR-analysis/#load-libraries","title":"Load libraries","text":"<pre><code>library(ggplot2) ## for plotting\nlibrary(tidyverse) ## for data table manipulation\n</code></pre> <pre><code>## \u2500\u2500 Attaching core tidyverse packages \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 tidyverse 2.0.0 \u2500\u2500\n## \u2714 dplyr     1.1.4     \u2714 readr     2.1.5\n## \u2714 forcats   1.0.0     \u2714 stringr   1.5.1\n## \u2714 lubridate 1.9.3     \u2714 tibble    3.2.1\n## \u2714 purrr     1.0.2     \u2714 tidyr     1.3.1\n## \u2500\u2500 Conflicts \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 tidyverse_conflicts() \u2500\u2500\n## \u2716 dplyr::filter() masks stats::filter()\n## \u2716 dplyr::lag()    masks stats::lag()\n## \u2139 Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n</code></pre> <pre><code>library(readr) ## for reading in tsv files\nlibrary(readxl) ## for reading in excel files\nlibrary(stringr) ## for data transformation\nlibrary(strex) ## for data transformation\nlibrary(writexl) ## for excel output\nlibrary(purrr) ## for data transformation\nlibrary(ggrepel)  ## For geom_text_repel\nlibrary(drc) ## for LOD calculations\n</code></pre> <pre><code>## Loading required package: MASS\n## \n## Attaching package: 'MASS'\n## \n## The following object is masked from 'package:dplyr':\n## \n##     select\n## \n## \n## 'drc' has been loaded.\n## \n## Please cite R and 'drc' if used for a publication,\n## for references type 'citation()' and 'citation('drc')'.\n## \n## \n## Attaching package: 'drc'\n## \n## The following objects are masked from 'package:stats':\n## \n##     gaussian, getInitial\n</code></pre> <pre><code>library(ggpubr)\n</code></pre>"},{"location":"eDNA%20qPCR/qPCR-analysis/#evaluating-standard-curve","title":"Evaluating standard curve","text":""},{"location":"eDNA%20qPCR/qPCR-analysis/#reading-in-data","title":"Reading in data","text":"<pre><code>### USER EDITS:\n## 1. Replace path of file with your own standard curve\nstd_curve &lt;- read.csv(\"example standard curve.csv\", skip = 19) %&gt;% dplyr::rename(St_Quant=7) %&gt;%\n\n  ## Calculate mean Cq\n  dplyr::group_by(Sample) %&gt;%\n  mutate(Cq_mean = mean(Cq, na.rm=TRUE))\n</code></pre> <p>Calculating standard curve metrics</p> <pre><code># Fit a linear model to calculate slope and intercept\nlinear_model &lt;- lm(Cq_mean ~ log10(St_Quant), data = std_curve)\n\n# Extract slope and y-intercept\nslope &lt;- coef(linear_model)[2]       # Slope (m)\ny_intercept &lt;- coef(linear_model)[1] # Y-intercept (b)\nefficiency &lt;- (-1+(10^(-1/slope)))\n\n## Standard curve calculations (assuming std_curve is already grouped by Sample)\nstd_curve &lt;- std_curve %&gt;%\n\n  ## calculate detection rates (number of Cqs present / number of replicates)\n  mutate(detection_rate = sum(!is.na(Cq)) / n(),\n\n  ## calculate Cq stats - SD, and CV\n         Cq_sd = sd(Cq, na.rm=TRUE),\n         #Cq_cv = (Cq_sd / Cq_mean) * 100 ## old method \n         Cq_cv = sqrt(((1+efficiency) ^ ((Cq_sd^2) * log(1+efficiency))) - 1)) %&gt;%\n  ungroup()\n</code></pre> <p>Confirming negatives were clean and filter them out</p> <pre><code>## all output should have NAs in Cq column\n## if any negatives have values, then pause and re-run those samples if needed\nstd_curve %&gt;% filter(Sample == \"neg\")\n</code></pre> <pre><code>## # A tibble: 3 \u00d7 11\n##   Well  Fluor Target  Content Sample    Cq St_Quant Cq_mean detection_rate Cq_sd\n##   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;  &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;          &lt;dbl&gt; &lt;dbl&gt;\n## 1 F12   SYBR  Library NTC     neg       NA       NA     NaN              0    NA\n## 2 G12   SYBR  Library NTC     neg       NA       NA     NaN              0    NA\n## 3 H12   SYBR  Library NTC     neg       NA       NA     NaN              0    NA\n## # \u2139 1 more variable: Cq_cv &lt;dbl&gt;\n</code></pre> <pre><code>std_curve &lt;- std_curve %&gt;% filter(!Sample == \"neg\")\n</code></pre>"},{"location":"eDNA%20qPCR/qPCR-analysis/#calculating-lod-and-loq","title":"Calculating LOD and LOQ","text":"<p>Making list of standard curve quantities</p> <pre><code># Assuming std_curve is your data frame\nquants &lt;- std_curve %&gt;% dplyr::select(St_Quant) %&gt;% distinct() %&gt;% arrange(desc(St_Quant))\n\n# Creating a factored list of these standard curve values\nquant_list &lt;- as.factor(quants$St_Quant)\n</code></pre> <p>Find the Limit of Detection (LOD)</p> <pre><code># Find the first value below the 0.95 detection rate\nfirst_below_LOD_threshold &lt;- std_curve %&gt;% filter(detection_rate &lt; 0.95) %&gt;% \n  slice(1) %&gt;% dplyr::select(St_Quant)\n\n# Get the standard quantity of the first value below the threshold\nbelow_threshold_LOD_quant &lt;- first_below_LOD_threshold$St_Quant[1]\n\n# Find the index of this quantity in the ordered list\nLOD_index &lt;- which(quants$St_Quant == below_threshold_LOD_quant)\n\nLOD &lt;- quants$St_Quant[LOD_index - 1]\n</code></pre> <p>Find the Limit of Quantification (LOQ)</p> <pre><code># Find the first value below the 0.35 CV\nfirst_above_LOQ_threshold &lt;- std_curve %&gt;% filter(Cq_cv &gt; 0.350) %&gt;% \n  slice(1) %&gt;% dplyr::select(St_Quant)\n\n# Get the standard quantity of the first value below the threshold\nabove_threshold_LOQ_quant &lt;- first_above_LOQ_threshold$St_Quant[1]\n\n# Find the index of this quantity in the ordered list\nLOQ_index &lt;- which(quants$St_Quant == above_threshold_LOQ_quant)\n\nLOQ &lt;- quants$St_Quant[LOQ_index - 1]\n</code></pre>"},{"location":"eDNA%20qPCR/qPCR-analysis/#plotting-standard-curve","title":"Plotting standard curve","text":"<pre><code>std_curve %&gt;% \n  ## Taking only those detections with &gt;50% to make curve \n  filter(detection_rate &gt; 0.50) %&gt;%\n\n  ## Plotting that std. curve \n  ggplot(., aes(x = log10(St_Quant), y = Cq)) +\n\n  geom_smooth(method = 'lm', se = FALSE, color = \"darkred\") +\n\n  stat_regline_equation(size = 4, color = 'darkred',\n                        aes(label = after_stat(rr.label)), hjust=1,\n                        label.x.npc = \"right\", label.y.npc = \"top\") +\n\n  geom_point(color = 'black', fill = 'white', shape = 21, size = 3.5, alpha=0.8) +\n\n  ## ADDING LOD LINE -- if no line shows up, all st. curve is qualitative \n  geom_vline(xintercept=log10(LOD), linetype=\"dashed\", color = \"grey40\") +\n  geom_text(aes(x = log10(LOD), y = max(std_curve$Cq, na.rm=TRUE) * 0.3,\n                label = paste(\"LOD =\", LOD, \"copies\")), \n            angle = 90, vjust = -0.5, hjust = -0.1, color = \"grey40\", size = 4) +\n\n  ## ADDING LOQ LINE -- if no line shows up, all st. curve is quantitative \n  geom_vline(xintercept=log10(LOQ), linetype=\"dashed\", color = \"grey40\") +\n  geom_text(aes(x = log10(LOQ), y = max(std_curve$Cq, na.rm=TRUE) * 0.3, \n                label = paste(\"LOQ =\", LOQ, \"copies\")), \n            angle = 90, vjust = -0.5, hjust = -0.1, color = \"grey40\", size = 4) +\n\n  theme_bw() +\n\n  labs(\n    x = \"Normalized standard concentrations\",\n    y = \"Cq\"\n  ) +\n\n  theme(axis.text.y = element_text(size=10, color=\"grey20\"),\n        axis.text.x = element_text(size=10, color=\"grey20\"),\n        axis.title.y = element_text(margin = margin(t = 0, r = 10, b = 0, l = 0), size=12, face=\"bold\"),\n        axis.title.x = element_text(margin = margin(t = 10, r = 0, b = 0, l = 0), size=12, face=\"bold\"))\n</code></pre> <pre><code>## Warning: Use of `std_curve$Cq` is discouraged.\n## \u2139 Use `Cq` instead.\n\n## Warning in geom_text(aes(x = log10(LOD), y = max(std_curve$Cq, na.rm = TRUE) * : All aesthetics have length 1, but the data has 93 rows.\n## \u2139 Please consider using `annotate()` or provide this layer with data containing\n##   a single row.\n\n## Warning: Use of `std_curve$Cq` is discouraged.\n## \u2139 Use `Cq` instead.\n\n## Warning in geom_text(aes(x = log10(LOQ), y = max(std_curve$Cq, na.rm = TRUE) * : All aesthetics have length 1, but the data has 93 rows.\n## \u2139 Please consider using `annotate()` or provide this layer with data containing\n##   a single row.\n\n## `geom_smooth()` using formula = 'y ~ x'\n\n## Warning: Removed 2 rows containing non-finite outside the scale range\n## (`stat_smooth()`).\n\n## Warning: Removed 2 rows containing non-finite outside the scale range\n## (`stat_regline_equation()`).\n\n## Warning: Removed 2 rows containing missing values or values outside the scale range\n## (`geom_point()`).\n</code></pre> <pre><code>ggsave(\"example output/Standard_curve.png\", width=6, height=4)\n</code></pre> <pre><code>## Warning: Use of `std_curve$Cq` is discouraged.\n## \u2139 Use `Cq` instead.\n\n## Warning in geom_text(aes(x = log10(LOD), y = max(std_curve$Cq, na.rm = TRUE) * : All aesthetics have length 1, but the data has 93 rows.\n## \u2139 Please consider using `annotate()` or provide this layer with data containing\n##   a single row.\n\n## Warning: Use of `std_curve$Cq` is discouraged.\n## \u2139 Use `Cq` instead.\n\n## Warning in geom_text(aes(x = log10(LOQ), y = max(std_curve$Cq, na.rm = TRUE) * : All aesthetics have length 1, but the data has 93 rows.\n## \u2139 Please consider using `annotate()` or provide this layer with data containing\n##   a single row.\n\n## `geom_smooth()` using formula = 'y ~ x'\n\n## Warning: Removed 2 rows containing non-finite outside the scale range\n## (`stat_smooth()`).\n\n## Warning: Removed 2 rows containing non-finite outside the scale range\n## (`stat_regline_equation()`).\n\n## Warning: Removed 2 rows containing missing values or values outside the scale range\n## (`geom_point()`).\n</code></pre>"},{"location":"eDNA%20qPCR/qPCR-analysis/#calculating-slope-and-y-intercept","title":"Calculating slope and y-intercept","text":"<p>These values get fed in later in data calculations.</p> <pre><code># Fit a linear model to calculate slope and intercept\nlinear_model &lt;- lm(Cq_mean ~ log10(St_Quant), data = std_curve)\n\n# Extract slope and y-intercept\nslope &lt;- coef(linear_model)[2]      # Slope (m)\ny_intercept &lt;- coef(linear_model)[1] # Y-intercept (b)\n</code></pre>"},{"location":"eDNA%20qPCR/qPCR-analysis/#reading-in-datafiles-and-merging-into-one-large-dataframe","title":"Reading in datafiles and merging into one large dataframe","text":"<pre><code>### USER EDITS:\n## 1. Replace path of files \n## 2. In str_after_nth(file.ID, \"results/\", 1), make sure this matches the folder ending referenced in list.files\n\ndf &lt;- \n  # list files in directory following a particular pattern\n  list.files(path = 'example input/qPCR_results/', pattern = \".csv\", full.names = TRUE) %&gt;%\n\n  # get the column names\n  set_names(.) %&gt;% \n\n  # join all files together in one data frame by file ID, skipping first 19 rows\n  map_dfr(~read.csv(., skip = 19), .id = \"file.ID\") %&gt;% \n\n  # turn file.ID into just plate information (plate.ID)\n  mutate(file.ID = str_after_nth(file.ID, \"results/\", 1),\n         file.ID = str_before_nth(file.ID, \".csv\", 1)) %&gt;%\n  dplyr::rename(plate.ID = file.ID, Sample_ID = Sample) %&gt;%\n\n  ## RI STRIPED BASS SPECIFIC CODE FOR SAMPLE ID MISMATCHES\n  mutate(Sample_ID = gsub(\" #1\", \"_1\", Sample_ID),\n         Sample_ID = gsub(\" #2\", \"_2\", Sample_ID)) %&gt;%\n\n  mutate(Sample_ID = case_when(\n    str_detect(Well, \"4\") &amp; Sample_ID == \"2/21/24_ONE_NTR\" ~ paste0(Sample_ID, \"_1\"),\n    str_detect(Well, \"5\") &amp; Sample_ID == \"2/21/24_ONE_NTR\" ~ paste0(Sample_ID, \"_2\"),\n    TRUE ~ Sample_ID\n  ))\n\nhead(df)\n</code></pre> <pre><code>##              plate.ID Well Fluor  Target Content       Sample_ID    Cq\n## 1 26 AUG 2024 PLATE 1  A01  SYBR Library    Unkn 1/17/24_IPC_GCO    NA\n## 2 26 AUG 2024 PLATE 1  A02  SYBR Library    Unkn 1/17/24_IPC_BBC    NA\n## 3 26 AUG 2024 PLATE 1  A03  SYBR Library    Unkn 1/17/24_NAN_EDI    NA\n## 4 26 AUG 2024 PLATE 1  A04  SYBR Library    Unkn 1/17/24_EPS_DMF 34.79\n## 5 26 AUG 2024 PLATE 1  A05  SYBR Library    Unkn 1/17/24_WBJ_MHB    NA\n## 6 26 AUG 2024 PLATE 1  A06  SYBR Library    Unkn 1/17/24_NBT_SAK    NA\n##   Starting.Quantity..SQ.\n## 1                     NA\n## 2                     NA\n## 3                     NA\n## 4                     NA\n## 5                     NA\n## 6                     NA\n</code></pre>"},{"location":"eDNA%20qPCR/qPCR-analysis/#reading-in-meta-df","title":"Reading in meta df","text":"<p>Sample ID from meta needs to match the Sample ID from the qPCR output</p> <pre><code>meta &lt;- read_xlsx(\"example input/client_metadata_example.xlsx\") %&gt;%\n\n  ## RI STRIPED BASS SPECIFIC CODE FOR SAMPLE ID ISSUES\n  ## For other projects, address any Sample_ID differences (Sample_ID on qPCR output needs to match meta)\n  mutate(Sample_ID = gsub(\" #1\", \"_1\", Sample_ID),\n         Sample_ID = gsub(\" #2\", \"_2\", Sample_ID))\n</code></pre>"},{"location":"eDNA%20qPCR/qPCR-analysis/#evaluating-no-template-controls-ntcs-negatives","title":"Evaluating No Template Controls (NTCs; Negatives)","text":"<pre><code>NTC &lt;- df %&gt;% \n  filter(grepl(\"neg\", Sample_ID, ignore.case = TRUE))\n\n## Calculate number of negatives with Cq values \nsum(!is.na(NTC$Cq))\n</code></pre> <pre><code>## [1] 1\n</code></pre> <pre><code>## Calculate number of negatives total \nnrow(NTC)\n</code></pre> <pre><code>## [1] 120\n</code></pre> <pre><code>## Calculate number of plates\nlength(unique(NTC$plate.ID))\n</code></pre> <pre><code>## [1] 20\n</code></pre>"},{"location":"eDNA%20qPCR/qPCR-analysis/#spike-information","title":"Spike information","text":"<p>For each sample, calculate the distance from the Positive Spike-in value.</p> <pre><code>spike_samples &lt;- df %&gt;% \n  ## subset to spiked samples \n  filter(grepl(\"spike|pos\", Sample_ID, ignore.case = TRUE)) %&gt;%\n\n  ## remove spike from SampleID column only if 'pos' is NOT in the Sample_ID\n  mutate(Sample_ID = case_when(\n    !grepl(\"pos\", Sample_ID, ignore.case = TRUE) ~ str_before_nth(Sample_ID, \" spike\", 1),\n    TRUE ~ Sample_ID\n  )) %&gt;%\n\n  ## group by plate ID and sample ID \n  group_by(plate.ID, Sample_ID) %&gt;%\n\n  ## calculate mean of spikes per plate and sample \n  ## ungroup by Sample_ID so the next calculation is only done grouped by Plate \n  mutate(Filter_Cq = mean(Cq)) %&gt;% \n  ungroup(Sample_ID) %&gt;%\n\n  ## calculate difference from plate's Cq value \n  mutate(Cq_diff = Filter_Cq - Filter_Cq[grepl(\"pos\", Sample_ID, ignore.case = TRUE)]) %&gt;%\n  ungroup(); spike_samples\n</code></pre> <pre><code>## # A tibble: 480 \u00d7 10\n##    plate.ID    Well  Fluor Target Content Sample_ID    Cq Starting.Quantity..SQ.\n##    &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt; &lt;lgl&gt;                 \n##  1 26 AUG 202\u2026 G01   SYBR  Libra\u2026 Unkn    1/17/24_\u2026  10.5 NA                    \n##  2 26 AUG 202\u2026 G02   SYBR  Libra\u2026 Unkn    1/17/24_\u2026  10.6 NA                    \n##  3 26 AUG 202\u2026 G03   SYBR  Libra\u2026 Unkn    1/17/24_\u2026  10.8 NA                    \n##  4 26 AUG 202\u2026 G04   SYBR  Libra\u2026 Unkn    1/17/24_\u2026  10.8 NA                    \n##  5 26 AUG 202\u2026 G05   SYBR  Libra\u2026 Unkn    1/17/24_\u2026  10.5 NA                    \n##  6 26 AUG 202\u2026 G06   SYBR  Libra\u2026 Unkn    1/17/24_\u2026  11.0 NA                    \n##  7 26 AUG 202\u2026 G07   SYBR  Libra\u2026 Unkn    1/17/24_\u2026  10.8 NA                    \n##  8 26 AUG 202\u2026 G08   SYBR  Libra\u2026 Unkn    1/17/24_\u2026  11   NA                    \n##  9 26 AUG 202\u2026 G09   SYBR  Libra\u2026 Unkn    1/17/24_\u2026  10.7 NA                    \n## 10 26 AUG 202\u2026 G10   SYBR  Libra\u2026 Unkn    1/18/24_\u2026  10.9 NA                    \n## # \u2139 470 more rows\n## # \u2139 2 more variables: Filter_Cq &lt;dbl&gt;, Cq_diff &lt;dbl&gt;\n</code></pre> <pre><code>## Remove any lab error samples \n## RI Striped Bass only \n## removing 2/21/24 DI Blank 1 (pipetting issue) \nspike_samples &lt;- spike_samples %&gt;%\n  filter(!Sample_ID == \"2/21/24_Tap Blank_1\")\n</code></pre> <p>Plot those distance values and save to output folder</p> <pre><code># Create a jitter position object\njitter_pos &lt;- position_jitter(width = 0.45, seed = 123)\n\n## Plot samples \nspike_samples %&gt;% dplyr::select(Target, Sample_ID, Cq_diff) %&gt;% distinct() %&gt;%\n\n  ggplot(., aes(x=Target, y = Cq_diff)) + \n  geom_hline(yintercept = c(-2, 2), linetype = \"dotted\", color = \"grey50\", size=1.5) +\n\n  geom_jitter(aes(fill = abs(Cq_diff) &gt; 2), \n              size = 2, alpha = 0.5, color = 'black', shape = 21, #width = 0.45,\n              position = jitter_pos) +\n\n  scale_fill_manual(values = c(\"TRUE\" = \"red\", \"FALSE\" = \"white\")) +\n  geom_text_repel(\n    aes(label = ifelse(abs(Cq_diff) &gt; 2, Sample_ID, \"\")),  position = jitter_pos,\n    size = 3, box.padding = 0.5, point.padding = 0.2, force = 2\n  ) +\n\n  labs(\n    x = \"Sample\",\n    y = \"Distance from Positive Control\"\n  ) +\n\n  theme_bw() +\n\n  ## keep y axis at least -2,2 but extend to max \n  coord_cartesian(\n    ylim = c(\n      min(-2.5, min(spike_samples$Cq_diff, na.rm = TRUE)),\n      max(2.5, max(spike_samples$Cq_diff, na.rm = TRUE))\n    )) +\n\n  ## theme variables\n    theme(panel.background=element_rect(fill='white', colour='black'),\n        legend.position = \"none\",\n        axis.text.y = element_text(size=10, color=\"grey20\"),\n        axis.text.x = element_blank(),\n        axis.title.y = element_text(margin = margin(t = 0, r = 10, b = 0, l = 0), size=12, face=\"bold\"),\n        axis.title.x = element_text(margin = margin(t = 10, r = 0, b = 0, l = 0), size=12, face=\"bold\"))\n</code></pre> <pre><code>## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n## \u2139 Please use `linewidth` instead.\n## This warning is displayed once every 8 hours.\n## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was\n## generated.\n</code></pre> <p></p> <pre><code>ggsave(\"example output/Inhibition.png\", width=4, height=4)\n</code></pre>"},{"location":"eDNA%20qPCR/qPCR-analysis/#filter-data-cq-number-of-replicates-and-copy-num-calculations","title":"Filter data Cq, Number of Replicates, and Copy Num calculations","text":"<pre><code>## Complete calculations\nfilters_df &lt;- df %&gt;% \n  ## subset out the spiked samples \n  filter(!grepl(\"spike|pos|neg\", Sample_ID, ignore.case = TRUE)) %&gt;%\n\n  ## group by plate ID and sample ID \n  group_by(plate.ID, Sample_ID) %&gt;%\n\n  ## calculate mean of Cq per plate and sample, replacing NaN with NA\n  mutate(Filter_Cq = if_else(is.nan(mean(Cq, na.rm = TRUE)), NA_real_, mean(Cq, na.rm = TRUE))) %&gt;%\n\n  ## calculate # of replicates \n  mutate(Filter_Num_Replicates = sum(!is.na(Cq))) %&gt;%\n\n  ## calculate copy number \n  mutate(Filter_Copy_Num = 10^((Filter_Cq-y_intercept)/slope)) %&gt;%\n\n  ## summarize df with specific columns \n  dplyr::select(plate.ID, Sample_ID, Filter_Cq, Filter_Num_Replicates, Filter_Copy_Num) %&gt;% \n  distinct() %&gt;% ungroup()\n</code></pre>"},{"location":"eDNA%20qPCR/qPCR-analysis/#addressing-duplicate-samples-aka-re-runs","title":"Addressing duplicate samples (a.k.a., re-runs)","text":"<p>This code assumes the user wants to always use the most recent plate if completing a sample for a 2nd time. If this is not the case, chat with Fisheries team to make sure code reflect user\u2019s needs.</p> <pre><code>filters_df &lt;- filters_df %&gt;%\n  ## Separating Plate.ID into Date and Plate Number\n  separate(plate.ID, c(\"Plate_date\", \"Plate_number\"), sep = \" PLATE\") %&gt;%\n\n  ## Change Date column to a Date format \n  mutate(Plate_date = dmy(Plate_date)) %&gt;%\n\n  ## Group by Sample_ID and keep only the row with the most recent date\n  group_by(Sample_ID) %&gt;%\n  slice_max(Plate_date, n = 1, with_ties = FALSE) %&gt;% ungroup()\n\n## confirm the number of rows matches the number of unique Sample IDs (output should be TRUE)\nnrow(filters_df) == length(unique(filters_df$Sample_ID))\n</code></pre> <pre><code>## [1] TRUE\n</code></pre>"},{"location":"eDNA%20qPCR/qPCR-analysis/#combing-with-meta-and-collapsing-by-sample","title":"Combing with meta and collapsing by sample","text":"<pre><code>samples_df &lt;- filters_df %&gt;% right_join(meta, ., by = \"Sample_ID\") %&gt;%\n  group_by(Date, Sample_Location) %&gt;%\n\n  ## mutate to mean Ct value\n  mutate(`Mean Ct` = if_else(is.nan(mean(Filter_Cq, na.rm = TRUE)), \n                                    NA_real_, mean(Filter_Cq, na.rm = TRUE)),\n\n         ## take highest value of number of replicates\n         `Number of Replicates` = max(Filter_Num_Replicates, na.rm=TRUE),\n\n         ## sum of 2 copy number values \n         `Mean Copy Number` = ifelse(sum(Filter_Copy_Num, na.rm=TRUE) == 0, NA, \n                            sum(Filter_Copy_Num, na.rm=TRUE))\n           ) %&gt;%\n\n  ungroup() %&gt;%\n\n  ## summarizing df \n  dplyr::select(Date, Sample_Location, Sample_Type, Number_of_Filters, \n                `Mean Ct`, `Number of Replicates`, `Mean Copy Number`) %&gt;%\n  distinct() %&gt;%\n\n  ## adding new SampleID back in\n  unite(Sample_ID, Date, Sample_Location, sep = \" \", remove=F)\n\nhead(samples_df)\n</code></pre> <pre><code>## # A tibble: 6 \u00d7 8\n##   Sample_ID    Date                Sample_Location Sample_Type Number_of_Filters\n##   &lt;chr&gt;        &lt;dttm&gt;              &lt;chr&gt;           &lt;chr&gt;       &lt;lgl&gt;            \n## 1 2024-01-17 \u2026 2024-01-17 00:00:00 DI Blank #1     Blank       NA               \n## 2 2024-01-17 \u2026 2024-01-17 00:00:00 DI Blank #2     Blank       NA               \n## 3 2024-01-17 \u2026 2024-01-17 00:00:00 GBP_OCA         Field       NA               \n## 4 2024-01-17 \u2026 2024-01-17 00:00:00 IPC_GCO         Field       NA               \n## 5 2024-01-17 \u2026 2024-01-17 00:00:00 IPC_BBC         Field       NA               \n## 6 2024-01-17 \u2026 2024-01-17 00:00:00 NAN_EDI         Field       NA               \n## # \u2139 3 more variables: `Mean Ct` &lt;dbl&gt;, `Number of Replicates` &lt;int&gt;,\n## #   `Mean Copy Number` &lt;dbl&gt;\n</code></pre> <pre><code>## Confirm that the number of samples output from qPCR matches the number of samples in the metadata (output = TRUE)\nnrow(samples_df) == nrow(meta %&gt;% dplyr::select(-Sample_ID) %&gt;% distinct())\n</code></pre> <pre><code>## [1] TRUE\n</code></pre>"},{"location":"eDNA%20qPCR/qPCR-analysis/#normalizing-data","title":"Normalizing data","text":"<pre><code>## normalize data \nnormalized_df &lt;- samples_df %&gt;%\n  ## take log10 of copy number \n  mutate(`Mean Copy Number Normalized` = log10(`Mean Copy Number` + 1))\n\n## create an outlier cut-off \ncutoff &lt;- quantile(normalized_df$`Mean Copy Number Normalized`, na.rm = TRUE, probs=0.75) + \n  1.5*IQR(normalized_df$`Mean Copy Number Normalized`, na.rm=TRUE)\n\n## create an outlier cut-off \ncutoff_below &lt;- quantile(normalized_df$`Mean Copy Number Normalized`, na.rm = TRUE, probs=0.25) - \n  1.5*IQR(normalized_df$`Mean Copy Number Normalized`, na.rm=TRUE)\n\n## Output the values that outside the cutoff\nnormalized_df %&gt;% filter(`Mean Copy Number Normalized` &gt; cutoff) \n</code></pre> <pre><code>## # A tibble: 3 \u00d7 9\n##   Sample_ID    Date                Sample_Location Sample_Type Number_of_Filters\n##   &lt;chr&gt;        &lt;dttm&gt;              &lt;chr&gt;           &lt;chr&gt;       &lt;lgl&gt;            \n## 1 2024-01-18 \u2026 2024-01-18 00:00:00 POP_SEC         Field       NA               \n## 2 2024-01-18 \u2026 2024-01-18 00:00:00 POP_OUT         Field       NA               \n## 3 2024-01-18 \u2026 2024-01-18 00:00:00 PJP_CFL         Field       NA               \n## # \u2139 4 more variables: `Mean Ct` &lt;dbl&gt;, `Number of Replicates` &lt;int&gt;,\n## #   `Mean Copy Number` &lt;dbl&gt;, `Mean Copy Number Normalized` &lt;dbl&gt;\n</code></pre> <pre><code>normalized_df %&gt;% filter(`Mean Copy Number Normalized` &lt; cutoff_below) \n</code></pre> <pre><code>## # A tibble: 0 \u00d7 9\n## # \u2139 9 variables: Sample_ID &lt;chr&gt;, Date &lt;dttm&gt;, Sample_Location &lt;chr&gt;,\n## #   Sample_Type &lt;chr&gt;, Number_of_Filters &lt;lgl&gt;, Mean Ct &lt;dbl&gt;,\n## #   Number of Replicates &lt;int&gt;, Mean Copy Number &lt;dbl&gt;,\n## #   Mean Copy Number Normalized &lt;dbl&gt;\n</code></pre> <pre><code>## the cutoff we move forward with is the cutoff (high) but confirm no values are below the lower cutoff value \n</code></pre>"},{"location":"eDNA%20qPCR/qPCR-analysis/#adding-presentabsent-information","title":"Adding present/absent information","text":"<pre><code>normalized_df &lt;- normalized_df %&gt;%\n  mutate(Detection = case_when(\n    is.na(`Mean Copy Number Normalized`) ~ \"Absent\",\n    TRUE ~ \"Present\"\n  ))\n</code></pre>"},{"location":"eDNA%20qPCR/qPCR-analysis/#blank-specific-information","title":"Blank specific information","text":"<pre><code>blank_df &lt;- normalized_df %&gt;% subset(Sample_Type == \"Blank\")\n\ndetection_counts &lt;- blank_df %&gt;%\n  count(Detection) %&gt;%\n  mutate(percentage = n / sum(n) * 100,\n         label = paste0(Detection, \"\\n\", \"n=\", n, \"\\n\", round(percentage, 1), \"%\"))\n\nggplot(detection_counts, aes(x = \"\", y = n, fill = Detection)) +\n  geom_bar(stat = \"identity\", width = 1, color = \"black\", alpha=0.75) +\n  coord_polar(\"y\", start = 0) +\n  geom_text(aes(label = label), \n            position = position_stack(vjust = 0.5), \n            size = 5,\n            fontface = \"bold\") + \n  theme_void() +\n  theme(legend.position = \"none\",\n        #plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\")\n        ) +\n  scale_fill_manual(values = c(\"#e9ecef\", \"#669bbc\"))\n</code></pre> <pre><code>ggsave(\"example output/Blanks_piechart.png\", width=4, height=4)\n</code></pre>"},{"location":"eDNA%20qPCR/qPCR-analysis/#sample-preliminary-data","title":"Sample preliminary data","text":"<p>Pie chart</p> <pre><code>fieldsamples_df &lt;- normalized_df %&gt;% subset(Sample_Type == \"Field\")\n\nfield_detection_counts &lt;- fieldsamples_df %&gt;%\n  count(Detection) %&gt;%\n  mutate(percentage = n / sum(n) * 100,\n         label = paste0(Detection, \"\\n\", \"n=\", n, \"\\n\", round(percentage, 1), \"%\"))\n\nggplot(field_detection_counts, aes(x = \"\", y = n, fill = Detection)) +\n  geom_bar(stat = \"identity\", width = 1, color = \"black\", alpha=0.75) +\n  coord_polar(\"y\", start = 0) +\n  geom_text(aes(label = label), \n            position = position_stack(vjust = 0.5), \n            size = 5,\n            fontface = \"bold\") + \n  theme_void() +\n  theme(legend.position = \"none\",\n        #plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\")\n        ) +\n  scale_fill_manual(values = c(\"#e9ecef\", \"#669bbc\"))\n</code></pre> <p></p> <pre><code>ggsave(\"example output/Fields_piechart.png\", width=4, height=4)\n</code></pre> <p>Bar chart</p> <pre><code>fieldsamples_df %&gt;% subset(Detection == \"Present\") %&gt;%\n\n  ggplot(., aes(x=Sample_ID, y=`Mean Copy Number Normalized`)) + \n  geom_bar(stat = \"identity\", width = 0.7, alpha=0.5, fill = \"#669bbc\") +\n  labs(\n    y=\"Log10-Normalized Copy Number\",\n    x = \"Sample ID\",\n  ) +\n\n  ## Outlier line\n  geom_hline(yintercept = cutoff, color = \"darkred\", linetype = \"dashed\", size = 0.5) +\n  geom_text(aes(y = cutoff, x = 2, label = \"Outlier\"), hjust = 0.4, vjust=-0.5, color = \"grey40\", size = 3) +\n\n  ## LOD line\n  geom_hline(yintercept=log10(LOD), linetype=\"dashed\", color = \"grey40\") +\n  geom_text(aes(y = log10(LOD), x = 2, label = \"LOD\"), hjust = 0.5, vjust=-0.5, color = \"grey40\", size = 3) +\n\n  ## LOQ line\n  geom_hline(yintercept=log10(LOQ), linetype=\"dashed\", color = \"grey40\") +\n  geom_text(aes(y = log10(LOQ), x=2, label = \"LOQ\"), hjust = 0.5, vjust=-0.5, color = \"grey40\", size = 3) +\n\n  theme_bw() +\n    ## theme variables\n    theme(panel.background=element_rect(fill='white', colour='black'),\n        #legend.position = c(0.98, 0.98),  # This puts the legend in the top right corner\n        legend.position = \"right\",\n        legend.justification = c(1, 1),  # This aligns the legend to the top right\n        axis.text.y = element_text(size=8, color=\"grey20\"),\n        axis.text.x = element_text(size=6, color=\"grey20\", angle=90, vjust=0.5, hjust=1),\n        axis.title.y = element_text(margin = margin(t = 0, r = 10, b = 0, l = 0), size=10, face=\"bold\"),\n        axis.title.x = element_text(margin = margin(t = 10, r = 0, b = 0, l = 0), size=10, face=\"bold\"),\n        # New theme elements for strip appearance\n        strip.background = element_rect(fill = \"white\", color = \"black\"),\n        strip.text = element_text(face = \"bold\", size = 9)\n    )\n</code></pre> <pre><code>## Warning in geom_text(aes(y = cutoff, x = 2, label = \"Outlier\"), hjust = 0.4, : All aesthetics have length 1, but the data has 58 rows.\n## \u2139 Please consider using `annotate()` or provide this layer with data containing\n##   a single row.\n\n## Warning in geom_text(aes(y = log10(LOD), x = 2, label = \"LOD\"), hjust = 0.5, : All aesthetics have length 1, but the data has 58 rows.\n## \u2139 Please consider using `annotate()` or provide this layer with data containing\n##   a single row.\n\n## Warning in geom_text(aes(y = log10(LOQ), x = 2, label = \"LOQ\"), hjust = 0.5, : All aesthetics have length 1, but the data has 58 rows.\n## \u2139 Please consider using `annotate()` or provide this layer with data containing\n##   a single row.\n</code></pre> <p></p> <pre><code>ggsave(\"example output/Fieldsamples_barchart.png\", width = 9.5, height=5)\n</code></pre> <pre><code>## Warning in geom_text(aes(y = cutoff, x = 2, label = \"Outlier\"), hjust = 0.4, : All aesthetics have length 1, but the data has 58 rows.\n## \u2139 Please consider using `annotate()` or provide this layer with data containing\n##   a single row.\n\n## Warning in geom_text(aes(y = log10(LOD), x = 2, label = \"LOD\"), hjust = 0.5, : All aesthetics have length 1, but the data has 58 rows.\n## \u2139 Please consider using `annotate()` or provide this layer with data containing\n##   a single row.\n\n## Warning in geom_text(aes(y = log10(LOQ), x = 2, label = \"LOQ\"), hjust = 0.5, : All aesthetics have length 1, but the data has 58 rows.\n## \u2139 Please consider using `annotate()` or provide this layer with data containing\n##   a single row.\n</code></pre> <p>Show distance of outliers</p> <pre><code>fieldsamples_df %&gt;% ggplot(., aes(x=Sample_Type, y=`Mean Copy Number`, \n                                  fill = ifelse(is.na(`Mean Copy Number Normalized`) | is.na(cutoff), FALSE, \n                                          `Mean Copy Number Normalized` &gt; cutoff))) + \n  geom_jitter(width=0.2, size=3.5, color='black', shape=21, alpha=0.5) +\n  scale_fill_manual(values = c(\"#545E56\", \"#c1121f\"),\n                    labels = c(\"Normal\", \"Outlier\")) +\n  theme_bw() +\n  #geom_hline(yintercept = cutoff, linetype = \"dotted\", color = \"grey50\") +\n  labs(x = \"Sample\",\n       fill = \"Outlier Detection\"\n       ) +\n      theme(panel.background=element_rect(fill='white', colour='black'),\n        legend.position = \"right\",\n        axis.text.y = element_text(size=8, color=\"grey20\"),\n        axis.text.x = element_blank(),\n        axis.title.y = element_text(margin = margin(t = 0, r = 10, b = 0, l = 0), size=10, face=\"bold\"),\n        axis.title.x = element_text(margin = margin(t = 10, r = 0, b = 0, l = 0), size=10, face=\"bold\"),\n    ) +\n    guides(fill = guide_legend(override.aes = list(alpha = 1)))\n</code></pre> <pre><code>## Warning: Removed 82 rows containing missing values or values outside the scale range\n## (`geom_point()`).\n</code></pre> <p></p> <pre><code>ggsave(\"example output/Outliers.png\", width = 5, height=4.5)\n</code></pre> <pre><code>## Warning: Removed 82 rows containing missing values or values outside the scale range\n## (`geom_point()`).\n</code></pre>"},{"location":"eDNA%20qPCR/qPCR-analysis/#exporting-data","title":"Exporting data","text":"<pre><code>blank_df %&gt;% mutate(Date = as.Date(Date)) %&gt;% \n  dplyr::select(Date, Sample_Location, `Mean Ct`, `Number of Replicates`, \n                `Mean Copy Number`, `Mean Copy Number Normalized`, Detection) %&gt;%\n\n  unite(Sample, Date, Sample_Location, sep = \" \", remove = TRUE) %&gt;%\n\n  # cut decimals down to 2\n  mutate(across(where(is.numeric), ~round(., 2))) %&gt;%\n\n  write_xlsx(\"example output/Results_Blanks.xlsx\")\n\n### adding some meta for Rich \nrich_meta &lt;- read_xlsx(\"C:/BoxDrive/Box/Science/Fisheries/Projects/eDNA/RI Striped Bass/metadata/eDNA_Data_RI_STB_2024.xlsx\") %&gt;%\n  dplyr::rename(Date = Sample_Date, Sample_Location = Station_Code) %&gt;%\n  mutate(Sample_Time = format(Sample_Time, format = \"%H:%M:%S\"))\n\n\nnormalized_df %&gt;% #full_join(rich_meta, ., by = c(\"Date\", \"Sample_Location\")) %&gt;%\n  mutate(Date = as.Date(Date)) %&gt;%\n  dplyr::select(-Number_of_Filters) %&gt;% write_xlsx(\"example output/Results.xlsx\")\n</code></pre>"}]}