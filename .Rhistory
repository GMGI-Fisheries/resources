) +
labs(
x = "Sample",
y = "Distance from Positive Control"
) +
theme_bw() +
## keep y axis at least -2,2 but extend to max
coord_cartesian(
ylim = c(
min(-2.5, min(spike_samples$Cq_diff, na.rm = TRUE)),
max(2.5, max(spike_samples$Cq_diff, na.rm = TRUE))
)) +
## theme variables
theme(panel.background=element_rect(fill='white', colour='black'),
legend.position = "none",
axis.text.y = element_text(size=10, color="grey20"),
axis.text.x = element_blank(),
axis.title.y = element_text(margin = margin(t = 0, r = 10, b = 0, l = 0), size=12, face="bold"),
axis.title.x = element_text(margin = margin(t = 10, r = 0, b = 0, l = 0), size=12, face="bold"))
ggsave("example output/Inhibition.png", width=5, height=4)
## USER EDITS: Replace values below with species-specific assay values for copy number calculation
yint <- 38.183
slope <- -3.386
## Complete calculations
filters_df <- df %>%
## subset out the spiked samples
filter(!grepl("spike|pos", Sample_ID, ignore.case = TRUE)) %>%
## group by plate ID and sample ID
group_by(plate.ID, Sample_ID) %>%
## calculate mean of Cq per plate and sample, replacing NaN with NA
mutate(Filter_Cq = if_else(is.nan(mean(Cq, na.rm = TRUE)), NA_real_, mean(Cq, na.rm = TRUE))) %>%
## calculate # of replicates
mutate(Filter_Num_Replicates = sum(!is.na(Cq))) %>%
## calculate copy number
mutate(Filter_Copy_Num = 10^((Filter_Cq-yint)/slope)) %>%
## summarize df with specific columns
dplyr::select(plate.ID, Sample_ID, Filter_Cq, Filter_Num_Replicates, Filter_Copy_Num) %>%
distinct() %>% ungroup()
filters_df <- filters_df %>%
## Separating Plate.ID into Date and Plate Number
separate(plate.ID, c("Plate_date", "Plate_number"), sep = " PLATE") %>%
## Change Date column to a Date format
mutate(Plate_date = dmy(Plate_date)) %>%
## Group by Sample_ID and keep only the row with the most recent date
group_by(Sample_ID) %>%
slice_max(Plate_date, n = 1, with_ties = FALSE) %>% ungroup()
## confirm the number of rows matches the number of unique Sample IDs (output should be TRUE)
nrow(filters_df) == length(unique(filters_df$Sample_ID))
samples_df <- filters_df %>% right_join(meta, ., by = "Sample_ID") %>%
group_by(Date, Sample_Location) %>%
## mutate to mean
mutate(Sample_Cq = if_else(is.nan(mean(Filter_Cq, na.rm = TRUE)),
NA_real_, mean(Filter_Cq, na.rm = TRUE)),
# take highest value
Sample_Num_Replicates = mean(Filter_Num_Replicates),
### change to sum of 2 values
Sample_Copy_Num = if_else(is.nan(mean(Filter_Copy_Num, na.rm = TRUE)),
NA_real_, mean(Filter_Copy_Num, na.rm = TRUE))) %>%
ungroup() %>%
## summarizing df
dplyr::select(Date, Sample_Location, Sample_Type, Number_of_Filters,
Sample_Cq, Sample_Num_Replicates, Sample_Copy_Num) %>%
distinct() %>%
## adding new SampleID back in
unite(Sample_ID, Date, Sample_Location, sep = " ", remove=F)
head(samples_df)
## normalize data
normalized_df <- samples_df %>%
## take log10 of copy number
mutate(Sample_Copy_Num_normalized = log10(Sample_Copy_Num + 1))
## create an outlier cut-off
cutoff <- median(normalized_df$Sample_Copy_Num_normalized, na.rm = TRUE) +
3*IQR(normalized_df$Sample_Copy_Num_normalized, na.rm=TRUE)
## filter based on the outlier cut-off (if log sample copy num is above cut-off, change to NA)
normalized_df <- normalized_df %>%
mutate(Sample_Copy_Num_normalized =
if_else(Sample_Copy_Num_normalized > cutoff, NA_real_, Sample_Copy_Num_normalized)) %>%
## calculate relative abundance based on this log normalized value to a value 0-1
mutate(Relative_Abundance =
(Sample_Copy_Num_normalized - min(Sample_Copy_Num_normalized, na.rm=TRUE))/
(max(Sample_Copy_Num_normalized, na.rm=TRUE) - min(Sample_Copy_Num_normalized, na.rm=TRUE))
)
## create table of what gets cut out
## plot
## flag but include
## try relative ab value/total and compare to the rel ab code above - what do we want to go with?
normalized_df <- normalized_df %>%
mutate(Detection = case_when(
is.na(Sample_Copy_Num_normalized) ~ "Absent",
TRUE ~ "Present"
))
blank_df <- normalized_df %>% subset(Sample_Type == "Blank")
detection_counts <- blank_df %>%
count(Detection) %>%
mutate(percentage = n / sum(n) * 100,
label = paste0(Detection, "\n", round(percentage, 1), "%"))
ggplot(detection_counts, aes(x = "", y = n, fill = Detection)) +
geom_bar(stat = "identity", width = 1, color = "black", alpha=0.75) +
coord_polar("y", start = 0) +
geom_text(aes(label = label),
position = position_stack(vjust = 0.5),
size = 5,
fontface = "bold") +
theme_void() +
theme(legend.position = "none",
#plot.title = element_text(hjust = 0.5, size = 16, face = "bold")
) +
scale_fill_manual(values = c("#D2D4D4", "#426999"))
ggsave("example output/Blanks_piechart.png", width=4, height=4)
detection_counts
ggplot(detection_counts, aes(x = "", y = n, fill = Detection)) +
geom_bar(stat = "identity", width = 1, color = "black", alpha=0.75) +
coord_polar("y", start = 0) +
geom_text(aes(label = label),
position = position_stack(vjust = 0.5),
size = 5,
fontface = "bold") +
theme_void() +
theme(legend.position = "none",
#plot.title = element_text(hjust = 0.5, size = 16, face = "bold")
) +
scale_fill_manual(values = c("#D2D4D4", "#426999"))
fieldsamples_df <- normalized_df %>% subset(Sample_Type == "Field")
field_detection_counts <- fieldsamples_df %>%
count(Detection) %>%
mutate(percentage = n / sum(n) * 100,
label = paste0(Detection, "\n", round(percentage, 1), "%"))
ggplot(field_detection_counts, aes(x = "", y = n, fill = Detection)) +
geom_bar(stat = "identity", width = 1, color = "black", alpha=0.75) +
coord_polar("y", start = 0) +
geom_text(aes(label = label),
position = position_stack(vjust = 0.5),
size = 5,
fontface = "bold") +
theme_void() +
theme(legend.position = "none",
#plot.title = element_text(hjust = 0.5, size = 16, face = "bold")
) +
scale_fill_manual(values = c("#D2D4D4", "#426999"))
ggsave("example output/Fields_piechart.png", width=4, height=4)
## add n=
fieldsamples_df %>%
filter(!is.na(Relative_Abundance)) %>%
ggplot(., aes(x=Sample_ID, y=Relative_Abundance)) +
#ggplot(., aes(x=Sample_Location, y=Relative_Abundance)) +
#geom_point(size = 2, alpha=0.75, color = 'black', fill = '#426999', shape=21) +
geom_bar(stat = "identity", width = 0.7, fill = '#426999', alpha=0.75) +
#facet_wrap(~Date) +
labs(
y="Relative Abundance",
x = "Sample ID"
) +
theme_bw() +
## theme variables
theme(panel.background=element_rect(fill='white', colour='black'),
legend.position = "none",
axis.text.y = element_text(size=8, color="grey20"),
axis.text.x = element_text(size=6, color="grey20", angle=90, hjust=1, vjust=0.5),
axis.title.y = element_text(margin = margin(t = 0, r = 10, b = 0, l = 0), size=10, face="bold"),
axis.title.x = element_text(margin = margin(t = 10, r = 0, b = 0, l = 0), size=10, face="bold"),
# New theme elements for strip appearance
strip.background = element_rect(fill = "white", color = "black"),
strip.text = element_text(face = "bold", size = 9)
)
ggsave("example output/Fieldsample_relative_abundancev2.png", width = 9.5, height=5)
blank_df %>% mutate(Date = as.Date(Date)) %>%
dplyr::select(Date, Sample_Location, Sample_Cq, Sample_Num_Replicates, Sample_Copy_Num, Relative_Abundance, Detection) %>%
dplyr::rename(`Number of Replicates` = Sample_Num_Replicates, `Mean Ct` = Sample_Cq,
`Mean Copy Number` = Sample_Copy_Num, `Relative Abundance` = Relative_Abundance,
Sample = Sample_Location) %>%
write_xlsx("example output/Results_Blanks.xlsx")
normalized_df %>% mutate(Date = as.Date(Date)) %>%
dplyr::select(Date, Sample_Location, Sample_Cq, Sample_Num_Replicates, Sample_Copy_Num, Relative_Abundance, Detection) %>%
dplyr::rename(`Number of Replicates` = Sample_Num_Replicates, `Mean Ct` = Sample_Cq,
`Mean Copy Number` = Sample_Copy_Num, `Relative Abundance` = Relative_Abundance,
Sample = Sample_Location) %>%
write_xlsx("example output/Results.xlsx")
df
NTC <- df %>%
filter(grepl("neg", Sample_ID, ignore.case = TRUE))
NTC
## Calculate number of plates
length(unique(NTC$plate.ID))
## Calculate number of negatives with Cq values
sum(!is.na(NTC$Cq)))
## Calculate number of negatives with Cq values
sum(!is.na(NTC$Cq))
library(ggplot2) ## for plotting
library(dplyr) ## for data table manipulation
library(tidyr) ## for data table manipulation
library(readr) ## for reading in tsv files
library(readxl) ## for reading in excel files
library(stringr) ## for data transformation
library(strex) ## for data transformation
library(writexl) ## for excel output
library(purrr) ## for data transformation
library(tidyverse) ## for data table manipulation
library(ggrepel)  # For geom_text_repel
### USER EDITS:
## 1. Replace path of files
## 2. In str_after_nth(file.ID, "results/", 1), make sure this matches the folder ending referenced in list.files
df <-
# list files in directory following a particular pattern
list.files(path = 'example input/qPCR_results/', pattern = ".xlsx", full.names = TRUE) %>%
# get the column names
set_names(.) %>%
# join all files together in one data frame by file ID, skipping first 19 rows
map_dfr(~read_xlsx(., skip = 19), .id = "file.ID") %>%
# turn file.ID into just plate information (plate.ID)
mutate(file.ID = str_after_nth(file.ID, "results/", 1),
file.ID = str_before_nth(file.ID, ".xlsx", 1)) %>%
dplyr::rename(plate.ID = file.ID, Sample_ID = Sample) %>%
## RI STRIPED BASS SPECIFIC CODE FOR SAMPLE ID MISMATCHES
mutate(Sample_ID = gsub(" #1", "_1", Sample_ID),
Sample_ID = gsub(" #2", "_2", Sample_ID))
head(df)
meta <- read_xlsx("example input/client_metadata_example.xlsx") %>%
## RI STRIPED BASS SPECIFIC CODE FOR SAMPLE ID ISSUES
## For other projects, address any Sample_ID differences (Sample_ID on qPCR output needs to match meta)
mutate(Sample_ID = gsub(" #1", "_1", Sample_ID),
Sample_ID = gsub(" #2", "_2", Sample_ID))
NTC <- df %>%
filter(grepl("neg", Sample_ID, ignore.case = TRUE))
## Calculate number of negatives with Cq values
sum(!is.na(NTC$Cq))
## Calculate number of negatives total
nrow(NTC)
## Calculate number of plates
length(unique(NTC$plate.ID))
spike_samples <- df %>%
## subset to spiked samples
filter(grepl("spike|pos", Sample_ID, ignore.case = TRUE)) %>%
## remove spike from SampleID column only if 'pos' is NOT in the Sample_ID
mutate(Sample_ID = case_when(
!grepl("pos", Sample_ID, ignore.case = TRUE) ~ str_before_nth(Sample_ID, " spike", 1),
TRUE ~ Sample_ID
)) %>%
## group by plate ID and sample ID
group_by(plate.ID, Sample_ID) %>%
## calculate mean of spikes per plate and sample
## ungroup by Sample_ID so the next calculation is only done grouped by Plate
mutate(Filter_Cq = mean(Cq)) %>%
ungroup(Sample_ID) %>%
## calculate difference from plate's Cq value
mutate(Cq_diff = Filter_Cq - Filter_Cq[grepl("pos", Sample_ID, ignore.case = TRUE)]) %>%
ungroup(); spike_samples
## Remove any lab error samples
## RI Striped Bass only
## removing 2/21/24 DI Blank 1 (pipetting issue)
spike_samples <- spike_samples %>%
filter(!Sample_ID == "2/21/24_Tap Blank_1")
# Create a jitter position object
jitter_pos <- position_jitter(width = 0.45, seed = 123)
## Plot samples
spike_samples %>% dplyr::select(Target, Sample_ID, Cq_diff) %>% distinct() %>%
ggplot(., aes(x=Target, y = Cq_diff)) +
geom_hline(yintercept = c(-2, 2), linetype = "dotted", color = "grey50") +
geom_jitter(aes(fill = abs(Cq_diff) > 2),
size = 2, alpha = 0.5, color = 'black', shape = 21, #width = 0.45,
position = jitter_pos) +
scale_fill_manual(values = c("TRUE" = "red", "FALSE" = "white")) +
geom_text_repel(
aes(label = ifelse(abs(Cq_diff) > 2, Sample_ID, "")),  position = jitter_pos,
size = 3, box.padding = 0.5, point.padding = 0.2, force = 2
) +
labs(
x = "Sample",
y = "Distance from Positive Control"
) +
theme_bw() +
## keep y axis at least -2,2 but extend to max
coord_cartesian(
ylim = c(
min(-2.5, min(spike_samples$Cq_diff, na.rm = TRUE)),
max(2.5, max(spike_samples$Cq_diff, na.rm = TRUE))
)) +
## theme variables
theme(panel.background=element_rect(fill='white', colour='black'),
legend.position = "none",
axis.text.y = element_text(size=10, color="grey20"),
axis.text.x = element_blank(),
axis.title.y = element_text(margin = margin(t = 0, r = 10, b = 0, l = 0), size=12, face="bold"),
axis.title.x = element_text(margin = margin(t = 10, r = 0, b = 0, l = 0), size=12, face="bold"))
ggsave("example output/Inhibition.png", width=5, height=4)
## USER EDITS: Replace values below with species-specific assay values for copy number calculation
yint <- 38.183
slope <- -3.386
## Complete calculations
filters_df <- df %>%
## subset out the spiked samples
filter(!grepl("spike|pos|neg", Sample_ID, ignore.case = TRUE)) %>%
## group by plate ID and sample ID
group_by(plate.ID, Sample_ID) %>%
## calculate mean of Cq per plate and sample, replacing NaN with NA
mutate(Filter_Cq = if_else(is.nan(mean(Cq, na.rm = TRUE)), NA_real_, mean(Cq, na.rm = TRUE))) %>%
## calculate # of replicates
mutate(Filter_Num_Replicates = sum(!is.na(Cq))) %>%
## calculate copy number
mutate(Filter_Copy_Num = 10^((Filter_Cq-yint)/slope)) %>%
## summarize df with specific columns
dplyr::select(plate.ID, Sample_ID, Filter_Cq, Filter_Num_Replicates, Filter_Copy_Num) %>%
distinct() %>% ungroup()
filters_df <- filters_df %>%
## Separating Plate.ID into Date and Plate Number
separate(plate.ID, c("Plate_date", "Plate_number"), sep = " PLATE") %>%
## Change Date column to a Date format
mutate(Plate_date = dmy(Plate_date)) %>%
## Group by Sample_ID and keep only the row with the most recent date
group_by(Sample_ID) %>%
slice_max(Plate_date, n = 1, with_ties = FALSE) %>% ungroup()
## confirm the number of rows matches the number of unique Sample IDs (output should be TRUE)
nrow(filters_df) == length(unique(filters_df$Sample_ID))
samples_df <- filters_df %>% right_join(meta, ., by = "Sample_ID") %>%
group_by(Date, Sample_Location) %>%
## mutate to mean Ct value
mutate(`Mean Ct` = if_else(is.nan(mean(Filter_Cq, na.rm = TRUE)),
NA_real_, mean(Filter_Cq, na.rm = TRUE)),
## take highest value of number of replicates
`Number of Replicates` = max(Filter_Num_Replicates, na.rm=TRUE),
## sum of 2 copy number values
`Mean Copy Number` = sum(Filter_Copy_Num)
) %>%
ungroup() %>%
## summarizing df
dplyr::select(Date, Sample_Location, Sample_Type, Number_of_Filters,
`Mean Ct`, `Number of Replicates`, `Mean Copy Number`) %>%
distinct() %>%
## adding new SampleID back in
unite(Sample_ID, Date, Sample_Location, sep = " ", remove=F)
head(samples_df)
## Confirm that the number of samples output from qPCR matches the number of samples in the metadata
nrow(samples_df) == nrow(meta %>% dplyr::select(-Sample_ID) %>% distinct())
## normalize data
normalized_df <- samples_df %>%
## take log10 of copy number
mutate(`Mean Copy Number Normalized` = log10(`Mean Copy Number` + 1),
## calculate relative abundance based on this log normalized value to a value 0-1
Relative_Abundance =
(`Mean Copy Number Normalized` - min(`Mean Copy Number Normalized`, na.rm=TRUE))/
(max(`Mean Copy Number Normalized`, na.rm=TRUE) - min(`Mean Copy Number Normalized`, na.rm=TRUE)),
Relative_Abudancev2 =
`Mean Copy Number Normalized`/
sum(`Mean Copy Number Normalized`, na.rm=TRUE)
)
## create an outlier cut-off
cutoff <- median(normalized_df$`Mean Copy Number Normalized`, na.rm = TRUE) +
3*IQR(normalized_df$`Mean Copy Number Normalized`, na.rm=TRUE)
## Output the values that outside the cutoff
normalized_df %>% filter(`Mean Copy Number Normalized` > cutoff)
normalized_df <- normalized_df %>%
mutate(Detection = case_when(
is.na(`Mean Copy Number Normalized`) ~ "Absent",
TRUE ~ "Present"
))
blank_df <- normalized_df %>% subset(Sample_Type == "Blank")
detection_counts <- blank_df %>%
count(Detection) %>%
mutate(percentage = n / sum(n) * 100,
label = paste0(Detection, "\n", "n=", n, "\n", round(percentage, 1), "%"))
ggplot(detection_counts, aes(x = "", y = n, fill = Detection)) +
geom_bar(stat = "identity", width = 1, color = "black", alpha=0.75) +
coord_polar("y", start = 0) +
geom_text(aes(label = label),
position = position_stack(vjust = 0.5),
size = 5,
fontface = "bold") +
theme_void() +
theme(legend.position = "none",
#plot.title = element_text(hjust = 0.5, size = 16, face = "bold")
) +
scale_fill_manual(values = c("#D2D4D4", "#426999"))
ggsave("example output/Blanks_piechart.png", width=4, height=4)
fieldsamples_df <- normalized_df %>% subset(Sample_Type == "Field")
field_detection_counts <- fieldsamples_df %>%
count(Detection) %>%
mutate(percentage = n / sum(n) * 100,
label = paste0(Detection, "\n", "n=", n, "\n", round(percentage, 1), "%"))
ggplot(field_detection_counts, aes(x = "", y = n, fill = Detection)) +
geom_bar(stat = "identity", width = 1, color = "black", alpha=0.75) +
coord_polar("y", start = 0) +
geom_text(aes(label = label),
position = position_stack(vjust = 0.5),
size = 5,
fontface = "bold") +
theme_void() +
theme(legend.position = "none",
#plot.title = element_text(hjust = 0.5, size = 16, face = "bold")
) +
scale_fill_manual(values = c("#D2D4D4", "#426999"))
ggsave("example output/Fields_piechart.png", width=4, height=4)
fieldsamples_df %>%
filter(!is.na(Relative_Abundance)) %>%
ggplot(., aes(x=Sample_ID, y=Relative_Abundance, fill = `Mean Copy Number Normalized` > cutoff)) +
geom_bar(stat = "identity", width = 0.7, alpha=0.75) +
scale_fill_manual(values = c("#426999", "#c1121f"),
labels = c("Normal", "Outlier"),
) +
labs(
y="Relative Abundance",
x = "Sample ID",
fill = "Outlier detection"
) +
theme_bw() +
## theme variables
theme(panel.background=element_rect(fill='white', colour='black'),
legend.position = "right",
axis.text.y = element_text(size=8, color="grey20"),
axis.text.x = element_text(size=6, color="grey20", angle=90, hjust=1, vjust=0.5),
axis.title.y = element_text(margin = margin(t = 0, r = 10, b = 0, l = 0), size=10, face="bold"),
axis.title.x = element_text(margin = margin(t = 10, r = 0, b = 0, l = 0), size=10, face="bold"),
# New theme elements for strip appearance
strip.background = element_rect(fill = "white", color = "black"),
strip.text = element_text(face = "bold", size = 9)
) +
guides(fill = guide_legend(override.aes = list(alpha = 1)))
ggsave("example output/Fieldsample_relative_abundance.png", width = 9.5, height=5)
fieldsamples_df %>%
filter(!is.na(Relative_Abudancev2)) %>%
ggplot(., aes(x=Sample_ID, y=Relative_Abudancev2, fill = `Mean Copy Number Normalized` > cutoff)) +
geom_bar(stat = "identity", width = 0.7, alpha=0.75) +
scale_fill_manual(values = c("#426999", "#c1121f"),
labels = c("Normal", "Outlier"),
) +
labs(
y="Relative Abundance",
x = "Sample ID",
fill = "Outlier detection"
) +
theme_bw() +
## theme variables
theme(panel.background=element_rect(fill='white', colour='black'),
legend.position = "right",
axis.text.y = element_text(size=8, color="grey20"),
axis.text.x = element_text(size=6, color="grey20", angle=90, hjust=1, vjust=0.5),
axis.title.y = element_text(margin = margin(t = 0, r = 10, b = 0, l = 0), size=10, face="bold"),
axis.title.x = element_text(margin = margin(t = 10, r = 0, b = 0, l = 0), size=10, face="bold"),
# New theme elements for strip appearance
strip.background = element_rect(fill = "white", color = "black"),
strip.text = element_text(face = "bold", size = 9)
) +
guides(fill = guide_legend(override.aes = list(alpha = 1)))
ggsave("example output/Fieldsample_relative_abundancev2.png", width = 9.5, height=5)
fieldsamples_df %>%
filter(!is.na(`Mean Copy Number Normalized`)) %>%
ggplot(., aes(x=Sample_ID, y=`Mean Copy Number Normalized`, fill = `Mean Copy Number Normalized` > cutoff)) +
geom_bar(stat = "identity", width = 0.7, alpha=0.75) +
scale_fill_manual(values = c("#426999", "#c1121f"),
labels = c("Normal", "Outlier"),
) +
labs(
y="Normalized Abundance",
x = "Sample ID",
fill = "Outlier detection"
) +
theme_bw() +
## theme variables
theme(panel.background=element_rect(fill='white', colour='black'),
legend.position = "right",
axis.text.y = element_text(size=8, color="grey20"),
axis.text.x = element_text(size=6, color="grey20", angle=90, hjust=1, vjust=0.5),
axis.title.y = element_text(margin = margin(t = 0, r = 10, b = 0, l = 0), size=10, face="bold"),
axis.title.x = element_text(margin = margin(t = 10, r = 0, b = 0, l = 0), size=10, face="bold"),
# New theme elements for strip appearance
strip.background = element_rect(fill = "white", color = "black"),
strip.text = element_text(face = "bold", size = 9)
) +
guides(fill = guide_legend(override.aes = list(alpha = 1)))
ggsave("example output/Fieldsample_logasRELAB.png", width = 9.5, height=5)
blank_df %>% mutate(Date = as.Date(Date)) %>%
dplyr::select(Date, Sample_Location, `Mean Ct`, `Number of Replicates`,
`Mean Copy Number`, Relative_Abundance, Detection) %>%
unite(Date, Sample_Location, sep = " ", remove = FALSE) %>%
# cut decimals down to 2
mutate(across(where(is.numeric), ~round(., 2)))
blank_df %>% mutate(Date = as.Date(Date)) %>%
dplyr::select(Date, Sample_Location, `Mean Ct`, `Number of Replicates`,
`Mean Copy Number`, Relative_Abundance, Detection) %>%
unite(Sample, Date, Sample_Location, sep = " ", remove = TRUE) %>%
# cut decimals down to 2
mutate(across(where(is.numeric), ~round(., 2)))
blank_df %>% mutate(Date = as.Date(Date)) %>%
dplyr::select(Date, Sample_Location, `Mean Ct`, `Number of Replicates`,
`Mean Copy Number`, Relative_Abundance, Detection) %>%
unite(Sample, Date, Sample_Location, sep = " ", remove = TRUE) %>%
# cut decimals down to 2
mutate(across(where(is.numeric), ~round(., 2))) %>%
write_xlsx("example output/Results_Blanks.xlsx")
blank_df %>% mutate(Date = as.Date(Date)) %>%
dplyr::select(Date, Sample_Location, `Mean Ct`, `Number of Replicates`,
`Mean Copy Number`, Relative_Abundance, Detection) %>%
unite(Sample, Date, Sample_Location, sep = " ", remove = TRUE) %>%
# cut decimals down to 2
mutate(across(where(is.numeric), ~round(., 2))) %>%
write_xlsx("example output/Results_Blanks.xlsx")
normalized_df %>% mutate(Date = as.Date(Date)) %>%
dplyr::select(Date, Sample_Location, `Mean Ct`, `Number of Replicates`,
`Mean Copy Number`, Relative_Abundance, Detection) %>%
unite(Sample, Date, Sample_Location, sep = " ", remove = TRUE) %>%
# cut decimals down to 2
mutate(across(where(is.numeric), ~round(., 2))) %>%
write_xlsx("example output/Results.xlsx")
library(ggplot2) ## for plotting
library(dplyr) ## for data table manipulation
library(tidyr) ## for data table manipulation
library(readr) ## for reading in tsv files
library(readxl) ## for reading in excel files
library(stringr) ## for data transformation
library(strex) ## for data transformation
library(writexl) ## for excel output
library(purrr) ## for data transformation
library(funrar) ## for make_relative()
library(tidyverse) ## for data table manipulation
